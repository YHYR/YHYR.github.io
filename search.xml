<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>Kafka Timestamp</title>
      <link href="/2019/01/23/Kafka-Timestamp/"/>
      <content type="html"><![CDATA[<p>简述Kafka在时间戳上的改进历程；实现基于时间戳操作Offset</p><a id="more"></a><h1 id="Timestamp来龙去脉"><a href="#Timestamp来龙去脉" class="headerlink" title="Timestamp来龙去脉"></a>Timestamp来龙去脉</h1><p><img src="./KIP-timestamp.png" alt="KIP-timestamp"></p><h2 id="Message-Body"><a href="#Message-Body" class="headerlink" title="Message Body"></a>Message Body</h2><p>出于对日志保存、日志切分和Kafka Streaming的改进和优化，Kafka从<font color="red">0.10.0.0版本</font>起，在消息内新增加了个timestamp字段；时间戳的类型有两种：可以设定为producer创建消息的时间(CreateTime)，也可以设定为该消息写入Broker的时间(LogAppendTime)。默认为CreateTime，可通过参数<code>message.timestamp.type</code> 实现Topic级别的类型更改，Broker级别的时间戳类型参数为<code>log.message.timestamp.type</code>。</p><p><em>有关Kafka Message新增时间戳的相关细节，可详见Kafka官方Doc [KIP-32 - Add timestamps to Kafka message</em>](<a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-32+-+Add+timestamps+to+Kafka+message" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/KAFKA/KIP-32+-+Add+timestamps+to+Kafka+message</a>)</p><h2 id="Log-Segment"><a href="#Log-Segment" class="headerlink" title="Log Segment"></a>Log Segment</h2><p>在Kafka 0.10.1.0以前(不包含0.10.1.0)，对于一个Topic而言，其Log Segment是由一个.log文件和一个.index文件组合而成，分别用来存储具体的消息数据和对应的偏移量；如下图所示：</p><p><img src="./老版本Segment.png" alt="老版本Segment"></p><p>从Kafka 0.10.1.0开始，对于日志文件，新增一个.timeindex文件，即每个Segment分别由.log、.index和.timeindex这三个文件组成。</p><p><img src="./新版本Segment.png" alt="新版本Segment"></p><p><em>有关Log Segment 新增.timeindex相关细节，可详见Kafka官方Doc <a href="https://cwiki.apache.org/confluence/display/KAFKA/KIP-33+-+Add+a+time+based+log+index" target="_blank" rel="noopener">KIP-33 - Add a time based log index</a></em></p><h1 id="根据时间戳查找消息"><a href="#根据时间戳查找消息" class="headerlink" title="根据时间戳查找消息"></a>根据时间戳查找消息</h1><p>在0.10.1.0以前，Kafka提供了通过指定Offset来消费消息(若不清楚如何实现，可参考<a href="https://yhyr.github.io/2019/01/01/Kafka%E5%AE%9E%E8%B7%B5%E4%B9%8BConsumer/" target="_blank" rel="noopener">Kafka实践之Consumer</a>)。讲道理，在那时候这个功能其实是比较鸡肋的，因为通常每天Kafka的消息量都会比较大，假如需要获取到前一天凌晨三点到五天之间产生的所有消息，维护人员压根不知道这个时间段内Offset的范围。当然这个问题从0.10.1.0以后将不复存在，接下来就来讨论如何实现按照时间戳查找消息。</p><p>以Python API为例，从0.10.1.0以后新增加了一个offsets_for_times方法(Java API对应的方法为offsetsForTimes)，可以通过给定时间戳获取每一个Partition上大于等于该时间戳的最早的Offset值。这句话读起来可能比较拗口，举个例子：统计2018-12-30 17:00:00 到2018-12-30 20:00:00期间，Topic(topic_demo)的消息偏移量范围</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> kafka <span class="keyword">import</span> KafkaConsumer, TopicPartition</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GetOffsetWithTimestamp</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, broker_list, topic)</span>:</span></span><br><span class="line">        self.topic = topic</span><br><span class="line">        self.consumer = KafkaConsumer(bootstrap_servers=broker_list)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_offset_time_window</span><span class="params">(self, begin_time, end_time)</span>:</span></span><br><span class="line">        partitions_structs = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> partition_id <span class="keyword">in</span> self.consumer.partitions_for_topic(self.topic):</span><br><span class="line">            partitions_structs.append(TopicPartition(self.topic, partition_id))</span><br><span class="line"></span><br><span class="line">        begin_search = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> partition <span class="keyword">in</span> partitions_structs:</span><br><span class="line">            begin_search[partition] = begin_time <span class="keyword">if</span> isinstance(begin_time, int) <span class="keyword">else</span> self.__str_to_timestamp(begin_time)</span><br><span class="line">        begin_offset = self.consumer.offsets_for_times(begin_search)</span><br><span class="line"></span><br><span class="line">        end_search = &#123;&#125;</span><br><span class="line">        <span class="keyword">for</span> partition <span class="keyword">in</span> partitions_structs:</span><br><span class="line">            end_search[partition] = end_time <span class="keyword">if</span> isinstance(end_time, int) <span class="keyword">else</span> self.__str_to_timestamp(end_time)</span><br><span class="line">        end_offset = self.consumer.offsets_for_times(end_search)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> topic_partition, offset_and_timestamp <span class="keyword">in</span> begin_offset.items():</span><br><span class="line">            b_offset = <span class="string">'null'</span> <span class="keyword">if</span> offset_and_timestamp <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">else</span> offset_and_timestamp[<span class="number">0</span>]</span><br><span class="line">            e_offset = <span class="string">'null'</span> <span class="keyword">if</span> end_offset[topic_partition] <span class="keyword">is</span> <span class="keyword">None</span> <span class="keyword">else</span> end_offset[topic_partition][<span class="number">0</span>]</span><br><span class="line">            print(<span class="string">'Between &#123;0&#125; and &#123;1&#125;, &#123;2&#125; offset range = [&#123;3&#125;, &#123;4&#125;]'</span>.format(begin_time, end_time, topic_partition,</span><br><span class="line">                                                                              b_offset, e_offset))</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str_to_timestamp</span><span class="params">(str_time, format_type=<span class="string">'%Y-%m-%d %H:%M:%S'</span>)</span>:</span></span><br><span class="line">        time_array = time.strptime(str_time, format_type)</span><br><span class="line">        <span class="keyword">return</span> int(time.mktime(time_array)) * <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    broker_list = <span class="string">'localhost:9092'</span></span><br><span class="line">    topic = <span class="string">'topic_demo'</span></span><br><span class="line"></span><br><span class="line">    action = GetOffsetWithTimestamp(broker_list, topic)</span><br><span class="line">    action.get_offset_time_window(<span class="string">'2018-12-30 17:00:00'</span>, <span class="string">'2018-12-30 20:00:00'</span>)</span><br></pre></td></tr></table></figure><p>对于offsets_for_times的返回结果是一个<code>{TopicPartition: OffsetAndTimestamp}</code>，对于不同的TopicPartition(topic, partition)，其对应的OffsetAndTimestamp(offset, timestamp)通常都会是不一样的，如果找不到满足给定时间的最早消息，则返回None。</p><p>若想实现消费指定时间范围内所产生的消息，源码详见<a href="https://github.com/YHYR/Kafka-Utils" target="_blank" rel="noopener">GitHub</a></p><h1 id="Kafka-tools-GetOffsetShell"><a href="#Kafka-tools-GetOffsetShell" class="headerlink" title="Kafka.tools.GetOffsetShell"></a>Kafka.tools.GetOffsetShell</h1><p>查看消息偏移量是平时使用和维护过程中一个比较常见的操作；Kafka提供了一个内置的工具脚本来满足这方面的需求，代码实现逻辑详见<a href="https://github.com/YHYR/Kafka-Utils" target="_blank" rel="noopener">GitHub</a>，在这里来聊一下Kafka提供的GetOffsetShell工具：可以通过下载Kafka相应版本的源码，在<code>core\src\main\scala\kafka\tools</code>下查看该脚本的源码。该命令的使用方法如下</p><p><code>bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list  &lt;address:port&gt; --topic &lt;string&gt; --time &lt;param&gt;</code></p><p>broker-list、topic和time是三个必须参数，与此同时还可以指定partitions；如果不显示的指定partition，则默认查看所有分区对应的offset信息；在这里主要讨论一下参数time：</p><p>time可以有三种值，分别为-1，-2和一个时间戳；-1代表latest，-2代表earliest，这两个是比较好理解的，可以分别执行一遍命令就能实现查看指定Topic的有效Offset的最大范围。</p><p>需要注意：<font color="red">当time的值为一个时间戳时，这里返回的结果并不是上文中提及到的那种含义：即就是并不是返回各Partition内大于等于当前timestamp的最早那条消息的offset值，而是返回当前时间戳的消息所在的Segment中最早的那条消息的Offset值，说白了就是该时间戳的消息在那个Segment上， 就返回该Segment的文件名(把文件名前无效的零都去掉)</font>。</p><p><img src="./GetOffsetShell-timestamp-result.png" alt="GetOffsetShell-timestamp-result"></p><p><img src="./GetOffsetShell-timestamp-segment.png" alt="GetOffsetShell-timestamp-segment"></p><h1 id=""><a href="#" class="headerlink" title=" "></a> </h1>]]></content>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>MySQL存储Json字符串</title>
      <link href="/2019/01/17/MySQL%E5%AD%98%E5%82%A8Json%E5%AD%97%E7%AC%A6%E4%B8%B2/"/>
      <content type="html"><![CDATA[<p>解决向MySQL中插入Json字符串无法正确解析的问题</p><a id="more"></a><h2 id="环境依赖"><a href="#环境依赖" class="headerlink" title="环境依赖"></a>环境依赖</h2><blockquote><p>Python 2.7</p><p>MySQL 5.7</p><p>MySQL-python 1.2.5</p><p>Pandas 0.18.1</p></blockquote><p>在日常的数据处理中，免不了需要将一些序列化的结果存入到MySQL中。这里以插入JSON数据为例，讨论这种问题发生的原因和解决办法。现在的MySQL已经支持JSON数据格式了，在这里不做讨论；主要讨论如何保证存入到MySQL字段中的JsonString能被正确解析。</p><h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> MySQLdb</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line">mysql_conn = MySQLdb.connect(host=<span class="string">'localhost'</span>, user=<span class="string">'root'</span>, passwd=<span class="string">'root'</span>, db=<span class="string">'test'</span>, port=<span class="number">3306</span>, charset=<span class="string">'utf8'</span>)</span><br><span class="line">mysql_cur = mysql_conn.cursor()</span><br><span class="line"></span><br><span class="line">increment_id = <span class="number">1</span></span><br><span class="line">dic = &#123;<span class="string">"value"</span>: <span class="string">"&lt;img src=\"xxx.jpg\"&gt;"</span>, <span class="string">"name"</span>: <span class="string">"小明"</span>&#125;</span><br><span class="line">json_str = json.dumps(dic, ensure_ascii=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">sql = <span class="string">"update demo set msg = '&#123;0&#125;' where id = '&#123;1&#125;'"</span>.format(json_str, increment_id)</span><br><span class="line">mysql_cur.execute(sql)</span><br><span class="line">mysql_conn.commit()</span><br><span class="line">mysql_cur.close()</span><br></pre></td></tr></table></figure><p>应用场景抽象如上所示，将一个字典经过经过Json序列化后作为一个表字段的值存入到Mysql中，按照如上的方式更新数据时，发现落库的JsonString反序列化失败；落库结果和反序列化结果分别如下所示：</p><p><img src="./原始插入结果图.png" alt="原始插入结果图"></p><p><img src="./反序列化错误结果图.png" alt="反序列化错误结果图"></p><h1 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h1><p>对于字符串中包含引号等其他特殊符号的处理思路在大多数编程语言中都是相通的：即就是通过转义符来保留所需要的特殊字符。Python中也不例外，如上所示，对于一个字典<code>{&quot;value&quot;: &quot;&lt;img src=&quot;xxx.jpg&quot;&gt;&quot;, &quot;name&quot;: &quot;小明&quot;}</code>，要想在编译器里正确的表示它，就需要通过对转义包裹xxx.jps的两个双引号，不然会提示错误，所以它的正确写法为：<code>{&quot;value&quot;: &quot;&lt;img src=\&quot;xxx.jpg\&quot;&gt;&quot;, &quot;name&quot;: &quot;小明&quot;}</code>；将序列化后的String作为参数传入待执行的sql语句中，通过编辑器的debug模式查看的效果如下所示：</p><p><img src="./原始序列化结果图.png" alt="原始序列化结果图"></p><p>而这句sql经过编译器解析后传入到MySQL去执行的本质为：<code>&#39;update demo set msg = &#39;{&quot;source&quot;: &quot;&lt;img src=&quot;xxx.jpg&quot;&gt;&quot;, &quot;type&quot;: &quot;图片&quot;}&#39; where id = &#39;1&#39;</code>，因此落库的实际结果其实并不是目标字典对应的序列化结果，而是目标数据对应的字面字符串值。</p><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><p>可以通过转义符替换、修改sql书写方式或通过DataFrame.to_sql()三种方式来解决。</p><h2 id="方案一-转义符替换"><a href="#方案一-转义符替换" class="headerlink" title="方案一  转义符替换"></a>方案一  转义符替换</h2><p>通过上文可以了解到，是因为<code>\\&quot;xxx.jpg\\&quot;</code>的本质即就是<code>&quot;xxx.jpg&quot;</code>，所以数据库读到的也就是<code>{&quot;source&quot;: &quot;&lt;img src=&quot;xxx.jpg&quot;&gt;&quot;, &quot;type&quot;: &quot;图片&quot;}</code>，从而导致插入的结果并不能被正确反序列化。可以通过简单粗暴的转义符替换方式来解决这个问题：<code>json_str.replace(&#39;\\&#39;, &#39;\\\\&#39;)</code>，这样就保证最终的解析结果为<code>\&quot;xxx.jpg\&quot;</code>。</p><h2 id="方案二-修改sql书写方式"><a href="#方案二-修改sql书写方式" class="headerlink" title="方案二  修改sql书写方式"></a>方案二  修改sql书写方式</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">execute</span><span class="params">(self, query, args=None)</span>:</span></span><br><span class="line">    <span class="keyword">del</span> self.messages[:]</span><br><span class="line">    db = self._get_db()</span><br><span class="line">    <span class="keyword">if</span> isinstance(query, unicode):</span><br><span class="line">        query = query.encode(db.unicode_literal.charset)</span><br><span class="line">    <span class="keyword">if</span> args <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="comment"># 通过调用内置的解析函数literal，将目标参数按照原义解析</span></span><br><span class="line">        <span class="comment"># 解析的依据详见源码的MySQLdb.converters</span></span><br><span class="line">        <span class="keyword">if</span> isinstance(args, dict):</span><br><span class="line">            query = query % dict((key, db.literal(item))</span><br><span class="line">                                 <span class="keyword">for</span> key, item <span class="keyword">in</span> args.iteritems())</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            query = query % tuple([db.literal(item) <span class="keyword">for</span> item <span class="keyword">in</span> args])</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        r = <span class="keyword">None</span></span><br><span class="line">        r = self._query(query)</span><br><span class="line">    <span class="keyword">except</span> TypeError, m:</span><br><span class="line">        <span class="keyword">if</span> m.args[<span class="number">0</span>] <span class="keyword">in</span> (<span class="string">"not enough arguments for format string"</span>,</span><br><span class="line">                         <span class="string">"not all arguments converted"</span>):</span><br><span class="line">            self.messages.append((ProgrammingError, m.args[<span class="number">0</span>]))</span><br><span class="line">            self.errorhandler(self, ProgrammingError, m.args[<span class="number">0</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.messages.append((TypeError, m))</span><br><span class="line">            self.errorhandler(self, TypeError, m)</span><br><span class="line">    <span class="keyword">except</span> (SystemExit, KeyboardInterrupt):</span><br><span class="line">        <span class="keyword">raise</span></span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">        exc, value, tb = sys.exc_info()</span><br><span class="line">        <span class="keyword">del</span> tb</span><br><span class="line">        self.messages.append((exc, value))</span><br><span class="line">        self.errorhandler(self, exc, value)</span><br><span class="line">    self._executed = query</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self._defer_warnings: self._warning_check()</span><br><span class="line">    <span class="keyword">return</span> r</span><br></pre></td></tr></table></figure><p>查看MySQL-python的execute源码(如上所示)可以发现，在传入待执行的sql语句的同时，还可以传入参数列表/字典；让MySQL-Python来帮我们进行sql语句的拼接和解析操作，修改上述样例的实现方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">increment_id = <span class="number">1</span></span><br><span class="line">dic = &#123;<span class="string">"value"</span>: <span class="string">"&lt;img src=\"xxx.jpg\"&gt;"</span>, <span class="string">"name"</span>: <span class="string">"小明"</span>&#125;</span><br><span class="line">json_str = json.dumps(dic, ensure_ascii=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">sql = <span class="string">"update demo set msg = %s where id = %s"</span></span><br><span class="line">mysql_cur.execute(sql, [json_str, increment_id])</span><br><span class="line">mysql_conn.commit()</span><br><span class="line">mysql_cur.close()</span><br></pre></td></tr></table></figure><p>通过走读源码发现参数经过literal()方法将Python的对象转化为对应SQL数据的字符串格式；在编译器Debug模式下可以看到最终将<code>\\&quot;xxx.jpg\\&quot;</code>转化为<code>\\\\\\&quot;xxx.jpg\\\\\\&quot;</code>。至于为什么是六个反斜杠我自己也不太清楚；不过姑且可以这样理解：把literal方法的操作可以假定为有一次的序列化，因为给定的数据源是<code>\\&quot;</code>，所以序列化的结果为应该为<code>\\\\&quot;</code>，即就是四个反斜杠；因为<code>\\&quot;</code>代表的即就是<code>&quot;</code>，而期望落库的结果为<code>\&quot;</code>，所以需要再添加两个反斜杠。这种解释不是那么准确和严谨，但是有利于帮助理解，若有了解底层机制和原理的，还请留言指教。</p><p><img src="./execute传参解析结果图.png" alt="execute传参解析结果图"></p><p><em>推荐使用</em></p><h2 id="方案三-DataFrame-to-sql"><a href="#方案三-DataFrame-to-sql" class="headerlink" title="方案三 DataFrame.to_sql()"></a>方案三 DataFrame.to_sql()</h2><p>处理数据离不开Panda工具包；Pandas的DataFrame.to_sql()方法可以便捷有效的实现数据的插入需求；同样该方法也能有效的规避上述这种序列化结果错误的情况，因为DataFrame.to_sql()底层的实现逻辑类似于方案二，也是通过参数解析的方式来拼接sql语句，核心源码如下所示，同于不难发现，DataFrame.to_sql()只能支持insert操作，适用场景比较局限。对于有唯一索引的表，当待插入数据与数据表中有冲突时会报错，实际使用时需要格外注意。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_statement</span><span class="params">(self)</span>:</span></span><br><span class="line">    names = list(map(text_type, self.frame.columns))</span><br><span class="line">    flv = self.pd_sql.flavor</span><br><span class="line">    wld = _SQL_WILDCARD[flv]  <span class="comment"># wildcard char</span></span><br><span class="line">    escape = _SQL_GET_IDENTIFIER[flv]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> self.index <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">        [names.insert(<span class="number">0</span>, idx) <span class="keyword">for</span> idx <span class="keyword">in</span> self.index[::<span class="number">-1</span>]]</span><br><span class="line"></span><br><span class="line">    bracketed_names = [escape(column) <span class="keyword">for</span> column <span class="keyword">in</span> names]</span><br><span class="line">    col_names = <span class="string">','</span>.join(bracketed_names)</span><br><span class="line">    wildcards = <span class="string">','</span>.join([wld] * len(names))</span><br><span class="line">    <span class="comment"># 只支持Insert操作</span></span><br><span class="line">    insert_statement = <span class="string">'INSERT INTO %s (%s) VALUES (%s)'</span> % (</span><br><span class="line">        escape(self.name), col_names, wildcards)</span><br><span class="line">    <span class="keyword">return</span> insert_statement</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> MySQL </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Kafka实践之Consumer</title>
      <link href="/2019/01/01/Kafka%E5%AE%9E%E8%B7%B5%E4%B9%8BConsumer/"/>
      <content type="html"><![CDATA[<p>通过自定义的操作Offset、监控Rebalance行为，进一步加深对Kafka Consumer的理解和使用</p><a id="more"></a><p>本文所涉及的代码详见<a href="https://github.com/YHYR/Kafka-Utils" target="_blank" rel="noopener">Github</a></p><p>上手Kafka Consumer是比较容易的，这里以原生的Java API为例，通常的实现逻辑如下所示：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">consumer</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    String brokers = <span class="string">"localhost:9092"</span>;</span><br><span class="line">    String group = <span class="string">"group_test"</span>;</span><br><span class="line">    String topic = <span class="string">"topic_demo"</span>;</span><br><span class="line"></span><br><span class="line">    Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">    props.put(<span class="string">"bootstrap.servers"</span>, brokers);</span><br><span class="line">    props.put(<span class="string">"group.id"</span>, group);</span><br><span class="line">    props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"true"</span>);</span><br><span class="line">    props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>);</span><br><span class="line">    props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">    props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line"></span><br><span class="line">    KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">    consumer.subscribe(Collections.singletonList(topic));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">        ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">        <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">            System.out.println(record.toString());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对应的maven依赖为</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.kafka<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>kafka-clients<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.0.1<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>可以看到通过配置一些参数，并调用poll方法就可以开始消费消息。如果想要了解Consumer的实现原理，当然绕不开阅读poll方法的源码，本篇先抛开poll的实现原理，可以先简单的理解为：调用poll方法可以实现将Client加入到Group中，获取Partition信息，并且拉去对应Partition上的数据。接下来首先会对Consumer的几个主要的参数做以说明，然后通过几个案例来加深对Consumer的使用。在案例的实现过程中涉及到有过Kafka底层的相关知识，若有不清楚的可先阅读 <a href="https://yhyr.github.io/2018/12/15/Kafka%E7%90%86%E8%AE%BA%E4%B9%8BPartition-Replication/" target="_blank" rel="noopener">Partition &amp; Replication</a>、<a href="https://yhyr.github.io/2018/12/26/Kafka%E7%90%86%E8%AE%BA%E4%B9%8BConsumer-Group-Coordinator/" target="_blank" rel="noopener">Consumer-Group-Coordinator</a> 。</p><p><em>poll()源码的分析可参考<a href="https://matt33.com/2017/11/11/consumer-pollonce/" target="_blank" rel="noopener">poll模型</a> </em></p><h1 id="参数解析"><a href="#参数解析" class="headerlink" title="参数解析"></a>参数解析</h1><h2 id="client-id"><a href="#client-id" class="headerlink" title="client.id"></a>client.id</h2><p>用来指定当前客户端的标识符名称，不同的客户端API会有与之对应的命名规则；例如原生的Java API中client.id的命名规则是以“consumer-”为前缀，后跟自增长Id；在Kafka-Python中，client.id的命名规则为“kafka-python-” + Kafka-Python版本号。通常是不需要业务代码里显示的指定，但是在特定场景下，可以根据client.id来改变Partition的分配结果，从而实现特殊的需求。</p><h2 id="session-timeout-ms"><a href="#session-timeout-ms" class="headerlink" title="session.timeout.ms"></a>session.timeout.ms</h2><p>在无心跳时，broker判定Consumer Client存活的最大时间间隔；即就是在此时间范围内，如果客户端不发送心跳，Coordinator也会认为该Client依旧是存活的；一旦超过该时间Coordinator仍未收到心跳，则会判定该Client已经dead，并主动触发Rebalance；默认时长为3000(3s)。</p><p>在实际应用中，必须斟酌在真实业务场景中消息处理的实际耗时与session.timeout.ms的大小，否则会出现因为处理超时导致在offset还未提交时Coordinator主动触发了Group的Rebalance，从而造成消息重复消费的情况。</p><h2 id="heartbeat-interval-ms"><a href="#heartbeat-interval-ms" class="headerlink" title="heartbeat.interval.ms"></a>heartbeat.interval.ms</h2><p>Consumer Client向Coordinator发送心跳的频率；默认为session.timeout.ms的三分之一</p><h2 id="auto-offset-reset"><a href="#auto-offset-reset" class="headerlink" title="auto.offset.reset"></a>auto.offset.reset</h2><p>控制Consumer读取Offset的方式，可选项为latest(default)或earlist；适用场景：</p><blockquote><p>1, 当没有Offset提交记录时(可以理解为加入新的Group)</p><p>2, 已存在的Offset失效</p></blockquote><p>对于该参数的使用，通常都只关注了场景1的使用，对于场景2的情况，在特定情况下也是非常有用的，具体可详见下文中有关不停服而修改offset位置案例。</p><h2 id="max-poll-records"><a href="#max-poll-records" class="headerlink" title="max.poll.records"></a>max.poll.records</h2><p>指定每调用一次poll方法所能拉取到的数据量；这里所指的数据量是消息的条数，即就是执行一次poll方法，返回的ConsumerRecords的size；默认大小为500；最大拉取量是基于当前客户端所有被分配的Partition而言，而不是每个Partition各五百条。</p><h1 id="Offset-Option"><a href="#Offset-Option" class="headerlink" title="Offset Option"></a>Offset Option</h1><h2 id="Consumer-Special-Offset"><a href="#Consumer-Special-Offset" class="headerlink" title="Consumer Special Offset"></a>Consumer Special Offset</h2><p>在数据监控中普遍存在这样一种场景：查找出某一天或者某几天内的消息用来进行数据验证或者数据嗅探。基于这种需求来讨论一下如果实现：“从2018-12-31当天产生的第一条消息开始，重新消费Topic(topic_demo)”。</p><p>首先获取2018-12-31(1546185600000)当天记录的第一条消息的offset值</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list  localhost:9092 --topic &lt;string&gt; --time 1546185600000</span><br></pre></td></tr></table></figure><p>为了方便举例，假设Topic只有一个分区。则通过执行如上命令可以得到topic对应的分区0在2018年12月31号这天所记录的第一条消息的offset值(还可以通过代码实现获取offset值，详见<a href="https://github.com/YHYR/Kafka-Utils/blob/master/Kafka-Utils-Java/src/main/java/com/yhyr/comsumer/GetOffsetWithTimestamp.java" target="_blank" rel="noopener">Java版</a>、<a href="https://github.com/YHYR/Kafka-Utils/blob/master/Kafka-Utils-Python/consumer/get_offset_with_timestamp.py" target="_blank" rel="noopener">Python版</a>)；假设该值等于500，则基于指定Offset进行消费的代码(Java)如下所示</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">consumerSpecialOffset</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  String brokers = <span class="string">"localhost:9092"</span>;</span><br><span class="line">  String topic = <span class="string">"topic_demo"</span>;</span><br><span class="line">  String group = <span class="string">"group_test"</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 自定义消费的位置</span></span><br><span class="line">  <span class="keyword">int</span> customPartitionOffset = <span class="number">500</span>;</span><br><span class="line"></span><br><span class="line">  Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">  props.put(<span class="string">"bootstrap.servers"</span>, brokers);</span><br><span class="line">  props.put(<span class="string">"group.id"</span>, group);</span><br><span class="line">  props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"false"</span>); <span class="comment">// 不提交Offset</span></span><br><span class="line">  props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>);</span><br><span class="line">  props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">  props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">  KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">         * KafkaConsumer.seek 是基于客户端的行为;</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * 因此在订阅Topic后, 必须调用一次poll方法, 用来加入指定Group并获取Client被分配的Partition, 方可通过seek重新指定消费位置</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">  consumer.subscribe(Collections.singletonList(topic));</span><br><span class="line">  consumer.poll(<span class="number">0</span>);</span><br><span class="line">  consumer.assignment().forEach(topicPartition -&gt; consumer.seek(topicPartition, customPartitionOffset));</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">    ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">    <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">      System.out.println(record.toString());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>因为这里只是数据的嗅探测试，并不想改变Broker端存储的Offset信息，所以这里指定enable.auto.commit为false；只消费不提交；当然如果是在生产环境中，为了尽可能的避免或者减少对线上服务的印象，可以优先考虑指定一个新的Group，这样就可以避免因为新增Consumer而导致的Rebalance操作。通过KafkaConsumer.seek()方法来改变和指定当前Consumer的Offset位置，这种操作不会影响到其他Client，且仅对本次有效。</p><p>如果要想实现获取指定时间窗的消息，其实也简单；实现的方法也很多，这里简单介绍两种</p><p>方法一：通过Offset来获取时间窗内的数据</p><p>分别获取时间窗前后的Offset临界值，然后通过对ConsumerRecord中的Offset值进行判断从而筛选数据</p><p>方法二：通过时间戳来获取指定时间窗内的数据</p><p>直接通过对比ConsumerRecord中的timestamp和目标时间窗来实现数据的筛选</p><h2 id="Commit-Special-Offset"><a href="#Commit-Special-Offset" class="headerlink" title="Commit Special Offset"></a>Commit Special Offset</h2><p><strong>在不触发Rebalance的前提下</strong>修改Broker端存储的Offset值</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commitSpecialOffset</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    String brokers = <span class="string">"localhost:9092"</span>;</span><br><span class="line">    String group = <span class="string">"group_test"</span>;</span><br><span class="line">    String topic = <span class="string">"topic_demo"</span>;</span><br><span class="line"></span><br><span class="line">    Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">    props.put(<span class="string">"bootstrap.servers"</span>, brokers);</span><br><span class="line">    props.put(<span class="string">"group.id"</span>, group);</span><br><span class="line">    props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"true"</span>);</span><br><span class="line">    props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>);</span><br><span class="line">    props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">    props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">    KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取Topic的Partition信息</span></span><br><span class="line">    List&lt;PartitionInfo&gt; partitionInfos = consumer.partitionsFor(topic);</span><br><span class="line">    <span class="comment">// 将所有Partition的offset设置为10</span></span><br><span class="line">    <span class="keyword">int</span> resetOffsetValue = <span class="number">10</span>;</span><br><span class="line">    partitionInfos.forEach(partitionInfo -&gt; currentOffsets.put(<span class="keyword">new</span> TopicPartition(partitionInfo.topic(),</span><br><span class="line">        partitionInfo.partition()), <span class="keyword">new</span> OffsetAndMetadata(resetOffsetValue)));</span><br><span class="line">    consumer.commitSync(currentOffsets);</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述例子的应用场景比较局限，仅限于指定Group下Partition未被其他Client所占用的时候才可以修改；但是在实际的应用场景中，很有可能存在这种场景：因为脏数据需要修改Offset的位置，但是又不能停止服务。</p><p>例如Group(group_test)下的Topic(topic_demo)只有一个Partition，正在被服务A所占用；分区topic_demo-0的有效offset范围为100 ~ 1000；且从500处开始一直到1000，所有的消息均是脏数据，现在需要在不停服的前提下，将Offset的位置改变成最新状态。</p><p>要实现<font color="red">不停服更新Offset</font>，就需要了解Consumer Rebalance和Partition Assignment；这里简单阐述一下(有关Rebalance的相关知识、分区分区策略的原理详见 <a href="https://yhyr.github.io/2018/12/26/Kafka%E7%90%86%E8%AE%BA%E4%B9%8BConsumer-Group-Coordinator/" target="_blank" rel="noopener">Consumer-Group-Coordinator</a> )：</p><p>Rebalance：Rebalance是基于Group而言的，一个Group中Consumer个数的变化会触发Rebalance；Group在Rebalance期间对外表现为不可用；没经过一次Rebalance，都会按照客户端的分区分配策略来分发客户端将要处理的Partition。</p><p>分区分配策略：Kafka的分区分配策略是用Consumer来决定的，默认的分区分配原则为RangeAssignor，即就是对于每个被订阅的Topic，尽可能将连续的Partition分给同一个Consumer Client。分配算法如下所示：</p><ul><li>1，将Consumer Client按照命名排序</li><li>2，计算平均每个Consumer可以分到的Partition个数 =&gt; n / m</li><li>3，计算平均分配后剩余的Partition个数 =&gt; n % m</li><li>4，如果n%m等于零，则代表所有客户端可以一次平均分配到n/m个Partition；如果n%m大于零，则前n%m个Consumer分配到(n / m + 1)个Partition，剩下的Consumer(m - n % m)分配(n / m)个Partition</li></ul><p>结合上述两点就可以很容易实现不停服更新Offset的需求：即就是只要保证我的Offset更新服务B的client.id在排序上位于服务A的client.id，这样就可以保证在Rebalance之后一定能分配到该Partition，从而来执行Offset的更新操作(Offset的更新可以分为两种：直接将Offset指定为最大值；或者将Offset设定为一个无效值；这里采用第二种方案，因为Partition的有效Offset范围为100 ~ 1000，所以将Offset设置为10，则会因为auto.offset.reset=true的配置自动将Offset重新设置为latest)；当更新完成并安全退出后，再次触发Rebalance，此时Group中只剩下服务A，当服务A再次获取到当前Partition时，根据服务A的配置 <code>auto.offset.reset=true</code>，就可以保证此时的Offset为最新位置。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">commitSpecialOffsetTriggerRebalance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    String brokers = <span class="string">"localhost:9092"</span>;</span><br><span class="line">    String group = <span class="string">"group_test"</span>;</span><br><span class="line">    String topic = <span class="string">"topic_demo"</span>;</span><br><span class="line"></span><br><span class="line">    Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">    <span class="comment">// 指定client.id的值, 保证在排序时在首位</span></span><br><span class="line">    String clientId = <span class="string">"aaa"</span>;</span><br><span class="line">    props.put(<span class="string">"client.id"</span>, clientId);</span><br><span class="line">    props.put(<span class="string">"bootstrap.servers"</span>, brokers);</span><br><span class="line">    props.put(<span class="string">"group.id"</span>, group);</span><br><span class="line">    props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"true"</span>);</span><br><span class="line">    props.put(<span class="string">"auto.offset.reset"</span>, <span class="string">"latest"</span>);</span><br><span class="line">    props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">    props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">    KafkaConsumer&lt;String, String&gt; consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 触发Rebalance</span></span><br><span class="line">    consumer.subscribe(Collections.singletonList(topic));</span><br><span class="line">    consumer.poll(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 获取Topic的Partition信息</span></span><br><span class="line">    List&lt;PartitionInfo&gt; partitionInfos = consumer.partitionsFor(topic);</span><br><span class="line">    <span class="comment">// 将所有Partition的offset设置为10</span></span><br><span class="line">    <span class="keyword">int</span> resetOffsetValue = <span class="number">10</span>;</span><br><span class="line">    partitionInfos.forEach(partitionInfo -&gt; currentOffsets</span><br><span class="line">        .put(<span class="keyword">new</span> TopicPartition(partitionInfo.topic(), partitionInfo.partition()),</span><br><span class="line">            <span class="keyword">new</span> OffsetAndMetadata(resetOffsetValue)));</span><br><span class="line">    consumer.commitSync(currentOffsets);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>因为由Rebalance造成的服务不可用时间通常都比较短，且远小于停服重启所带来的影响；假设由Rebalance造成的不可用时长是可以接受的，这样就可以基于上述方案实现Offset的修改。</p><h1 id="Consumer-Rebalance-Listener"><a href="#Consumer-Rebalance-Listener" class="headerlink" title="Consumer Rebalance Listener"></a>Consumer Rebalance Listener</h1><p>Rebalance的设计很好的提升了Kafka的容错率和可扩展性；但并非所有的Rebalance都是我们期望的。在介绍Consumer参数时有说到一个参数session.timeout.ms，在session.timeout.ms内没有发送心跳就会触发Rebalance。在实际应用中，造成心跳未按时发送的原因很多，网络、消息处理延迟等等，并不是所有的未发送我们都期望进行Rebalance，那么该如何消除这种情况呢？增大session.timeout.ms的时间区间是一个很直接办法，但并不是一个有效的办法，因为时间区间的界定是个难度比较大的问题。太小还是容易触发Rebalance，但是太大肯定是不合理。所以在这里介绍一种：通过客户端监听Rebalance行为来自定义控制Offset、告警，从而解决非正常Rebalance导致的消息重复问题，也可以快速感知到问题的发生。</p><p>例如有如下场景，当一次性拉取多条数据，且在数据处理未完全处理完时，发生Rebalance导致新分配的Client基于从原始的消息位置开始消费，从而导致数据的重复消费问题</p><p><img src="./offset-消息重复.png" alt="offset-消息重复"></p><p><em>图片来自Kafka权威指南 Chapter 4</em></p><p>要解决该问题，可以通过监听Consumer的Rebalance行为，在发生前将该处理的操作及时的处理完，样例代码如下所示，每次Rebalance前打印当前的Offset信息</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> Map&lt;TopicPartition, OffsetAndMetadata&gt; currentOffsets = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> String brokers = <span class="string">"localhost:9092"</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> String group = <span class="string">"group_test"</span>;</span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> String topic = <span class="string">"topic_demo"</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> KafkaConsumer&lt;String, String&gt; consumer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">static</span> &#123;</span><br><span class="line">    Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">    props.put(<span class="string">"bootstrap.servers"</span>, brokers);</span><br><span class="line">    props.put(<span class="string">"group.id"</span>, group);</span><br><span class="line">    props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"true"</span>);</span><br><span class="line">    props.put(<span class="string">"auto.commit.interval.ms"</span>, <span class="string">"1000"</span>);</span><br><span class="line">    props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">    props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">    consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomHandleRebalance</span> <span class="keyword">implements</span> <span class="title">ConsumerRebalanceListener</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsRevoked</span><span class="params">(Collection&lt;TopicPartition&gt; collection)</span> </span>&#123;</span><br><span class="line">        System.out.println(String.format(</span><br><span class="line">            <span class="string">"Before Rebalance, Assignment partitions is: %s; Current each partition's latest offset is: %s"</span>,</span><br><span class="line">            collection.toString(), currentOffsets.toString()));</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsAssigned</span><span class="params">(Collection&lt;TopicPartition&gt; collection)</span> </span>&#123;</span><br><span class="line">        System.out.println(String.format(</span><br><span class="line">            <span class="string">"After Rebalance, Assignment partitions is: %s; Current each partition's latest offset is: %s"</span>,</span><br><span class="line">            collection.toString(), currentOffsets.toString()));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">consumer</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        consumer.subscribe(Collections.singletonList(topic), <span class="keyword">new</span> CustomHandleRebalance());</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                <span class="comment">// record current msg's offset</span></span><br><span class="line">                currentOffsets.put(<span class="keyword">new</span> TopicPartition(record.topic(), record.partition()), <span class="keyword">new</span> OffsetAndMetadata(</span><br><span class="line">                    record.offset()));</span><br><span class="line">                <span class="comment">// processing msg</span></span><br><span class="line">                System.out.println(<span class="string">"Processing msg : "</span> + record.toString());</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e) &#123;</span><br><span class="line">        System.out.println(<span class="string">"Unexpected error: "</span> + e.getMessage());</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        consumer.commitSync(currentOffsets);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Kafka实践之常用命令</title>
      <link href="/2019/01/01/Kafka%E5%AE%9E%E8%B7%B5%E4%B9%8B%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
      <content type="html"><![CDATA[<p>常用的命令汇总</p><a id="more"></a><h1 id="Topic"><a href="#Topic" class="headerlink" title="Topic"></a>Topic</h1><h2 id="新建Topic"><a href="#新建Topic" class="headerlink" title="新建Topic"></a>新建Topic</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper &lt;zookeeper connect&gt; --create --topic &lt;string&gt; --replication-factor &lt;integer&gt; --partitions &lt;integer&gt;</span><br></pre></td></tr></table></figure><h2 id="删除Topic"><a href="#删除Topic" class="headerlink" title="删除Topic"></a>删除Topic</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper &lt;zookeeper connect&gt; --delete --topic &lt;string&gt;</span><br></pre></td></tr></table></figure><h2 id="查看Topic列表"><a href="#查看Topic列表" class="headerlink" title="查看Topic列表"></a>查看Topic列表</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper &lt;zookeeper connect&gt; --list</span><br></pre></td></tr></table></figure><h2 id="查看所有Topic的详细信息"><a href="#查看所有Topic的详细信息" class="headerlink" title="查看所有Topic的详细信息"></a>查看所有Topic的详细信息</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper &lt;zookeeper connect&gt; --describe</span><br></pre></td></tr></table></figure><h2 id="增加Partition个数"><a href="#增加Partition个数" class="headerlink" title="增加Partition个数"></a>增加Partition个数</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper &lt;zookeeper connect&gt; --alter --topic &lt;string&gt; --partitions &lt;integer&gt;</span><br></pre></td></tr></table></figure><h2 id="查看指定Topic的详细信息"><a href="#查看指定Topic的详细信息" class="headerlink" title="查看指定Topic的详细信息"></a>查看指定Topic的详细信息</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper &lt;zookeeper connect&gt; --describe --topic &lt;string&gt;</span><br></pre></td></tr></table></figure><p>相比于上一条查看所有topic的信息命令，查看指定topic的信息具有更切实的实际作用，更有利于在实际工作中快速定位和发现问题(推荐使用)。通过该命令可以查看给定Topic的分区、副本集个数；以及各分区、副本集的实际分布情况，同时还可以看到每个分区的ISR列表信息(有关ISR介绍可详见<a href="https://yhyr.github.io/2018/12/15/Kafka%E7%90%86%E8%AE%BA%E4%B9%8BPartition-Replication/" target="_blank" rel="noopener">ISR</a>)。通过该命令可以根据各Partition的ISR情况分析Broker状况。执行结果如下所示</p><p><img src="./查看Topic详细信息.png" alt="查看Topic详细信息"></p><h2 id="查看副本集同步出现异常的分区"><a href="#查看副本集同步出现异常的分区" class="headerlink" title="查看副本集同步出现异常的分区"></a>查看副本集同步出现异常的分区</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper &lt;zookeeper connect&gt; --describe --under-replicated-partitions</span><br></pre></td></tr></table></figure><h2 id="查看缺失leader的分区"><a href="#查看缺失leader的分区" class="headerlink" title="查看缺失leader的分区"></a>查看缺失leader的分区</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper &lt;zookeeper connect&gt; --describe --unavailable-partitions</span><br></pre></td></tr></table></figure><h2 id="查看Topic各Partition的offset极值"><a href="#查看Topic各Partition的offset极值" class="headerlink" title="查看Topic各Partition的offset极值"></a>查看Topic各Partition的offset极值</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-run-class.sh kafka.tools.GetOffsetShell --broker-list  &lt;address:port&gt; --topic &lt;string&gt; --time &lt;param&gt;</span><br></pre></td></tr></table></figure><p>time参数详解</p><blockquote><p>-1：查看Partition的最大Offset，即就是latest</p><p>-2：查看Partition的最小Offset，即就是earlist</p><p>timestamp：查询满足时间戳的消息所在Segment的最早Offset值；详见 <a href="https://yhyr.github.io/2019/01/23/Kafka-Timestamp/" target="_blank" rel="noopener">Kafka Timestamp</a></p></blockquote><h1 id="Consumer-Group"><a href="#Consumer-Group" class="headerlink" title="Consumer Group"></a>Consumer Group</h1><p>因为Consumer的版本有新旧之分，且旧版本(Scala)的Consumer Client是依赖于Zookeeper来保存Offset的，而新版本(Java)的则基于内置的topic(__consumer_offsets )来保存offset，所以在使用Group相关命令时，必须根据实际情况确定Consumer的版本，即就是：旧版本的查询只需要指定Zookeeper参数即可，而新版本的查询需要指定–new-consumer参数的同时，指定bootstrap-server参数</p><h2 id="查看Group列表"><a href="#查看Group列表" class="headerlink" title="查看Group列表"></a>查看Group列表</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-consumer-groups.sh --new-consumer --bootstrap-server &lt;address:port&gt; --list</span><br></pre></td></tr></table></figure><h2 id="查看指定Group下各Topic对应Partition的实际消费情况"><a href="#查看指定Group下各Topic对应Partition的实际消费情况" class="headerlink" title="查看指定Group下各Topic对应Partition的实际消费情况"></a>查看指定Group下各Topic对应Partition的实际消费情况</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-consumer-groups.sh --new-consumer --bootstrap-server &lt;address:port&gt; --group &lt;name&gt; --describe</span><br></pre></td></tr></table></figure><p>通过该命令可以了解到给定Group下每个Topic各Partition的最大Offset值，当前所处的消费位置；还显示出当前Partition所操作的客户端信息，LAG表示消费滞后的情况；执行结果如下所示：</p><p><img src="./查看group下topic的消费情况.png" alt="查看group下topic的消费情况"></p><h1 id="Console-Option"><a href="#Console-Option" class="headerlink" title="Console Option"></a>Console Option</h1><h2 id="控制台消费数据"><a href="#控制台消费数据" class="headerlink" title="控制台消费数据"></a>控制台消费数据</h2><p>旧版本消费</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --zookeeper &lt;zookeeper connect&gt; --topic &lt;string&gt; --from-beginning</span><br></pre></td></tr></table></figure><p>新版本消费</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-consumer.sh --bootstrap-server &lt;address:port&gt; --topic &lt;string&gt; --from-beginning</span><br></pre></td></tr></table></figure><h2 id="控制台生产数据"><a href="#控制台生产数据" class="headerlink" title="控制台生产数据"></a>控制台生产数据</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-console-producer.sh --broker-list &lt;address:port&gt; --topic &lt;string&gt;</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Kafka理论之Consumer Group &amp; Coordinator</title>
      <link href="/2018/12/26/Kafka%E7%90%86%E8%AE%BA%E4%B9%8BConsumer-Group-Coordinator/"/>
      <content type="html"><![CDATA[<p>归纳总结Group Cooridinator的基本作用，Partition的分配策略和分配机制；了解Rebalance的触发机制和主动监控；</p><a id="more"></a><h1 id="Consumer-Group"><a href="#Consumer-Group" class="headerlink" title="Consumer Group"></a>Consumer Group</h1><p>提及Consumer Group，最先想到的就是Group与Consumer Client的关联关系：</p><ul><li>1，Consumer Group用group.id(String)作为全局唯一标识符</li><li>2，每个Group可以有零个、一个或多个Consumer Client</li><li>3，每个Group可以管理零个、一个或多个Topic</li><li>4，Group下每个Consumer Client可同时订阅Topic的一个或多个Partition</li><li>5，Group下同一个Partition只能被一个Client订阅，多Group下的Client订阅不受影响</li></ul><p>Consumer Group的作用主要有：管理Partition的Offset信息；管理Consumer Client与Partition的分配。正因为所有Partition的Offset信息是由Group统一管理，所以如果一个Partition有多个Consumer，那么每个Consumer在该Partition上的Offset很可能会不一致，这样会导致在Rebalance后赋值处理的Client的消费起点发生混乱；与此同时，这种场景也不符合Kafka中Partition消息消费的一致性；因此在同一Group下一个Partition只能对应一个Consumer Client。</p><p>接下来将通过介绍Group的管理者Coordinator来了解Group是如何管理Offset；此外通过介绍Group的Rebalance机制了解Partition分配的原理，并介绍如何通过代码实现Rebalance的监控。下图是基于自己的理解绘制的逻辑图，有不对的地方还请指正：</p><p><img src="./group-coordinator逻辑图.png" alt="group-coordinator逻辑图"></p><h1 id="Group-Coordinator"><a href="#Group-Coordinator" class="headerlink" title="Group Coordinator"></a>Group Coordinator</h1><p>Group Coordinator是一个服务，每个Broker在启动的时候都会启动一个该服务。Group Coordinator的作用是用来存储Group的相关Meta信息，并将对应Partition的Offset信息记录到Kafka内置Topic(__consumer_offsets)中。Kafka在0.9之前是基于Zookeeper来存储Partition的Offset信息(consumers/{group}/offsets/{topic}/{partition})，因为ZK并不适用于频繁的写操作，所以在0.9之后通过内置Topic的方式来记录对应Partition的Offset。</p><p>每个Group都会选择一个Coordinator来完成自己组内各Partition的Offset信息，选择的规则如下：</p><ul><li>1，计算Group对应在__consumer_offsets上的Partition</li><li>2，根据对应的Partition寻找该Partition的leader所对应的Broker，该Broker上的Group Coordinator即就是该Group的Coordinator</li></ul><p>Partition计算规则</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">partition-Id(__consumer_offsets) = Math.abs(groupId.hashCode() % groupMetadataTopicPartitionCount)</span><br></pre></td></tr></table></figure><p>其中groupMetadataTopicPartitionCount对应offsets.topic.num.partitions参数值，默认值是50个分区</p><p>查看指定Partition各Replica的分布情况</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/kafka-topics.sh --zookeeper &lt;address:port&gt; --topic __consumer_offsets --describe</span><br></pre></td></tr></table></figure><h2 id="Group-Coordinator-Generation"><a href="#Group-Coordinator-Generation" class="headerlink" title="Group Coordinator Generation"></a>Group Coordinator Generation</h2><p>Group Coordinaror是Group Rebalance中不可或缺的一环，因为Coordinator负责记录了每次Rebalance后的Partition分配结果，因此Kafka为Coordinator赋予了一个Generation的概念。Generation(谷歌翻译为“代”)可以通俗的理解为版本的概念，Generation未开始时值为-1，在第一次Rebalance后Group进入Stable状态时值为1，此后每发生一次Rebalance，Generation的Id会自增长加1。</p><p><img src="./generation源码.png" alt="generation源码"></p><p><img src="./Group_generation版本信息.png" alt="Group_generation版本信息"></p><p>网上有的人把Generation归结为Group的概念，说Group是有版本(代)的概念，个人觉得这是不太恰当的，因为从源码层面看，Generation是抽象类AbstractCoordinator的内部类；个人感觉Group更像是一个逻辑侧面的概念，用来规范、管理一些消费端抽象抽来的一种约束手段。</p><p><img src="./Coordinator-Generation类图.png" alt="Coordinator-Generation类图"></p><h1 id="Partition-Assignment"><a href="#Partition-Assignment" class="headerlink" title="Partition Assignment"></a>Partition Assignment</h1><p>Partition的分配策略和分配执行均是有Consumer Client完成的，而不是由Server(Group Coordinator)决定、执行的。因为如果换做用Server端实现，则不仅会增加Broker的负担，同时无法灵活的改变分配策略。</p><h2 id="Assignment-Strategy"><a href="#Assignment-Strategy" class="headerlink" title="Assignment Strategy"></a>Assignment Strategy</h2><p>Kafka目前支持RangeAssignor、RoundRobinAssignor两种内置的分配策略，在0.11.x以后还内置了一种分配策略StickyAssignor；当然因为Partition的分配策略是有Client控制的，所以Kafka支持用户自定义分配策略。这里详细介绍一下前两种分配策略</p><h3 id="RangeAssignor"><a href="#RangeAssignor" class="headerlink" title="RangeAssignor"></a>RangeAssignor</h3><p>按照范围进行划分：对于每个被订阅的Topic，尽可能将连续的Partition分给同一Consumer Client。假设一个Group下有m个Consumer同时订阅一个Topic，该Topic有n个Partition，则具体的分配算法如下：</p><ul><li>1，将Consumer Client按照命名排序</li><li>2，计算平均每个Consumer可以分到的Partition个数 =&gt; n / m</li><li>3，计算平均分配后剩余的Partition个数 =&gt; n % m</li><li>4，如果n%m等于零，则代表所有客户端可以一次平均分配到n/m个Partition；如果n%m大于零，则前n%m个Consumer分配到(n / m + 1)个Partition，剩下的Consumer(m - n % m)分配(n / m)个Partition</li></ul><p><font color="red">RangeAssignor分配策略是基于一个Topic而已的，如果同时订阅多个Topic，则分别对每个Topic进行Range分配</font>，接下来举三个例子加以阐述：</p><p>Eg1：有两个Consumer(分别为C1，C2)，同时订阅两个Topic(T1, T2)，每个Topic都有4个Partition，则最终的分配结果为：</p><blockquote><p>C1：T1P0，T1P1，T2P0，T2P1</p></blockquote><blockquote><p>C2：T1P2，T1P3，T2P2，T2P3</p></blockquote><p>Eg2：有两个Consumer(分别为C1，C2)，同时订阅两个Topic(T1, T2)，每个Topic都有3个Partition，则最终的分配结果为：</p><blockquote><p>C1：T1P0，T1P1，T2P0，T2P1</p></blockquote><blockquote><p>C2：T1P2，T2P2</p></blockquote><p>Eg3：有两个Consumer(分别为C1，C2)，同时订阅两个Topic(T1, T2)，T1有3个Partition，T2有4个Partition，则最终的分配结果为：</p><blockquote><p>C1：T1P0，T1P1，T2P0，T2P1</p></blockquote><blockquote><p>C2：T1P2，T2P2，T2P3</p></blockquote><p><img src="./range_Eg_3.jpg" alt="range_Eg_3"></p><h3 id="RoundRobinAssignor"><a href="#RoundRobinAssignor" class="headerlink" title="RoundRobinAssignor"></a>RoundRobinAssignor</h3><p>轮询分配：将Group内所有Topic的Partition合并到一起，然后按顺序依次将Partition分配给所有订阅该Topic的Consumer(按照name排序)；如果某个Topic只有一个Consumer订阅，则该Consumer将独自订阅该Topic下的所有Partition。在上述三种场景下的分配结果分别为</p><p>Eg1 =&gt; C1：T1P0，T1P2，T2P0， T2P2；C2：T1P1，T1P3，T2P1，T2P3</p><p>Eg2 =&gt; C1：T1P0，T1P2，T2P1；C2：T1P1，T2P0，T2P2</p><p>Eg3 =&gt; C1：T1P0，T1P2，T2P1，T2P3；C2：T1P1，T2P0，T2P2</p><h2 id="Assignment-Principle"><a href="#Assignment-Principle" class="headerlink" title="Assignment Principle"></a>Assignment Principle</h2><ul><li>Step 1，GCR(GroupCoordinatorRequest)</li><li>Step 2，JGR(JoinGroupRequest)</li><li>Step 3，SGR(SyncGroupRequest)</li></ul><p>首先根据GroupId选择对应的Coordinator(Step1)，然后Group内所有的Consumer Client向Coordinator发送JoinGroup请求，此时Coordinator会从所有Client中选择一个作为leader，其他的作为follower；leader会根据客户端所指定的分区分配策略执行分配任务，并将最终的分配结果发送到Coordinator(Step 2)；最后所有的客户端向Coordinator发送SyncGroup请求，用于获取Partition的分区结果(Step 3)。</p><p>分配逻辑的源码详解可参考<a href="https://blog.csdn.net/chunlongyu/article/details/52791874" target="_blank" rel="noopener">Kafka源码深度解析－序列7 －Consumer －coordinator协议与heartbeat实现原理</a></p><h1 id="Rebalance"><a href="#Rebalance" class="headerlink" title="Rebalance"></a>Rebalance</h1><p>Rebalance是一个分区-客户端重分配协议。旨在特定条件下，基于给定的分配策略来为Group下所有Consumer Client重新分配所要订阅的Partition。Rebalance是Consumer Group中一个重要的特性，也为Group提供了High Availability and Scalability。但同样Rebalance也存在相应的弊端：在Rebalance期间，整个Group对外不可用。</p><ul><li><p>Rebalance 触发条件</p><ul><li>Group中有新Consumer加入</li><li>Group中已有的Consumer挂掉</li><li>Coordinator挂了，集群选出新Coordinator</li><li>Topic新增Partition个数</li><li>Consumer Client调用unsubscrible()，取消订阅Topic</li></ul></li><li><p>Rebalance 过程</p><p>Rebalance的本质即就是Partition的分配；首先客户端会向Coordinator发送JGR，等待leader发送Partition分配结果到Coordinator后，然后再向Coordinator发送SGR获取分配结果。</p></li></ul><p>Kafka通过Heartbeats(心跳)的方式实现Consumer Client与Coordinator之间的通信，用来相互告知对方的存在。如果Coordinator挂掉导致的Rebalance，则Kafka会重新选择一个Coordinator，然后所有的Client会执行JGR、SGR；如果由于Client的变化导致Rebalance，则会通知有效Client进行JGR、SGR。</p><blockquote><p>session.timeout.ms：Consumer Session过期时间；默认为10000(10s)；这个值的大小必须介于broker configuration中的group.min.session.timeout.ms 与 group.max.session.timeout.ms之间。</p><p>heartbeat.interval.ms： 心跳间隔；默认为3000(3s)；通常设置值低于session.timeout.ms的1/3</p></blockquote><p><em>后续会对消费中遇到的所有时间戳做进一步的归纳整理</em></p><h2 id="Rebalance-Listener"><a href="#Rebalance-Listener" class="headerlink" title="Rebalance Listener"></a>Rebalance Listener</h2><p>因为触发Rebalance的可能性太多，并且在实际的工作中并不是所有的Rebalance都是有益的，所以可以在代码层面实现对Rebalance的监控，从而根据真实的业务场景做出相应的对策。这里贴出监控Demo：通过在客户端维护Offset信息可以自定义控制消息的commit，尽可能保证Exactly Once语义，避免重复消费。</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * ConsumerRebalanceListener: 监听客户端Rebalance 包含两个方法onPartitionsRevoked和onPartitionsAssigned</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * onPartitionsRevoked： 在客户端停止消费消息后、在Rebalance开始前调用可以在此时提交offset信息、保证在Rebalance后的consumer可以准确知晓Partition的消费起点</span></span><br><span class="line"><span class="comment"> * onPartitionsAssigned：在Rebalance完成后调用</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> yhyr</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ConsumerRebalanceListenerDemo</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Map&lt;TopicPartition, OffsetAndMetadata&gt; currentOffsets = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> List&lt;String&gt; topics = Collections.singletonList(<span class="string">"demoTopic"</span>);</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> KafkaConsumer&lt;String, String&gt; consumer;</span><br><span class="line">    <span class="keyword">static</span> &#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(<span class="string">"bootstrap.servers"</span>, <span class="string">"localhost:9092"</span>);</span><br><span class="line">        props.put(<span class="string">"group.id"</span>, <span class="string">"group_test"</span>);</span><br><span class="line">        props.put(<span class="string">"enable.auto.commit"</span>, <span class="string">"true"</span>);</span><br><span class="line">        props.put(<span class="string">"auto.commit.interval.ms"</span>, <span class="string">"1000"</span>);</span><br><span class="line">        props.put(<span class="string">"key.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        props.put(<span class="string">"value.deserializer"</span>, <span class="string">"org.apache.kafka.common.serialization.StringDeserializer"</span>);</span><br><span class="line">        consumer = <span class="keyword">new</span> KafkaConsumer&lt;&gt;(props);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">CustomHandleRebalance</span> <span class="keyword">implements</span> <span class="title">ConsumerRebalanceListener</span> </span>&#123;</span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsRevoked</span><span class="params">(Collection&lt;TopicPartition&gt; collection)</span> </span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"Before Rebalance, Assignment partitions is : "</span> + collection.toString());</span><br><span class="line">            System.out.println(<span class="string">"Before Rebalance, Each partition's lastest consumer offset : "</span></span><br><span class="line">                + currentOffsets.toString());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="meta">@Override</span></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onPartitionsAssigned</span><span class="params">(Collection&lt;TopicPartition&gt; collection)</span> </span>&#123;</span><br><span class="line">            System.out.println(<span class="string">"After Rebalance, Assignment partitions is : "</span> + collection.toString());</span><br><span class="line">            System.out.println(<span class="string">"After Rebalance, Each partition's lastest consumer offset : "</span></span><br><span class="line">                + currentOffsets.toString());</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">consumer</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 通过自定义Rebalance监听方式来订阅Topic</span></span><br><span class="line">        consumer.subscribe(topics, <span class="keyword">new</span> CustomHandleRebalance());</span><br><span class="line">        <span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">            ConsumerRecords&lt;String, String&gt; records = consumer.poll(<span class="number">100</span>);</span><br><span class="line">            <span class="keyword">for</span> (ConsumerRecord&lt;String, String&gt; record : records) &#123;</span><br><span class="line">                <span class="comment">// deal with msg</span></span><br><span class="line">                System.out.println(<span class="string">"Current Processing msg info : "</span> + record.toString());</span><br><span class="line">                <span class="comment">// increase offset</span></span><br><span class="line">                currentOffsets.put(<span class="keyword">new</span> TopicPartition(record.topic(), record.partition()),</span><br><span class="line">                    <span class="keyword">new</span> OffsetAndMetadata(record.offset() + <span class="number">1</span>));</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// submit offset by consumer.commitSync() or consumer.commitAsync() if you need; Default kafka auto commit</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        ConsumerRebalanceListenerDemo action = <span class="keyword">new</span> ConsumerRebalanceListenerDemo();</span><br><span class="line">        action.consumer();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="Group-State"><a href="#Group-State" class="headerlink" title="Group State"></a>Group State</h1><p>最后贴一个大牛整理的Group状态机，其中的各种状态转换还未对其加以实践；以后有机会结合源码，再来做进一步的学习</p><p><img src="/images/placeholder.png" alt="GroupStat" data-src="./GroupStat.png" class="lazyload"></p><p><em>图片来<a href="https://matt33.com/2017/01/16/kafka-group/" target="_blank" rel="noopener">Matt’s Blog</a></em></p><p><em>参考</em></p><p><em><a href="https://pan.baidu.com/s/1Ol3vd1Ehx8EC8yWAufPjWA" target="_blank" rel="noopener">Kafka权威指南</a></em></p><p><em><a href="https://blog.csdn.net/zhanyuanlin/article/details/76021614" target="_blank" rel="noopener">Consumer分区分配策略</a></em></p><p><em><a href="https://www.cnblogs.com/huxi2b/p/6223228.html" target="_blank" rel="noopener">Consumer Group介绍</a></em></p>]]></content>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Linux磁盘空间释放</title>
      <link href="/2018/12/22/Linux%E7%A3%81%E7%9B%98%E7%A9%BA%E9%97%B4%E9%87%8A%E6%94%BE/"/>
      <content type="html"><![CDATA[<p>基于文件伪删除的问题，进一步学习和了解Linux相关的命令和文件系统</p><a id="more"></a><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>在近期的实际工作中，遇到过几次因为磁盘空间满而导致服务不可用的情况，所以免不了要对系统进行清理。 在最开始的几次清理过程中，通过删除一些大日志文件可以得到立竿见影的效果，所以就没怎么注意；但是在最近一次的清理过程中，发现根目录的使用率已经到达百分百，但是并没有在根目录下发现有什么大文件，所以无法仅通过<code>rm -rf ***</code>这种简单粗暴的方式来解决问题。</p><h1 id="问题分析"><a href="#问题分析" class="headerlink" title="问题分析"></a>问题分析</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>说起<font color="red">被删除文件的磁盘空间并未释放</font>问题，还存在一个与之类似的说法：<font color="red">df、du结果不一致</font>，这两个问题在很大程度上指的是一回事：即就是被删除文件在被执行rm命令时，还有其他进程在操作该文件；所以该文件只是被标记为“deleted”，而非立马被删除；除非等到操作该文件的所有进程都结束后该文件才会被删除。</p><h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><p>俗话说好记性不如赖笔头，既然知道了原因，那就亲自实践一下，在验证结论正确与否的同时加深记忆也未尝不可。</p><p>首先执行命令<code>df -h</code>，可以看到当前机器的根目录的大小为48G，可用空间为35G；</p><p><img src="./磁盘空间.png" alt="磁盘空间"></p><p>然后在当前用户目录(/home/yhyr)下新建一个数据文件，为了提升试验的效果，这里生成文件的大小为1G；数据文件生成 命令<code>dd if=/dev/zero of=./data_file bs=1M count=1024</code>；然后分别查看当前目录的文件大小信息<code>du -sh ./*</code>和磁盘的占用情况<code>df -h</code></p><p><img src="./查看新建数据文件.png" alt="查看新建数据文件"></p><p><img src="./新增文件后的磁盘空间.png" alt="新增文件后的磁盘空间"></p><p>可以看到此时相较于之前35G的可用空间，现在已经少了一个G，符合我们的预期。接下来分别验证一下有无进程操作下，删除数据文件后磁盘空间的使用情况：</p><p>场景一：当没有任何进程操作该数据文件时，执行<code>rm -f data_file</code>后磁盘空间成功释放了1G的空间，执行结果如下所示：</p><p><img src="./无进程操作下的删除结果.png" alt="无进程操作下的删除结果"></p><p>场景二：首先通过<code>tail data_file &amp;</code>模拟操作该数据文件，然后删除该文件并查看磁盘空间的释放情况：</p><p><img src="./有进程操作下的删除结果.png" alt="有进程操作下的删除结果"></p><p>这样就可以复现磁盘空间未释放的情况；然后通过<code>lsof | grep data_file</code>查看文件的占用信息：</p><p><img src="/images/placeholder.png" alt="lsof文件占用信息" data-src="./lsof文件占用信息.png" class="lazyload"></p><p>通过lsof命令可以看到该文件被tail进程所占用，对应的进程PID为92489，文件的路径即就是当初删除前文件的实际路径，只是目前被标记为deleted；现在通过<code>kill -9 PID</code>命令杀死tail进程后，磁盘空间信息如下所示：</p><p><img src="/images/placeholder.png" alt="释放文件占用进程" data-src="./释放文件占用进程.png" class="lazyload"></p><p>到这有关如何解决因为进程占用而导致磁盘空间无法释放的问题应该都已有所了解；因为这个问题看起没什么难度，网上有关此问题的描述也要很多，在这里就不做过多的赘述。接下来基于此次问题的发现和实践解决过程，更进一步的了解一些有关系统监控基本系统监控命令、进程命令，来提升问题的排查和定位能力。</p><h1 id="du-amp-df"><a href="#du-amp-df" class="headerlink" title="du &amp; df"></a>du &amp; df</h1><h2 id="du-disk-usage"><a href="#du-disk-usage" class="headerlink" title="du(disk usage)"></a>du(disk usage)</h2><p>面向文件的命令；通过计算文件的大小来统计指定目录磁盘的使用情况；通俗的讲就是计算文件具体的占用空间</p><p>常用命令</p><blockquote><p><code>du -sh</code> 统计当前目录的总大小</p><p><code>du -sh ./*</code> 查看当前目录下各文件(文件夹)的大小，不统计隐藏文件(夹)</p><p><code>du -h --max-depth=1</code> 查看当前目录下各文件(夹)的大小【囊括隐藏文件(夹)】；等价于<code>du -h -d 1</code></p></blockquote><h2 id="df-disk-free"><a href="#df-disk-free" class="headerlink" title="df(disk free)"></a>df(disk free)</h2><p>面向文件系统的命令；通过文件系统获取分区信息；Eg：分区大小、使用量、剩余量、使用率</p><p>常用命令</p><blockquote><p>df -h</p></blockquote><h1 id="lsof"><a href="#lsof" class="headerlink" title="lsof"></a>lsof</h1><p>lsof(list open file)，列出打开文件；</p><h1 id="Process-amp-PID-amp-Port"><a href="#Process-amp-PID-amp-Port" class="headerlink" title="Process &amp; PID &amp; Port"></a>Process &amp; PID &amp; Port</h1><h2 id="PID查看对应的进程信息"><a href="#PID查看对应的进程信息" class="headerlink" title="PID查看对应的进程信息"></a>PID查看对应的进程信息</h2><p>方案一：通过ps命令查看</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ps -aux | grep pid</span><br></pre></td></tr></table></figure><p>方案二：进入进程目录查看</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> Step 1</span><br><span class="line">cd /proc/pid</span><br><span class="line"><span class="meta">#</span> Step 2</span><br><span class="line">ls -lt</span><br><span class="line"><span class="meta">#</span> cwd表示进程所在的目录；exe表示进程类型</span><br></pre></td></tr></table></figure><h2 id="查看端口占用"><a href="#查看端口占用" class="headerlink" title="查看端口占用"></a>查看端口占用</h2><p>方案一：<code>lsof -i:PID</code></p><p>方案二：<code>netstat -nlp | grep PID</code></p><h1 id="Linux文件描述符"><a href="#Linux文件描述符" class="headerlink" title="Linux文件描述符"></a>Linux文件描述符</h1><h1 id=""><a href="#" class="headerlink" title=" "></a> </h1>]]></content>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>用Python优雅的做数据处理</title>
      <link href="/2018/12/20/%E7%94%A8Python%E4%BC%98%E9%9B%85%E7%9A%84%E5%81%9A%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"/>
      <content type="html"><![CDATA[<p>主要包含Python、Pandas的使用、代码风格、编码技巧和问题总结</p><a id="more"></a><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>本文主要是针对在实际工作中，有关数据处理方面所遇到的问题解决思路和代码优化心得的一个归纳汇总。通篇没有什么太强的逻辑和关联，基本就是想到哪就写到哪，当做一次随笔记录。将会持续不断的把自己在看到、用到的好的代码在这里做已记录和分享，相互学习，共同进步。本文主要涉及到有关Python和数据处理包Pandas的相关记录，有关基础在这里不做赘述，话不多说直入正题。</p><h1 id="基于字典构造DataFrame"><a href="#基于字典构造DataFrame" class="headerlink" title="基于字典构造DataFrame"></a>基于字典构造DataFrame</h1><p>Eg：有如下字典：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">dic = &#123;<span class="string">'name'</span>: [<span class="string">'jack'</span>, <span class="string">'lucy'</span>], <span class="string">'sex'</span>: [<span class="string">'male'</span>, <span class="string">'female'</span>], <span class="string">'age'</span>: [<span class="number">10</span>, <span class="number">9</span>]&#125;</span><br></pre></td></tr></table></figure><p>对应如上的数据，要想转成DataFrame很容易：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">frame = pd.DataFrame(dic)</span><br><span class="line">print(frame)</span><br></pre></td></tr></table></figure><p>最终结果如下所示：</p><p><img src="./1.png" alt="pic1"></p><p>但是假如字典结构变成如下形式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dic = &#123;<span class="string">'name'</span>: <span class="string">'jack'</span>, <span class="string">'sex'</span>: <span class="string">'male'</span>, <span class="string">'age'</span>: <span class="number">10</span>&#125;</span><br></pre></td></tr></table></figure><p>若继续用上述方式来生成DataFrame，则会报错；错误如下所示：</p><p><img src="./2.png" alt="pic2"></p><p>要规避这种错误，可以采用如下方式：</p><p><img src="./3.png" alt="pic3"></p><p>但是不难发现，这种转化结构和前者相比不太一样。倘若第二种的转化结果就是我们的期望形式，那无可厚非，可以通过指定index和column属性来完善行属性值和列属性值；但是如果我们目标结构是第一种DataFrame：字典的key作为DataFrame的列名，在这里推荐一种比较便捷的实现方式：</p><ul><li>Step1: 将字典转化成字典列表<code>dicList = [{&#39;name&#39;: &#39;jack&#39;, &#39;sex&#39;: &#39;male&#39;, &#39;age&#39;: 10}]</code></li><li>基于pd.DataFrame.from_records(dicList)来构造DataFrame</li></ul><p><img src="./4.png" alt="pic4"></p><p>在实际的项目开发中，存在如下需求：字典的key和value均期望作为DataFrame的值，而非将key做为column；对于此类需求场景，可采用如下方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 性别-代号 映射</span></span><br><span class="line">dic = &#123;<span class="string">'male'</span>: <span class="string">'1'</span>, <span class="string">'female'</span>: <span class="string">'2'</span>&#125;</span><br></pre></td></tr></table></figure><p><img src="./5.png" alt="pic5"></p><h1 id="基于DataFrame的数据筛选"><a href="#基于DataFrame的数据筛选" class="headerlink" title="基于DataFrame的数据筛选"></a>基于DataFrame的数据筛选</h1><h2 id="获取列中的值"><a href="#获取列中的值" class="headerlink" title="获取列中的值"></a>获取列中的值</h2><p>Eg：DataFrame中存在一列相同的值，但该列的index未必有序，此处若想获取其中一个值，有如下两种方法：</p><p>方案一：将该列转化为list，然后获取第一个值即可<code>value = list(frame[columns_name])[0]</code></p><p>方案二：<code>value = frame[columns_name].values[0]</code></p><p><em>注：方案一在数据量过大存在性能问题，推荐使用方案二</em></p><h2 id="列筛选"><a href="#列筛选" class="headerlink" title="列筛选"></a>列筛选</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataframe[columns_name] <span class="comment"># 返回值类型：Series， 仅支持单列筛选</span></span><br><span class="line">dataframe.loc[:, [columns_name]] <span class="comment"># 返回值类型：DataFrame, 支持同时筛选多列</span></span><br></pre></td></tr></table></figure><h2 id="行筛选"><a href="#行筛选" class="headerlink" title="行筛选"></a>行筛选</h2><p>类比列筛选，行筛选也可以根据<strong>返回值类型</strong>划分为两大类：返回DataFrame类型和返回Series类型</p><h3 id="DataFrame类型-均支持多行筛选"><a href="#DataFrame类型-均支持多行筛选" class="headerlink" title="DataFrame类型(均支持多行筛选)"></a>DataFrame类型(均支持多行筛选)</h3><ul><li>1) 按照index的具体值筛选：<code>dataframe.loc[[index]]</code>；如果指定值不存在，则会报错</li><li>2) 按照index的序号值筛选：<code>dataframe.iloc[[index]]</code>；无需care当前index的具体值是多少(index的起始下标为0)</li></ul><p><img src="/images/placeholder.png" alt="pic6" data-src="./6.png" class="lazyload"></p><h3 id="Series类型-均不支持多行筛选"><a href="#Series类型-均不支持多行筛选" class="headerlink" title="Series类型(均不支持多行筛选)"></a>Series类型(均不支持多行筛选)</h3><ul><li>1) 按照index的具体值筛选：<code>dataframe.loc[index]</code></li><li>2) 按照index的序号之筛选：<code>dataframe.iloc[index]</code></li></ul><h2 id="条件筛选"><a href="#条件筛选" class="headerlink" title="条件筛选"></a>条件筛选</h2><h3 id="列条件筛选"><a href="#列条件筛选" class="headerlink" title="列条件筛选"></a>列条件筛选</h3><p>Eg：有如下数据集frame</p><p><img src="/images/placeholder.png" alt="pic7" data-src="./7.png" class="lazyload"></p><p>如果只想获得所有的name和age属性，可以通过loc函数很容易实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = frame.loc[:, [<span class="string">'name'</span>, <span class="string">'age'</span>]]</span><br></pre></td></tr></table></figure><h4 id="场景1-单一条件筛选"><a href="#场景1-单一条件筛选" class="headerlink" title="场景1 单一条件筛选"></a>场景1 单一条件筛选</h4><p><code>frame.loc[condition, :]</code> 等价于 <code>frame.loc[condition]</code></p><p>eg：找到所有年龄小与等于10 的人员信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">condition = frame[<span class="string">'age'</span>] &lt;= <span class="number">10</span></span><br><span class="line">df = frame.loc[condition, :]</span><br></pre></td></tr></table></figure><p>或者</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 个人习惯用这种方式，减少代码量 ^o^</span></span><br><span class="line">df = frame.loc[frame[<span class="string">'age] &lt;= 10, :]</span></span><br></pre></td></tr></table></figure><p><img src="/images/placeholder.png" alt="pic8" data-src="./8.png" class="lazyload"></p><h4 id="场景2-范围筛选"><a href="#场景2-范围筛选" class="headerlink" title="场景2 范围筛选"></a>场景2 范围筛选</h4><p>eg：找到名叫Tom或者Lucy或者Jessica的人员信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = frame.loc[frame[<span class="string">'name'</span>].isin([<span class="string">'Lucy'</span>, <span class="string">'Tom'</span>, <span class="string">'Jessica'</span>]), :]</span><br></pre></td></tr></table></figure><p><img src="/images/placeholder.png" alt="pic9" data-src="./9.png" class="lazyload"></p><h4 id="场景3-多条件筛选"><a href="#场景3-多条件筛选" class="headerlink" title="场景3 多条件筛选"></a>场景3 多条件筛选</h4><p><code>frame[(condition1) &amp; (condition2), :]</code> 等价于 <code>frame[(condition1) &amp; (condition2)]</code></p><p>eg：找到年龄等于10且性别为女的所有人的详细信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = frame.loc[frame[<span class="string">'age'</span>].isin([<span class="number">10</span>]) &amp; frame[<span class="string">'sex'</span>].isin([<span class="string">'female'</span>]), :]</span><br></pre></td></tr></table></figure><p><img src="/images/placeholder.png" alt="pic10" data-src="./10.png" class="lazyload"></p><p><em><font color="red">注意：多列，每个条件只能用isin做判断，不能直接用等于、大/小于进行判断</font></em></p><h4 id="场景4-条件取反"><a href="#场景4-条件取反" class="headerlink" title="场景4 条件取反"></a>场景4 条件取反</h4><p>eg：找出除Jack和Tom以为的所有人员信息</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = frame.loc[~frame[<span class="string">'name'</span>].isin([<span class="string">'Jack'</span>, <span class="string">'Tom'</span>]), :]</span><br></pre></td></tr></table></figure><h1 id="DataFrame增添列时报SettingWithCopyWarning"><a href="#DataFrame增添列时报SettingWithCopyWarning" class="headerlink" title="DataFrame增添列时报SettingWithCopyWarning"></a>DataFrame增添列时报SettingWithCopyWarning</h1><p>当在项目代码中按<code>df[&#39;newCol&#39;] = value</code>这种方式为已知的DataFrame新增列值时，有时候会报如下warning：</p><p>SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. </p><p>对代码洁癖的人来说，这当然是无法容忍的。可以通过一种更加友好的方式来实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataFrame.insert（列序值， 列名， 待插入的值）</span><br></pre></td></tr></table></figure><h1 id="DataFrame行添加"><a href="#DataFrame行添加" class="headerlink" title="DataFrame行添加"></a>DataFrame行添加</h1><p>在已有的DataFrame中增添一行数据：<code>df = frame.append(dic, ignore_index=True)</code></p><p>eg：新增一个用户信息</p><p><img src="/images/placeholder.png" alt="pic11" data-src="./11.png" class="lazyload"></p><h1 id="DataFrame排序"><a href="#DataFrame排序" class="headerlink" title="DataFrame排序"></a>DataFrame排序</h1><h2 id="场景1-按照某一列排序"><a href="#场景1-按照某一列排序" class="headerlink" title="场景1 按照某一列排序"></a>场景1 按照某一列排序</h2><p>数值类型默认按照升序，字符串类型默认按照首字母排序</p><p>eg：按照你年龄由小到大排序</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ascending=1 代表升序（默认）, =0代表降序</span></span><br><span class="line">frame.sort_values([<span class="string">'age'</span>], ascending=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><h2 id="场景2-按照自定义排序"><a href="#场景2-按照自定义排序" class="headerlink" title="场景2 按照自定义排序"></a>场景2 按照自定义排序</h2><p>eg：将上述数据按照【Tom，Jessica， Jack， Lucy】的次序进行排序</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sortList = [<span class="string">'Tom'</span>, <span class="string">'Jessica'</span>, <span class="string">'Jack'</span>, <span class="string">'Lucy'</span>]</span><br><span class="line">frame[<span class="string">'name'</span>] = frame[<span class="string">'name'</span>].astype(<span class="string">'category'</span>)</span><br><span class="line">frame[<span class="string">'name'</span>].cat.set_categories(sortList, inplace=<span class="keyword">True</span>)</span><br><span class="line">frame.sort_values(<span class="string">'name'</span>, inplace=<span class="keyword">True</span>)</span><br><span class="line">frame[<span class="string">'name'</span>] = frame[<span class="string">'name'</span>].astype(<span class="string">'object'</span>)</span><br></pre></td></tr></table></figure><p><img src="/images/placeholder.png" alt="pic12" data-src="./12.png" class="lazyload"></p><p>这里新引入了一个Pandas的数据类型：Categorical </p><p>个人理解：该数据类型主要有两中应用场景（官方文档上说有三种，个人感觉第三种在实际工作中用到的并不多，也未多涉猎，故在此省略）：第一，可以数据的内存占有率，官方Doc上有个对比，同等数据量的object对象和category对象相比，后者的大小明显小于前者；这里直接截下官网Doc中的样例加以说明：</p><p><img src="/images/placeholder.png" alt="pic13" data-src="./13.png" class="lazyload"></p><p>因为笔者在实际工作中对该场景的应用不是很多，所在在此未做过多涉猎，就在这简单提及一下，有兴趣的可以参考：官方Doc<br>在这里重点说一下第二个应用场景，即就是列表排序；可以将传统的Object类型的DataFrame列转化为category类型，即可采用：reorder_categories方法或者set_categories方法进行排序。两者之间的区别： </p><blockquote><p>reorder_categories：要求指定排序列表的个数必须和待排序的DataFrame列中元素个数相等；即内容必须相同，但顺序不同； </p></blockquote><blockquote><p>set_categories：排序列表的个数可以不等同于DataFrame列中的元素个数；当排序列表的元素个数大于DataFrame列中的元素时，排序后去交集；反之，排序后DataF会有Nan值。</p></blockquote><h1 id="DataFrame合并"><a href="#DataFrame合并" class="headerlink" title="DataFrame合并"></a>DataFrame合并</h1><h2 id="场景1-纵向合并-concat"><a href="#场景1-纵向合并-concat" class="headerlink" title="场景1 纵向合并(concat)"></a>场景1 纵向合并(concat)</h2><p>有如下数据集</p><p><img src="/images/placeholder.png" alt="pic14" data-src="./14.png" class="lazyload"></p><p><img src="/images/placeholder.png" alt="pic15" data-src="./15.png" class="lazyload"></p><p><img src="/images/placeholder.png" alt="pic16" data-src="./16.png" class="lazyload"></p><p>若把这三个表纵向合并成一个表</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.concat([dataSet1, dataSet2, dataSet3])</span><br></pre></td></tr></table></figure><h2 id="场景2-横向合并-merge"><a href="#场景2-横向合并-merge" class="headerlink" title="场景2 横向合并(merge)"></a>场景2 横向合并(merge)</h2><p>有如下数据集</p><p><img src="/images/placeholder.png" alt="pic17" data-src="./17.png" class="lazyload"></p><p><img src="/images/placeholder.png" alt="pic18" data-src="./18.png" class="lazyload"></p><p><img src="/images/placeholder.png" alt="pic19" data-src="./19.png" class="lazyload"></p><p>这三个数据集都有一个共有字段：name，如果想把这三个数据集做横向合并（merge），原生的pandas是没有提供类似concat这种接口可供直接使用的；所以就得我们自己手动实现该功能：<br>最容易想到的办法：循环遍历数据集列表，然后做merge。</p><p><img src="/images/placeholder.png" alt="pic20" data-src="./20.png" class="lazyload"></p><p>或者先通过concat，然后转置。</p><p>在这里，<font color="red">推荐一下比较简洁的写法：采用函数式编程中的<strong>reduce</strong>函数实现</font>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df = reduce(<span class="keyword">lambda</span> x, y: pd.merge(x, y, on=[<span class="string">'name'</span>]), [df1, df2, df3])</span><br></pre></td></tr></table></figure><h1 id="基于函数式编程消除基于for循环遍历List"><a href="#基于函数式编程消除基于for循环遍历List" class="headerlink" title="基于函数式编程消除基于for循环遍历List"></a>基于函数式编程消除基于for循环遍历List</h1><p>map函数是一种映射思想，既可以基于map函数来映射list中的每一个值，从而实现遍历的效果。<br>例如：对数组中每个元素的平方，map函数的返回值即就是一个list。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataList = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">map(<span class="keyword">lambda</span> x: x * x, dataList)</span><br></pre></td></tr></table></figure><h1 id="两个List的奇偶合并"><a href="#两个List的奇偶合并" class="headerlink" title="两个List的奇偶合并"></a>两个List的奇偶合并</h1><p>该逻辑的实现方案有很多，在此推荐一种笔者比较钟意的方案：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">list1 = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>]</span><br><span class="line">list2 = [<span class="number">2</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>]</span><br><span class="line">mergeList = [<span class="keyword">None</span>] * (len(list1) + len(list2))</span><br><span class="line">mergeList[::<span class="number">2</span>] = list1</span><br><span class="line">mergeList[<span class="number">1</span>::<span class="number">2</span>] = list2</span><br></pre></td></tr></table></figure><p><em>注：【：：2】表示以数组中第一个位置作为起始位置，间隔跨度为2；【1：：2】表示以数组中第二个位置作为起始位置，间隔跨度为2</em></p>]]></content>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Kafka理论之Partition &amp; Replication</title>
      <link href="/2018/12/15/Kafka%E7%90%86%E8%AE%BA%E4%B9%8BPartition-Replication/"/>
      <content type="html"><![CDATA[<p>基于分区和副本集的相关知识，初步了解Kafka的数据存储、同步原理</p><a id="more"></a><h1 id="Kafka基本架构"><a href="#Kafka基本架构" class="headerlink" title="Kafka基本架构"></a>Kafka基本架构</h1><p><img src="./kafka模型.png" alt="kafka模型"></p><p><em>此图来源于朱小厮博客</em></p><p>　　上图为Kafka的典型架构图，对于消息的生产以及消费逻辑不在本文的讨论范畴，主要就Broker的数据存储做以浅显的总结。首先解释一下常见的相关专业术语：</p><blockquote><p>Broker：消息中间件处理节点；每个Kafka服务节点称之为一个Broker，一个Kafka集群由一个或多个Broker组成</p><p>Topic：一类特定数据集合的统称；可类比DB中Table的概念；<font color="red">逻辑概念</font></p><p>Producer：消息的生产者，向Broker发送消息的客户端</p><p>Consumer：消息的消费者，向Broker读取消息的客户端</p><p>Consumer Group：每一个Consumer隶属于一个特定的Consumer Group，一条消息可以被不同Group中的Consumer消费，但同一Group内的消息只能被一个Consumer消费</p><p>Partition：是对Topic中所包含数据集的物理分区；<font color="red">物理概念</font></p><p>Replication：副本集；是Kafka高可用的一种保障机制</p></blockquote><h1 id="Partition-amp-Replication"><a href="#Partition-amp-Replication" class="headerlink" title="Partition &amp; Replication"></a>Partition &amp; Replication</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="Partition-分区"><a href="#Partition-分区" class="headerlink" title="Partition(分区)"></a>Partition(分区)</h3><p>　　Partition是作用于具体的Topic而言的，而不是一个独立的概念。Partition能水平扩展客户端的读写性能，是高吞吐量的 保障。通俗的将，Partition就是一块保存具体数据的空间，本质就是磁盘上存放数据的文件夹，所以Partition是不能跨Broker存在，也不能在同一个Broker上跨磁盘。对于一个Topic，可以根据需要设定Partition的个数；Kafka默认的Partition个数num.partitions为1($KAFKA_HOME/config/server.properties)，表示该Topic的所有数据均写入至一个文件夹下；用户也可以在新建Topic的时候通过显示的指定<code>--partitions &lt;integer&gt;</code>参数实现自定义Partition个数。在数据持久化时，每条消息都是根据一定的分区规则路由到对应的Partition中，并append在log文件的尾部(这一点类似于HDFS)；在同一个Partition中消息是顺序写入的且始终保持有序性；但是不同Partition之间不能保证消息的有序性(高吞吐量的保障)。</p><p>　　Kafka也支持动态增加一个已存在Topic的Partition个数，但<font color="red">不支持动态减少Partition个数</font>。因为被减少Partition所对应的数据处理是个难题；由于Kafka的数据写模式的限制，所以如果要把这些Partition的历史数据集追加到有效的Partition的尾部，就会破坏了Kafka在Partition上消息的有序性，显然是不合理的；但如果按照时间戳重新构分区的数据文件，可操作性和难度都将是非常大的，所以目前并不支持动态减少Partition个数。</p><p>　　Partition是用来存储数据的，但并不是最小的数据存储单元。Partition下还可以细分成Segment，每个Partition是由一个或多个Segment组成。每个Segment分别对应两个文件：一个是以.index结尾的索引文件，另一个是以.log结尾的数据文件，且两个文件的文件名完全相同。所有的Segment均存在于所属Partition的目录下。</p><p><em>Segment的必要性：如果以partition作为数据存储的最小单元，那么partition将会是一个很大的数据文件，且数据量是持续递增的；当进行过期数据清理或消费指定offset数据时，操作如此的大文件将会是一个很严重的性能问题。</em></p><h3 id="Replication-副本集"><a href="#Replication-副本集" class="headerlink" title="Replication(副本集)"></a>Replication(副本集)</h3><p>　　Replication是Kafka架构中一个比较重要的概念，是系统高可用的一种保障。Replication逻辑上是作用于Topic的，但实际上是体现在每一个Partition上。例如：有一个Topic，分区(partitions)数为3(分别为a, b, c)，副本因子(replication-factor)数也为3；其本质就是该Topic一共有3个a分区，3个b分区，3个c分区。这样的设计在某种意义上就很大程度的提高了系统的容错率。接着上述的例子想另外一个问题：一个Topic下a分区一共有三个，既然是副本集，那这三个所包含的数据都完全一样吗？作用都一样吗？说到这就不得不引出两个概念：</p><ul><li><p>Leader Replica</p><blockquote><p>概念：每一个Partition有且只有一个Replica可以作为Leader</p><p>职责：负责处理所有Producer、Consumer的请求；与此同时，Leader还负责监管和维护ISR(In-Sync Replicas：副本同步队列)中所有follower的滞后状态。</p></blockquote></li><li><p>Follower Replica</p><blockquote><p>概念：每个Partition中除了Leader以外的所有Replica均为follower</p><p>职责：不处理任何来自客户端的请求；只通过Fetch Request拉取leader replica的数据进行同步</p></blockquote></li></ul><p><em>tips: leader partition(主分区) &amp; leader replica(主副本集)：其实这两个概念是一回事；因为副本集策略只是一种机制，是为了提高可用性而生的。这种策略就是作用于partition上的，通俗的说增加副本集个数其实就是增加同一个partition的备份个数；同样的对于主分区而言，就是同一个partition下所有备份中的主副本集。</em></p><p><em>注意：同一个topic下的不同partition之间是没有主次之分，都是同等重要且存储不同数据的。</em></p><h2 id="命名规则-amp-数据存储"><a href="#命名规则-amp-数据存储" class="headerlink" title="命名规则 &amp; 数据存储"></a>命名规则 &amp; 数据存储</h2><h3 id="Partition"><a href="#Partition" class="headerlink" title="Partition"></a>Partition</h3><p>　　当新建一个topic，并指定partition个数后，会在log.dirs参数($KAFKA_HOME/config/server.properties)所指定的目录下创建对应的分区目录，用来存储落到该分区上的数据。分区目录的命名格式为：topic名称 + 短横线 + 分区序号；序号默认从0开始，最大为分区数 - 1。</p><p>为了尽可能的提升服务的可用性和容错率，Kafka遵循如下的分区分配原则：</p><blockquote><p>所有的replica要尽可能的平均分配到集群中的每一台broker上</p><p>尽可能保证同一个partition的leader和follower分在不同的broker上</p><p>如果集群跨机架，尽可能的保证每个partition的replica分配到不同的机架上</p></blockquote><p>　　Eg：集群中有四个节点，均在统一机架上，新建一个topic：demoTopic，指定分区个数为4，副本因子为3；则对应的partition目录分别为：demoTopic-0、demoTopic-1、demoTopic-2、demoTopic-3；具体如下图所示：</p><p><img src="./partition命名.png" alt="partition命名"></p><p>　　因为集群未跨机架，所以在这里主要验证一下前两条分区分配原则：四个主分区分别位于四个不同的broker上，且另外两个replica也随机分配到除leader所在节点以外的其他三个broker上；具体的分区分布图如下所示：</p><p><img src="分区分配图.png" alt="分区分配图"></p><h3 id="Segment"><a href="#Segment" class="headerlink" title="Segment"></a>Segment</h3><p>　　每个Partition全局的第一个Segment文件名均是从0开始，后续每个Segment的文件名为上一个Segment文件中最后一条消息的offset值；数据的大小为64位，20位数字字符的长度，未用到的用0填充。同一个Segment的.index文件和.log文件的文件名完全相同；所以初始化每个Partition下的Segment的文件名如下所示：</p><p><img src="./初始化Segment文件命名.png" alt="初始化Segment文件命名"></p><p>　　这种命名格式的好处在于可以有效的规避单文件数据量过大导致的操作难问题，不仅如此，还可以方便、快速的定位数据。例如：要实现从指定offset处开始读取数据，只需要根据给定的offset值与对应Partition下的segment文件名所比对，就可以快速的定位目标数据所在的segment文件，然后根据目标segment的.index文件查找给定offset值所对应的实际磁盘偏移量，即可快速在.log中读取目标数据。</p><p>　　在Kafka 0.10.1.0以后，对于每个Segment文件，在原有的.index和.log文件的基础上，新增加一个.timeindex文件，通过该索引文件 可以实现基于时间戳操作消息的功能，具体实现详见<a href="https://yhyr.github.io/2019/01/23/Kafka-Timestamp/" target="_blank" rel="noopener">Kafka Timestamp</a></p><font color="red">Kafka中所说的Offset本质上是一个逻辑值，代表的是目标数据对应在Partition上的偏移量；而数据在磁盘上的实际偏移量是存储在对应Segment的.index文件中。</font><h1 id="数据同步"><a href="#数据同步" class="headerlink" title="数据同步"></a>数据同步</h1><p>　　通过简单介绍replica之间的offset的变化和更新逻辑，来初步了解Kafka的数据同步机制。首先引入几个概念：</p><ul><li><p>Offset相关概念</p><blockquote><p>LEO(LogEndOffset)：表示每个Partition中log最后一条message的位置</p><p>HW(HighWatermark)：表示Consumer能够看到该Partition的位置</p></blockquote></li><li><p>Replica相关概念</p><blockquote><p>ISR(In-Sync Replicas)：副本同步列表【包含Leader和Follower】</p><p>OSR(Outof-Sync Replicas)：由于同步落后而被剔除的副本列表，阈值参数：replica.lag.time.max.ms</p><p>AR(Assigned Replicas)：所有副本集；AR = ISR + OSR</p></blockquote></li></ul><p>　　清楚LEO、HW和ISR之间的相互关系是了解Kafka底层数据同步的关键：Kafka取Partition所对应的ISR中最小的LEO作为整个Partition的HW；每个Partition都会有自己独立的HW，与此同时leader和follower都会负责维护和更新自己的HW。对于leader新写入的消息，Consumer不能立刻被发现并进行消费，leader会等待该消息被ISR中所有的replica同步更新HW后，此时leader才会更新该partition的HW为之前新写入消息的offset，此时该消息对外才可见。LEO和HW的转化逻辑如下图所示：</p><p><img src="./HW-LEO逻辑图.png" alt="HW-LEO逻辑图"></p><p><em>图片来自于朱小厮博客</em></p><h1 id="可用性-amp-一致性"><a href="#可用性-amp-一致性" class="headerlink" title="可用性 &amp; 一致性"></a>可用性 &amp; 一致性</h1><p>　　在分布式架构中，服务的可用性和数据的一致性是一个绕不开的话题，Kafka也不例外。如上文所说：当leader接受到一条消息后，需要等待ISR中所有的replica都同步复制完成以后，该消息才能被消费。如果在同步的过程中，ISR中如果有follower replica的同步落后延迟超过了阈值，则会被leader从ISR中剔除；只要ISR中所有的replica均同步成功，则该消息就一定不会丢失。从数据的角度出发，这种方式很契合一致性的需求，但是当集群的节点数较多，ISR队里的副本数变大时，每条消息的同步时长可能并不是所有业务场景所能容忍的，所以Kafka在Producer阶段通过request.required.acks参数提供了不同类型的应答机制以方便用户在系统吞吐量和一致性之间进行权衡：</p><blockquote><p>1(Default)：表示Producer在ISR中的Leader成功接收到消息后并确认后，则代表该消息以成功写入</p><p>0：表示Producer将消息发送到Broker中后无需等待Broker的确认；即就是：只管发消息，不关注消息是否被成功接收</p><p>-1(all)：表示Producer需要等待ISR中所有的Replica都确认收到消息才算写入成功；如果ISR中只剩下Leader，则等通过request.required.acks=1的效果</p></blockquote><p>　　在老版本的Kafka(0.11.0.0以前)中，存在一个潜在的数据一致性问题：假如一个Partition有两个Replica，A(Leader)中包含的数据为a, b, c, d, e，LEO为5；B(Follower)包含的数据为a, b, c，LEO为3；此时该Partition的HW为3，Consumer可见的消息为a, b, c，系统对外表示正常；当follower还未来得及同步消息d、e时，leader挂了，此时B变成Leader，并且Producer重新发了两条消息f和g；因为此时系统中只有B一个存活，所以Partition对外的HW这会更新为5没有问题，Consumer可见的内容为a, b, c, f, g；此时A被唤醒并作为Follower开始从Leader中拉取数据，因为follower自身的HW等于Leader的HW，所以B没有拉去到任何数据，当Producer继续发送消息时，就会导致副本A、B的数据集不一致。这个问题在0.11.0.0中通过leader epoch机制来消除该问题。可以把epoch理解为代(版本)的概念，即每一次的leader对应一个唯一的epoch，如果leader更换，则对应的epoch值也会随之更换，而过期的epoch请求则都会被忽略。</p><p><em>更多关于Kafka High Watermark可详见<a href="http://www.cnblogs.com/huxi2b/p/7453543.html" target="_blank" rel="noopener">huxi博客</a></em></p><p><em>本文参考文献</em></p><p><em><a href="https://pan.baidu.com/s/1Ol3vd1Ehx8EC8yWAufPjWA" target="_blank" rel="noopener">kafka权威指南</a></em></p><p><em><a href="https://blog.csdn.net/u013256816" target="_blank" rel="noopener">朱小厮博客</a></em></p><p><em><a href="https://www.cnblogs.com/huxi2b/" target="_blank" rel="noopener">huxi博客</a></em></p>]]></content>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>基于网易云音乐的分布式爬虫实现</title>
      <link href="/2018/10/28/%E5%9F%BA%E4%BA%8E%E7%BD%91%E6%98%93%E4%BA%91%E9%9F%B3%E4%B9%90%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%E5%AE%9E%E7%8E%B0/"/>
      <content type="html"><![CDATA[<p>通过scrapy-redis + HDFS 实现网易云音乐的用户、评论数据的爬取和持久化。源代码详见<a href="https://github.com/YHYR/CloudMusicSpider" target="_blank" rel="noopener">Github</a></p><a id="more"></a><p><em>注：此爬虫项目及其数据仅作学术学习使用</em></p><h1 id="Prepare"><a href="#Prepare" class="headerlink" title="Prepare"></a>Prepare</h1><h4 id="Python-版本"><a href="#Python-版本" class="headerlink" title="Python 版本"></a>Python 版本</h4><blockquote><p>Python 3.6.5</p></blockquote><h4 id="依赖包"><a href="#依赖包" class="headerlink" title="依赖包"></a>依赖包</h4><blockquote><p>scrapy_redis</p><p>redis</p><p>mysql-python</p><p>kafka-python</p><p>hdfs</p></blockquote><h3 id="数据API接口"><a href="#数据API接口" class="headerlink" title="数据API接口"></a>数据API接口</h3><blockquote><p>详见<a href="https://github.com/YHYR/CloudMusicSpider" target="_blank" rel="noopener">Github</a></p></blockquote><h1 id="Implement"><a href="#Implement" class="headerlink" title="Implement"></a>Implement</h1><h3 id="数据依赖关系"><a href="#数据依赖关系" class="headerlink" title="数据依赖关系"></a>数据依赖关系</h3><p><img src="./数据关系.png" alt="数据关系"></p><h3 id="时序"><a href="#时序" class="headerlink" title="时序"></a>时序</h3><p><img src="./爬虫时序.png" alt="爬虫时序"></p><p>　　上图详细说明了整个爬虫工程的前一半的数据抽取逻辑；关于用户类数据的抽取在实现逻辑上与上图基本一致。在用户相关数据的爬取上，实现了在尽可能多的爬取用户数据的同时，有效规避重复爬取。实现逻辑如下：</p><p>　　<strong>在代码实现层面上，显示的指定用户相关数据的爬取逻辑</strong>。优先级为：用户基本信息 &gt; 用户粉丝信息 = 用户关注信息 = 用户听歌记录。即就是只有在爬取到一个用户的基本信息以后，才初始化这个用户的附属信息的URL(例：粉丝列表、关注列表、听歌记录)。这样就可以保证只要爬取用户基本信息时不重复，则附属属性数据的爬取就不会重复。所以在Redis中单独维护一个用户UserId的数据集，每当爬取歌曲的评论数据、用户的粉丝或者关注者数据时，都会先校验当前用户是否在该数据集内；如果不在则初始化用户的基本信息URL到请求队列中，反之则认为该用户已经爬取过。</p><p>　　为了提升用户数据量，在收集歌曲评论中所涉及到的用户信息的同时，深度爬取每个用户所对应的关注和粉丝列表的信息。</p><h3 id="数据存储"><a href="#数据存储" class="headerlink" title="数据存储"></a>数据存储</h3><p>　　这里选择将爬取到的数据持久化到HDFS中，便于单机自测；如果想真正的基于多机器实现分布式爬取数据，建议将数据改写到Kafka中；因为HDFS的特性为一次写，多次读；且不支持在统一文件的任意offset进行写操作；因此对于HDFS文件的操作只有append，且不支持并发写操作。如果提升爬虫效率，建议将数据先写入Ｋafka，然后通过后台脚本通过多线程/多进程进行消费和持久化操作。</p><h1 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h1><p>　由于我们的数据链路存在一定的依赖关系，但也并非是单线程的地步。所以在具体实现时采用scrapy-redis框架来实现分布式的效果。在请求队列上，可以为每一个Spider在Redis中开辟一个Request队列，这样有效的提升爬取效率。当然要想持续爬取，加代理也是必不可少的。有关<font color="red">免费代理IP的爬取和有效性校验</font>的实现，可详见<a href="https://github.com/YHYR/ProxyIpSpider" target="_blank" rel="noopener">Github</a>。</p><p>需要注意的是，scrapy-redis不支持在请求队列中实现去重。源码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedisMixin</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""Mixin class to implement reading urls from a redis queue."""</span></span><br><span class="line">    redis_key = <span class="keyword">None</span></span><br><span class="line">    redis_batch_size = <span class="keyword">None</span></span><br><span class="line">    redis_encoding = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Redis client placeholder.</span></span><br><span class="line">    server = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">start_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Returns a batch of start requests from redis."""</span></span><br><span class="line">        <span class="keyword">return</span> self.next_requests()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">setup_redis</span><span class="params">(self, crawler=None)</span>:</span></span><br><span class="line">        <span class="string">"""Setup redis connection and idle signal.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        This should be called after the spider has set its crawler object.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> self.server <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> crawler <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="comment"># We allow optional crawler argument to keep backwards</span></span><br><span class="line">            <span class="comment"># compatibility.</span></span><br><span class="line">            <span class="comment"># <span class="doctag">XXX:</span> Raise a deprecation warning.</span></span><br><span class="line">            crawler = getattr(self, <span class="string">'crawler'</span>, <span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> crawler <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"crawler is required"</span>)</span><br><span class="line"></span><br><span class="line">        settings = crawler.settings</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.redis_key <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            self.redis_key = settings.get(</span><br><span class="line">                <span class="string">'REDIS_START_URLS_KEY'</span>, defaults.START_URLS_KEY,</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        self.redis_key = self.redis_key % &#123;<span class="string">'name'</span>: self.name&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.redis_key.strip():</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"redis_key must not be empty"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.redis_batch_size <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span> Deprecate this setting (REDIS_START_URLS_BATCH_SIZE).</span></span><br><span class="line">            self.redis_batch_size = settings.getint(</span><br><span class="line">                <span class="string">'REDIS_START_URLS_BATCH_SIZE'</span>,</span><br><span class="line">                settings.getint(<span class="string">'CONCURRENT_REQUESTS'</span>),</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            self.redis_batch_size = int(self.redis_batch_size)</span><br><span class="line">        <span class="keyword">except</span> (TypeError, ValueError):</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"redis_batch_size must be an integer"</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> self.redis_encoding <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">            self.redis_encoding = settings.get(<span class="string">'REDIS_ENCODING'</span>, defaults.REDIS_ENCODING)</span><br><span class="line"></span><br><span class="line">        self.logger.info(<span class="string">"Reading start URLs from redis key '%(redis_key)s' "</span></span><br><span class="line">                         <span class="string">"(batch size: %(redis_batch_size)s, encoding: %(redis_encoding)s"</span>,</span><br><span class="line">                         self.__dict__)</span><br><span class="line"></span><br><span class="line">        self.server = connection.from_settings(crawler.settings)</span><br><span class="line">        <span class="comment"># The idle signal is called when the spider has no requests left,</span></span><br><span class="line">        <span class="comment"># that's when we will schedule new requests from redis queue</span></span><br><span class="line">        crawler.signals.connect(self.spider_idle, signal=signals.spider_idle)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Returns a request to be scheduled or none."""</span></span><br><span class="line">        use_set = self.settings.getbool(<span class="string">'REDIS_START_URLS_AS_SET'</span>, defaults.START_URLS_AS_SET)</span><br><span class="line">        fetch_one = self.server.spop <span class="keyword">if</span> use_set <span class="keyword">else</span> self.server.lpop</span><br><span class="line">        <span class="comment"># <span class="doctag">XXX:</span> Do we need to use a timeout here?</span></span><br><span class="line">        found = <span class="number">0</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Use redis pipeline execution.</span></span><br><span class="line">        <span class="keyword">while</span> found &lt; self.redis_batch_size:</span><br><span class="line">            data = fetch_one(self.redis_key)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> data:</span><br><span class="line">                <span class="comment"># Queue empty.</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            req = self.make_request_from_data(data)</span><br><span class="line">            <span class="keyword">if</span> req:</span><br><span class="line">                <span class="keyword">yield</span> req</span><br><span class="line">                found += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                self.logger.debug(<span class="string">"Request not made from data: %r"</span>, data)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> found:</span><br><span class="line">            self.logger.debug(<span class="string">"Read %s requests from '%s'"</span>, found, self.redis_key)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">make_request_from_data</span><span class="params">(self, data)</span>:</span></span><br><span class="line">        <span class="string">"""Returns a Request instance from data coming from Redis.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        By default, ``data`` is an encoded URL. You can override this method to</span></span><br><span class="line"><span class="string">        provide your own message decoding.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Parameters</span></span><br><span class="line"><span class="string">        ----------</span></span><br><span class="line"><span class="string">        data : bytes</span></span><br><span class="line"><span class="string">            Message from redis.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        url = bytes_to_str(data, self.redis_encoding)</span><br><span class="line">        <span class="keyword">return</span> self.make_requests_from_url(url)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">schedule_next_requests</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Schedules a request if available"""</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> While there is capacity, schedule a batch of redis requests.</span></span><br><span class="line">        <span class="keyword">for</span> req <span class="keyword">in</span> self.next_requests():</span><br><span class="line">            self.crawler.engine.crawl(req, spider=self)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">spider_idle</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Schedules a request if available, otherwise waits."""</span></span><br><span class="line">        <span class="comment"># <span class="doctag">XXX:</span> Handle a sentinel to close the spider.</span></span><br><span class="line">        self.schedule_next_requests()</span><br><span class="line">        <span class="keyword">raise</span> DontCloseSpider</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RedisCrawlSpider</span><span class="params">(RedisMixin, CrawlSpider)</span>:</span></span><br><span class="line">    <span class="string">"""Spider that reads urls from redis queue when idle.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Attributes</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    redis_key : str (default: REDIS_START_URLS_KEY)</span></span><br><span class="line"><span class="string">        Redis key where to fetch start URLs from..</span></span><br><span class="line"><span class="string">    redis_batch_size : int (default: CONCURRENT_REQUESTS)</span></span><br><span class="line"><span class="string">        Number of messages to fetch from redis on each attempt.</span></span><br><span class="line"><span class="string">    redis_encoding : str (default: REDIS_ENCODING)</span></span><br><span class="line"><span class="string">        Encoding to use when decoding messages from redis queue.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Settings</span></span><br><span class="line"><span class="string">    --------</span></span><br><span class="line"><span class="string">    REDIS_START_URLS_KEY : str (default: "&lt;spider.name&gt;:start_urls")</span></span><br><span class="line"><span class="string">        Default Redis key where to fetch start URLs from..</span></span><br><span class="line"><span class="string">    REDIS_START_URLS_BATCH_SIZE : int (deprecated by CONCURRENT_REQUESTS)</span></span><br><span class="line"><span class="string">        Default number of messages to fetch from redis on each attempt.</span></span><br><span class="line"><span class="string">    REDIS_START_URLS_AS_SET : bool (default: True)</span></span><br><span class="line"><span class="string">        Use SET operations to retrieve messages from the redis queue.</span></span><br><span class="line"><span class="string">    REDIS_ENCODING : str (default: "utf-8")</span></span><br><span class="line"><span class="string">        Default encoding to use when decoding messages from redis queue.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(self, crawler, *args, **kwargs)</span>:</span></span><br><span class="line">        obj = super(RedisCrawlSpider, self).from_crawler(crawler, *args, **kwargs)</span><br><span class="line">        obj.setup_redis(crawler)</span><br><span class="line">        <span class="keyword">return</span> obj</span><br></pre></td></tr></table></figure><p>　　当启动一个Spider后，就会读取Redis中指定key下的url信息。如果当前key下没有相应的value就等待；当有值时，则会调用<code>next_requests</code>方法来获取数据；查看方法next_requests的源码不难看出，<strong>无论你当前的key的数据类型是什么，最终都会pop掉</strong>，从而导致Redis中不在有这个值。这也就是上述中提到的为什么要自己通过维护userId数据集来实现抽取的唯一性，而不是用这个请求队列作为唯一性校验的原因。对于一个正常的设计，应该是在项目运行一段时间后会出现所有的Spider都处于挂起等待的状态，此时所涉及到的所有请求队列应该均为空；否则就有可能因为设计问题导致无限死循环，从而出现永不休止的爬取相同数据。</p><p>　　Scrapy-Redis自带的去重功能目前还未研究，效果如何暂不做评论；不过网上有很多关于修改源码通过实现去重逻辑，有兴趣的可查阅有关BloomFilter相关的资料。</p><h2 id="答谢"><a href="#答谢" class="headerlink" title="答谢"></a>答谢</h2><p>感谢<a href="https://github.com/sqaiyan/netmusic-node" target="_blank" rel="noopener">sqaiyan</a>在数据API上给予的灵感</p><p>感谢<a href="https://github.com/LiuXingMing/SinaSpider" target="_blank" rel="noopener">LiuXingMing</a>在分布式爬虫实现上给予的灵感</p>]]></content>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 爬虫 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Pandas.read_json()踩坑总结 &amp; 源码初探</title>
      <link href="/2018/09/16/Pandas-read-json-%E8%B8%A9%E5%9D%91%E6%80%BB%E7%BB%93-%E6%BA%90%E7%A0%81%E5%88%9D%E6%8E%A2/"/>
      <content type="html"><![CDATA[<p>　　基于实际工作中遇到的一种极端场景来分析Pandas.read_json()方法的源码实现；顺便站在个人学习和使用的角度出发，吐槽一下该方法底层设计的不合理之处。</p><a id="more"></a><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>环境依赖：Python 2.7</p><p>样例数据(json文件)</p><p><img src="./样例数据.png" alt="样例数据"></p><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>　　通过Pandas.read_json(jsonFilePath)方法读取json文件时，会出现数据内容发生奇怪的转变；Eg：假设样例数据的文件名为data.json，则执行pd.read_json(data.json)后的结果以及各列数据的数据类型分别如下图所示：</p><p><img src="./read_json解析结果.png" alt="read_json解析结果"></p><p><img src="./read_json结果数据类型.png" alt="read_json结果数据类型"></p><p>　　相较于原始数据集，经过该方法执行后的结果有两处不一致的地方：第一，userId和telephone这两列的数据类型由原本的String变成了int64；第二，<font color="red">userId字段的值发生了变化</font>。</p><h2 id="源码剖析"><a href="#源码剖析" class="headerlink" title="源码剖析"></a>源码剖析</h2><p>接下来将从深入源码来探究这种情况发生的原因；pd.read_json()的源码及其该方法之间的调用时序分别如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_json</span><span class="params">(path_or_buf=None, orient=None, typ=<span class="string">'frame'</span>, dtype=True,</span></span></span><br><span class="line"><span class="function"><span class="params">              convert_axes=True, convert_dates=True, keep_default_dates=True,</span></span></span><br><span class="line"><span class="function"><span class="params">              numpy=False, precise_float=False, date_unit=None)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Convert a JSON string to pandas object</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Parameters</span></span><br><span class="line"><span class="string">    ----------</span></span><br><span class="line"><span class="string">    path_or_buf : a valid JSON string or file-like, default: None</span></span><br><span class="line"><span class="string">        The string could be a URL. Valid URL schemes include http, ftp, s3, and</span></span><br><span class="line"><span class="string">        file. For file URLs, a host is expected. For instance, a local file</span></span><br><span class="line"><span class="string">        could be ``file://localhost/path/to/table.json``</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    orient</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        * `Series`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">          - default is ``'index'``</span></span><br><span class="line"><span class="string">          - allowed values are: ``&#123;'split','records','index'&#125;``</span></span><br><span class="line"><span class="string">          - The Series index must be unique for orient ``'index'``.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        * `DataFrame`</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">          - default is ``'columns'``</span></span><br><span class="line"><span class="string">          - allowed values are: &#123;'split','records','index','columns','values'&#125;</span></span><br><span class="line"><span class="string">          - The DataFrame index must be unique for orients 'index' and</span></span><br><span class="line"><span class="string">            'columns'.</span></span><br><span class="line"><span class="string">          - The DataFrame columns must be unique for orients 'index',</span></span><br><span class="line"><span class="string">            'columns', and 'records'.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        * The format of the JSON string</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">          - split : dict like</span></span><br><span class="line"><span class="string">            ``&#123;index -&gt; [index], columns -&gt; [columns], data -&gt; [values]&#125;``</span></span><br><span class="line"><span class="string">          - records : list like</span></span><br><span class="line"><span class="string">            ``[&#123;column -&gt; value&#125;, ... , &#123;column -&gt; value&#125;]``</span></span><br><span class="line"><span class="string">          - index : dict like ``&#123;index -&gt; &#123;column -&gt; value&#125;&#125;``</span></span><br><span class="line"><span class="string">          - columns : dict like ``&#123;column -&gt; &#123;index -&gt; value&#125;&#125;``</span></span><br><span class="line"><span class="string">          - values : just the values array</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    typ : type of object to recover (series or frame), default 'frame'</span></span><br><span class="line"><span class="string">    dtype : boolean or dict, default True</span></span><br><span class="line"><span class="string">        If True, infer dtypes, if a dict of column to dtype, then use those,</span></span><br><span class="line"><span class="string">        if False, then don't infer dtypes at all, applies only to the data.</span></span><br><span class="line"><span class="string">    convert_axes : boolean, default True</span></span><br><span class="line"><span class="string">        Try to convert the axes to the proper dtypes.</span></span><br><span class="line"><span class="string">    convert_dates : boolean, default True</span></span><br><span class="line"><span class="string">        List of columns to parse for dates; If True, then try to parse</span></span><br><span class="line"><span class="string">        datelike columns default is True; a column label is datelike if</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        * it ends with ``'_at'``,</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        * it ends with ``'_time'``,</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        * it begins with ``'timestamp'``,</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        * it is ``'modified'``, or</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        * it is ``'date'``</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    keep_default_dates : boolean, default True</span></span><br><span class="line"><span class="string">        If parsing dates, then parse the default datelike columns</span></span><br><span class="line"><span class="string">    numpy : boolean, default False</span></span><br><span class="line"><span class="string">        Direct decoding to numpy arrays. Supports numeric data only, but</span></span><br><span class="line"><span class="string">        non-numeric column and index labels are supported. Note also that the</span></span><br><span class="line"><span class="string">        JSON ordering MUST be the same for each term if numpy=True.</span></span><br><span class="line"><span class="string">    precise_float : boolean, default False</span></span><br><span class="line"><span class="string">        Set to enable usage of higher precision (strtod) function when</span></span><br><span class="line"><span class="string">        decoding string to double values. Default (False) is to use fast but</span></span><br><span class="line"><span class="string">        less precise builtin functionality</span></span><br><span class="line"><span class="string">    date_unit : string, default None</span></span><br><span class="line"><span class="string">        The timestamp unit to detect if converting dates. The default behaviour</span></span><br><span class="line"><span class="string">        is to try and detect the correct precision, but if this is not desired</span></span><br><span class="line"><span class="string">        then pass one of 's', 'ms', 'us' or 'ns' to force parsing only seconds,</span></span><br><span class="line"><span class="string">        milliseconds, microseconds or nanoseconds respectively.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    result : Series or DataFrame</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    filepath_or_buffer, _, _ = get_filepath_or_buffer(path_or_buf)</span><br><span class="line">    <span class="keyword">if</span> isinstance(filepath_or_buffer, compat.string_types):</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            exists = os.path.exists(filepath_or_buffer)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># if the filepath is too long will raise here</span></span><br><span class="line">        <span class="comment"># 5874</span></span><br><span class="line">        <span class="keyword">except</span> (TypeError, ValueError):</span><br><span class="line">            exists = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> exists:</span><br><span class="line">            <span class="keyword">with</span> open(filepath_or_buffer, <span class="string">'r'</span>) <span class="keyword">as</span> fh:</span><br><span class="line">                json = fh.read()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            json = filepath_or_buffer</span><br><span class="line">    <span class="keyword">elif</span> hasattr(filepath_or_buffer, <span class="string">'read'</span>):</span><br><span class="line">        json = filepath_or_buffer.read()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        json = filepath_or_buffer</span><br><span class="line"></span><br><span class="line">    obj = <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">if</span> typ == <span class="string">'frame'</span>:</span><br><span class="line">        obj = FrameParser(json, orient, dtype, convert_axes, convert_dates,</span><br><span class="line">                          keep_default_dates, numpy, precise_float,</span><br><span class="line">                          date_unit).parse()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> typ == <span class="string">'series'</span> <span class="keyword">or</span> obj <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> isinstance(dtype, bool):</span><br><span class="line">            dtype = dict(data=dtype)</span><br><span class="line">        obj = SeriesParser(json, orient, dtype, convert_axes, convert_dates,</span><br><span class="line">                           keep_default_dates, numpy, precise_float,</span><br><span class="line">                           date_unit).parse()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> obj</span><br></pre></td></tr></table></figure><p><img src="./readJson方法间调用时序.png" alt="read_json方法间调用时序"></p><p>　　通过这段源码可以看出，read_json方法主要做了三件事：首先基于给定的参数做校验，然后获取指定url或流中的数据信息转化为jsonStr，最后一步对该jsonStr进行解析。用户可显示的通过typ字段来指定解析结果的类型(DataFrame or Series)。解析逻辑所对应的对象模型如下所示：</p><p><img src="./Parse对象模型.png" alt="Parse对象模型"></p><p>　　由于Series的解析逻辑比较简单，且实际工作中直接基于DataFrame的操作比较多，因此这里主要对jsonStr解析成DataFrame的过程做进一步的梳理。在第三步数据解析的过程中，<code>FrameParser.parse()</code>方法的本质其实是调用了父类Parse的parse方法，该方法的职能有三个：首先将jsonStr解析成DataFrame数据结构；其次对解析结果的轴做数据类型转化；最后<font color="red">尝试对数据进行类型转化</font>。源码及对应的方法间调用时序如下图所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span><span class="params">(self)</span>:</span></span><br><span class="line"></span><br><span class="line">      <span class="comment"># try numpy</span></span><br><span class="line">      numpy = self.numpy</span><br><span class="line">      <span class="keyword">if</span> numpy:</span><br><span class="line">          self._parse_numpy()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">          self._parse_no_numpy()</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> self.obj <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">          <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">      <span class="keyword">if</span> self.convert_axes:</span><br><span class="line">          self._convert_axes()</span><br><span class="line">      self._try_convert_types()</span><br><span class="line">      <span class="keyword">return</span> self.obj</span><br></pre></td></tr></table></figure><p><img src="/images/placeholder.png" alt="Parse.parse方法间调用时序" data-src="./Parse.parse方法间调用时序.png" class="lazyload"></p><p>　　在解析jsonStr时，首先会根据参数numpy来判断是否需要将数据反序列化为numpy数组类型；这个反序列化的过程是通过Pandas内部封装的json工具类的loads方法来实现的；然后将反序列化后的Dict对象经过DataFrame类进行数据初始化，从而得到该jsonFile所对应的DataFrame数据结构。由于<code>_parse_no_numpy()</code> 和<code>_parse_numpy()</code>这两个方法的原理类似，这里以<code>FrameParse._parse_numpy()</code>为例，看一下对应的源码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_parse_numpy</span><span class="params">(self)</span>:</span></span><br><span class="line"></span><br><span class="line">    json = self.json</span><br><span class="line">    orient = self.orient</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> orient == <span class="string">"columns"</span>:</span><br><span class="line">        args = loads(json, dtype=<span class="keyword">None</span>, numpy=<span class="keyword">True</span>, labelled=<span class="keyword">True</span>,</span><br><span class="line">                     precise_float=self.precise_float)</span><br><span class="line">        <span class="keyword">if</span> args:</span><br><span class="line">            args = (args[<span class="number">0</span>].T, args[<span class="number">2</span>], args[<span class="number">1</span>])</span><br><span class="line">        self.obj = DataFrame(*args)</span><br><span class="line">    <span class="keyword">elif</span> orient == <span class="string">"split"</span>:</span><br><span class="line">        decoded = loads(json, dtype=<span class="keyword">None</span>, numpy=<span class="keyword">True</span>,</span><br><span class="line">                        precise_float=self.precise_float)</span><br><span class="line">        decoded = dict((str(k), v) <span class="keyword">for</span> k, v <span class="keyword">in</span> compat.iteritems(decoded))</span><br><span class="line">        self.check_keys_split(decoded)</span><br><span class="line">        self.obj = DataFrame(**decoded)</span><br><span class="line">    <span class="keyword">elif</span> orient == <span class="string">"values"</span>:</span><br><span class="line">        self.obj = DataFrame(loads(json, dtype=<span class="keyword">None</span>, numpy=<span class="keyword">True</span>,</span><br><span class="line">                                   precise_float=self.precise_float))</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        self.obj = DataFrame(*loads(json, dtype=<span class="keyword">None</span>, numpy=<span class="keyword">True</span>,</span><br><span class="line">                                    labelled=<span class="keyword">True</span>,</span><br><span class="line">                                    precise_float=self.precise_float))</span><br></pre></td></tr></table></figure><p>　　<strong>注意这里的写法</strong>：父类Parse中是没有<code>_parse_no_numpy()</code> 和<code>_parse_numpy()</code>这两个方法的，也就是说是在父类调用子类的方法。其实不同于Java这类编程语言，在Python中需要从对象生成的角度来看待这个问题；因为此时的Parse类就是FrameParser，所以<code>self._parse_no_numpy()</code>的调用本质就是其实现类自身的方法，所以就有了这种看似奇怪的父调子写法。</p><p>　　当执行完数据解析后，我们已经得到DataFrame，那么先别着急往下看，在Debug下看看此时解析出来的结果如何：</p><p><img src="/images/placeholder.png" alt="parse解析结果" data-src="./parse解析结果.png" class="lazyload"></p><p><img src="/images/placeholder.png" alt="parse解析字段类型" data-src="./parse解析字段类型.png" class="lazyload"></p><p>　　对比发现，走到这一步时解析结果和字段类型都和我们原始的数据集保持一致，所以可以肯定数据的解析逻辑是没有问题的，那么跟着源码继续往下走，就来到frame轴类型转化的过程；截止目前个人还是不太明白这层处理的意义是什么；因为在对index和column进行数据类型转化时，index列的类型是int64，而column的名字也都是字符串从而导致尝试类型转化无效。所以对于这处有了解的环境补充和指教。因为这次的逻辑判断可通过显示的控制，且经过测试后发现执行对结果并无影响，因此在这里不做过多的讨论。</p><p>　　最后来看看parse的最后一个职能：尝试对数据进行类型转化。该方法在父类的实现只是简单的异常捕获，具体的处理逻辑在对应的子类中实现，在这里看一下FrameParser类中方法<code>_try_convert_types()</code>的源码实现和对应的方法间调用时序：</p><p><img src="/images/placeholder.png" alt="FrameParse._try_convert_types源码.png" data-src="./FrameParse._try_convert_types源码.png" class="lazyload"></p><p><img src="/images/placeholder.png" alt="try_convert_types时序" data-src="./try_convert_types时序.png" class="lazyload"></p><p>　　在FrameParse中，会对数据进行两次转化尝试：首先会尝试进行日期类型的转化，其次会对数据进行数值类型转化。在日期类型的尝试转化中，是基于特殊命名的列数据进行处理，具体包括列名以“_at”，“_time”结尾、或者以“timestamp”开头或者列名等于“modified”，“date”， “datetime”。因为这种处理对最终结果不会产生影响，所以在这里不做过多讨论。</p><p>　　跳过日期类型转化后，就来到最后一步，数据的数值类型转化尝试。方法<code>_process_converter()</code>可以抽象的理解为一个数据转化工具类，负责对数据集中的每一列数据按照指定的转化规则进行转化尝试；该方法的第一个参数类型是一个方法，作用就是指定需要对数据列做哪种转化。在这里传入父类的<code>Parse._try_convert_data()</code>方法，该方法的作用就是尝试将数据转化成数值类型；该方法的源码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_try_convert_data</span><span class="params">(self, name, data, use_dtypes=True,</span></span></span><br><span class="line"><span class="function"><span class="params">                      convert_dates=True)</span>:</span></span><br><span class="line">    <span class="string">""" try to parse a ndarray like into a column by inferring dtype """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># don't try to coerce, unless a force conversion</span></span><br><span class="line">    <span class="keyword">if</span> use_dtypes:</span><br><span class="line">        <span class="keyword">if</span> self.dtype <span class="keyword">is</span> <span class="keyword">False</span>:</span><br><span class="line">            <span class="keyword">return</span> data, <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">elif</span> self.dtype <span class="keyword">is</span> <span class="keyword">True</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line"></span><br><span class="line">            <span class="comment"># dtype to force</span></span><br><span class="line">            dtype = (self.dtype.get(name)</span><br><span class="line">                     <span class="keyword">if</span> isinstance(self.dtype, dict) <span class="keyword">else</span> self.dtype)</span><br><span class="line">            <span class="keyword">if</span> dtype <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    dtype = np.dtype(dtype)</span><br><span class="line">                    <span class="keyword">return</span> data.astype(dtype), <span class="keyword">True</span></span><br><span class="line">                <span class="keyword">except</span>:</span><br><span class="line">                    <span class="keyword">return</span> data, <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> convert_dates:</span><br><span class="line">        new_data, result = self._try_convert_to_date(data)</span><br><span class="line">        <span class="keyword">if</span> result:</span><br><span class="line">            <span class="keyword">return</span> new_data, <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line">    result = <span class="keyword">False</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> data.dtype == <span class="string">'object'</span>:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># try float</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            data = data.astype(<span class="string">'float64'</span>)</span><br><span class="line">            result = <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> data.dtype.kind == <span class="string">'f'</span>:</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> data.dtype != <span class="string">'float64'</span>:</span><br><span class="line"></span><br><span class="line">            <span class="comment"># coerce floats to 64</span></span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                data = data.astype(<span class="string">'float64'</span>)</span><br><span class="line">                result = <span class="keyword">True</span></span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">                <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># do't coerce 0-len data</span></span><br><span class="line">    <span class="keyword">if</span> len(data) <span class="keyword">and</span> (data.dtype == <span class="string">'float'</span> <span class="keyword">or</span> data.dtype == <span class="string">'object'</span>):</span><br><span class="line"></span><br><span class="line">        <span class="comment"># coerce ints if we can</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            new_data = data.astype(<span class="string">'int64'</span>)</span><br><span class="line">            <span class="keyword">if</span> (new_data == data).all():</span><br><span class="line">                data = new_data</span><br><span class="line">                result = <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># coerce ints to 64</span></span><br><span class="line">    <span class="keyword">if</span> data.dtype == <span class="string">'int'</span>:</span><br><span class="line"></span><br><span class="line">        <span class="comment"># coerce floats to 64</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            data = data.astype(<span class="string">'int64'</span>)</span><br><span class="line">            result = <span class="keyword">True</span></span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> data, result</span><br></pre></td></tr></table></figure><p>　　回顾一下文章一开始提到的例子，具体的调用方法为<code>pd.read_json(filePath)</code>，则通过查看源码的参数注解可知dtype默认为True，此时在方法<code>_try_convert_data</code>里，由于user_dtypes和dtype同为True，convert_dates为False，所以代码直接跳过前面的逻辑判断和时间类型转化尝试，直接进入数值类型的转化尝试中，读完源码就可以看到数据会先尝试转化成float64类型，然后尝试转化为int64类型。通过一步步的Debug，也终于找到问题发生的根源：当代码经过如下位置时，查看一下此时对应的数据结果，如下图所示：</p><p><img src="/images/placeholder.png" alt="str转float上溢" data-src="./str转float上溢.png" class="lazyload"></p><p>　　看到这也就真想大白了：基于<code>pd.read_json(path)</code>这种写法，底层会对每列数据进行数值类型转化尝试；又因为原始数据集中的userId是数值类型的字符串，所以在将Object转为float64时不会报错，从而再经过后面的int类型的转化，从而导致我们的最终看到的数据类型发生变化。我们可以看到telephone的类型发生了变化，但是数据类型缺没有发生改变，而userId的内容都发生的奇怪的变化，这个原因又是什么呢？</p><p>　　其实这个问题的本质和Python和Pandas就没太大关系了，要弄清这个原因，就需要从计算机存储浮点数的机制说起。因为Python的float类型是存在IEEE 745标准，因此这里的float64即就是双精度浮点数，所以在内存中，每个双精度浮点数所占用的总位数为64位，符号位占1位，阶数占11位，尾数占52位，那么：2<sup>52</sup>=4503599627370496，即双精度浮点数的有效位数为16位。由于userId是一个19位的字符串，所以在做类型转化的时候会因为浮点数上溢现象导致数据失真；这就是为什么经过<code>astype(&#39;float64&#39;)</code>后数据内容发生变化的原因。</p><p>　　既然在实际应用中，无法保证、要求或者约束原始数据集，那么如果规避这种因为浮点数上溢带来的数据失真的情况呢？我们再来看一下那个数据类型转化的方法<code>_try_converty_data()</code>的源码：</p><p><img src="/images/placeholder.png" alt="try_convert_data源码_解析判断" data-src="./try_convert_data源码_解析判断.png" class="lazyload"></p><p>　　通过阅读上下文的代码可知，user_dtyps恒为True，但是dtype可以让用户显示的指定，只是如果不指定默认为True，从而导致浮点数存储上溢的情况；当再次看到这里的判断逻辑不难发现，如果把dtype设置为False，则可以完全避免数据类型尝试转化的过程，从而可以保证数据的真实性和有效性；与此同时，通过查阅<code>pd.read_josn()</code>的参数注解可知道，dtype可以为Boolean型，同样也可以为Dict类型，结合源码可以发现可以自定义指定需要转化的数据列和数据类型；这样我们也可以通过显示的指定userId的转化类型来规避这种上溢带来的问题。上述两种方法都是有效的，在这里我以第二种为例，重修修改一下代码：</p><p><img src="/images/placeholder.png" alt="read_json自定义dtype" data-src="./read_json自定义dtype.png" class="lazyload"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>　　在基于pandas处理数据时，尤其是通过读取外部数据源来做分析时，一定要注意数据类型的转化问题，避免出现类似这种因为底层数据存储溢出导致数据失真、或者数据类型变化导致的错误【Eg：pd.read_json()，pd.read_csv()】。</p><p>　　最后在顺便吐槽一下，pandas底层的这个设定也太过于奇葩；应该将read_json()方法中的参数dtype的默认值设置为False，让用户去显示的做类型转化；而不应该为了凸显在数据处理上的便利性，去容忍这种这种数据溢出的潜在Bug(亦或是pandas的开发者压根没意识到这个问题 ~~~ 哈哈 ^v^)。</p>]]></content>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 数据分析 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>基于centOS 7搭建FTP服务器</title>
      <link href="/2018/07/03/%E5%9F%BA%E4%BA%8EcentOS-7%E6%90%AD%E5%BB%BAFTP%E6%9C%8D%E5%8A%A1%E5%99%A8/"/>
      <content type="html"><![CDATA[<p>分别基于匿名用户、本地用户、虚拟用户这三种登录模式详解FTP的搭建过程</p><a id="more"></a><p>　　在开始接触服务器搭建的时候，参考过网上很多资料，不过内容五花八门，而且表述也不清楚。归根结底，是因为自己在动手搭建的时候并没有对FTP有一个初步的认识；没有正确理解参考博客所对应的配置模式，从而导致同时参考多份博客的配置但最终结果已然无效。所以在此总结一下自己的搭建心得，分别对不同的登录模式进行归纳总结。</p><p>关于FTP的工具类详见<a href="https://github.com/YHYR/SpringBoot-FTPDemo" target="_blank" rel="noopener">Github</a></p><h1 id="安装FTP"><a href="#安装FTP" class="headerlink" title="安装FTP"></a>安装FTP</h1><p>首先检查本机是否装有FTP服务器，命令：<code>rpm -qa | grep vsftpd</code>；效果如下图所示：</p><p><img src="./1.png" alt="ftp服务查看"></p><p>如果没有，则执行如下命令进行安装:</p><p><code>yum -y install vsftpd</code></p><p>若想卸载FTP，执行命令：<code>rpm -e vsftpd-***</code>；执行效果如下图所示：</p><p><img src="./2.png" alt="卸载ftp服务"></p><p><strong><em>删除后会保留主要的配置文件，并命名为.rpmsave后缀；如不需要，则可自行手动删除</em></strong>。</p><h1 id="防火墙、SELinux-设置"><a href="#防火墙、SELinux-设置" class="headerlink" title="防火墙、SELinux 设置"></a>防火墙、SELinux 设置</h1><h3 id="防火墙设置"><a href="#防火墙设置" class="headerlink" title="防火墙设置"></a>防火墙设置</h3><p>首先保证开启firewalld服务：<code>systemctl start firewalld.service</code></p><p>然后分别执行如下命令</p><p><code>firewall-cmd --permanent --zone=public --add-service=ftp</code></p><p><code>firewall-cmd --reload</code></p><p>如果执行firewall-cmd命令报错：<code>ImportError: No module named gi.repository</code>，如下所示：</p><p><img src="./3.png" alt="firewalld报错-"></p><p>则可通过修改/usr/bin/firewall-cmd文件，将第一行的<code>#!/usr/bin/python -Es</code>修改为<code>#!/usr/bin/python2.7 -Es</code>即可解决该问题，因为CentOS 7默认的python版本是2.7</p><p><img src="./4.png" alt="firewall-cmd文件修改"></p><h3 id="SELinux设置"><a href="#SELinux设置" class="headerlink" title="SELinux设置"></a>SELinux设置</h3><blockquote><p>a) 临时关闭：<code>setenforce 0</code></p><p>b) 永久关闭：修改/etc/selinux/config文件中，设置SELINUX=disable，并重启服务器</p></blockquote><h1 id="匿名用户登录"><a href="#匿名用户登录" class="headerlink" title="匿名用户登录"></a>匿名用户登录</h1><p>　　用户登录时不需要用户名和密码，就可直接进入FTP服务器；默认的用户名为ftp，密码为空；在该模式下，默认的文件存储路径为：/var/ftp</p><p>　　该目录下有一个pub文件夹，若想上传文件到该目录下，则需要修改pub目录的所属组用户信息和目录权限信息；若要上传目录到当前根目录(即：/var/ftp)，则需要修改ftp目录的组用户信息和权限信息；这里以pub目录为例：</p><blockquote><p>修改目录的所属组用户信息：<code>chown -R ftp:ftp /var/ftp/pub</code></p><p>修改目录的权限信息：<code>chmod -R 777 /var/ftp/pub</code></p></blockquote><p>　　其次，在/etc/vsftpd下修改ftp的配置文件vsftpd.conf；在修改前，尽量养成修改原始配置文件的习惯(<code>cp vsftpd.conf vsftpd.conf.bak</code>)。</p><p>　　默认情况下FTP的登录模式就是匿名登录，即：<code>anonymous_enable=YES</code>；如下图所示：</p><p><img src="./5.png" alt="默认配置文件"></p><p>　　在配置文件中添加文件根目录、上传权限和写权限，如下图所示：</p><p><img src="/images/placeholder.png" alt="匿名模式配置文件" data-src="./6.png" class="lazyload"></p><p>　　到此匿名登录的配置已完成，可以通过<code>systemctl start vsftpd.service</code>、<code>systemctl stop vsftpd.service</code>、<code>systemctl restart vsftpd.service</code>分别开启、关闭和重启ftp服务，进而可以开始的上传和下载任务。</p><h1 id="本地用户登录"><a href="#本地用户登录" class="headerlink" title="本地用户登录"></a>本地用户登录</h1><p>　　本地用户登录是指使用当前系统中所存在的用户来作为登录FTP服务器的认证信息。</p><p>　　FTP默认是开启了本地用户登录模式，即：<code>local_enable=YES</code>；如下图所示：</p><p><img src="/images/placeholder.png" alt="本地登录模式原始配置信息" data-src="./7.png" class="lazyload"></p><p>　　目前系统中存在一个用户：yhyr</p><p>　　修改FTP配置文件 <code>vi /etc/vsftpd/vsftpd.conf</code>，添加如下两行配置(需要关闭匿名登录模式，即设置anonymous_enable=NO)：</p><p><img src="/images/placeholder.png" alt="本地用户登录新增配置" data-src="./8.png" class="lazyload"></p><p>　　<code>chroot_local_user=YES</code>表示限制所有用户都只能访问该用户的home目录；前提条件是配置文件里没有配置local_root属性；如果配置了local_root属性，则该用户只能访问local_root所指向的路径。当配置了chroot_local_user=YES，则一定要顺便配置一下allow_writeable_chroot=YES；否则在用户登录的时候会报如下错误：</p><p><img src="/images/placeholder.png" alt="本地用户登录7" data-src="./9.png" class="lazyload"></p><p>　　到此本地登录模式已经基本配置完成，接下来验证一下；在终端连接ftp：ftp localhost，如下所示：</p><p><img src="/images/placeholder.png" alt="本地用户连接ftp" data-src="./10.png" class="lazyload"></p><p>　　按照提示输入用户名：yhyr</p><p><img src="/images/placeholder.png" alt="本地用户登录输入用户名" data-src="./11.png" class="lazyload"></p><p>　　输入该用户的密码，反馈信息如下：</p><p><img src="/images/placeholder.png" alt="本地用户登录成功反馈信息" data-src="./12.png" class="lazyload"></p><p>　　这样就实现了用本地用户登录ftp；因为配置了chroot_local_user=YES，所以当前登录的目录位置即就是用户yhyr的根目录，即：/home/yhyr；且只能访问当前目录及其子目录，并不能访问其他用户的目录或者系统目录(eg：执行cd ..的操作是无效的)</p><p><img src="/images/placeholder.png" alt="本地用户无权限访问其他目录" data-src="./13.png" class="lazyload"></p><h2 id="本地用户访问权限控制"><a href="#本地用户访问权限控制" class="headerlink" title="本地用户访问权限控制"></a>本地用户访问权限控制</h2><p><strong>配置项解析</strong></p><blockquote><p><code>userlist_enable=YES</code>：表示用/etc/vsftpd下的user_list名单来限制可访问的用户</p><p><code>userlist_deny=NO</code>：表示只允许user_list名单中的用户可以访问；相反当<code>userlist_deny=YES</code>时，表示user_list名单中的用户不可访问</p></blockquote><p>　　user_list是安装ftp时默认带有的一个文件，里面主要包含了诸如root、sync、games、nobody等系统用户，如下图所示：</p><p><img src="/images/placeholder.png" alt="user_list信息" data-src="./14.png" class="lazyload"></p><p>　　Eg：在配置文件中添加上述两项配置项，如下图所示：</p><p><img src="/images/placeholder.png" alt="本地用户添加登录权限控制" data-src="./15.png" class="lazyload"></p><p>　　重启服务后，分别用yhyr用户和root用户来做测试，你会发现用yhyr用户登录时在输入完用户名后就直接报错：<font color="red">530 Permission denied<font>；而用root用户登录时，当输入完密码后也会报错：<font color="red">530 Login incorrect</font>；如下图所示：</font></font></p><p><img src="/images/placeholder.png" alt="本地用户登录权限验证" data-src="./16.png" class="lazyload"></p><p>　　yhyr用户在登录时在直接拒绝就正是因为添加了登录权限的配置，因为userlist_deny设置为NO，所以除user_list中所包含的用户外，其他用户一律拒绝，因此会报Permission denied错误；而root用户登录时，你会发现该用户其实是有访问权限的，因为此时登录的root用户不像刚才的yhyr用户，是给了你输入密码的机会，但是当输入完密码后，报出的错误Login incorrect并不是因为该用户没权限或者改配置没生效，而是因为系统默认是禁用/etc/vsftpd/ftpusers中的用户，查看ftpusers可以看到，root用户模式是存在于ftpusers中的。</p><p><img src="/images/placeholder.png" alt="系统禁用ftpusers中的用户" data-src="./17.png" class="lazyload"></p><p>这个问题其实很好解决，而且解决的方案也很多：</p><p>　　方案一：对比观察user_list和ftpusers可以发现其实内容是一模一样的，所以不难看出设计者是觉得这些系统级的用户是不应该暴露出去供外部使用的；而我们的实际应用场景也应该是分配一个或多个低权限的账户供用户使用，因此可以改变原有的配置项，在开启用户登录认证(userlist_enable=YES)的同时，设置userlist_deny为YES，这样默认的系统级用户则变为不可用的，就可以使用自定用的用户来作为认证信息。</p><p>　　方案二：在设置userlist_deny为NO的同时，把需要授权的用户名添加到user_list中，并保证在ftpusers中不存在即可；例如上述例子中如果想让root用户成功登陆，则只需要把ftpusers中的root那一行删除即可(<em>不建议使用该方法</em> )。</p><h2 id="自定义数据存放路径"><a href="#自定义数据存放路径" class="headerlink" title="自定义数据存放路径"></a><font color="red">自定义数据存放路径</font></h2><p><strong>配置项解析</strong></p><blockquote><p><code>chroot_local_user=YES</code>：代表使用登陆用户的home目录作为路径，eg：用yhyr用户登录，则默认的文件路径为：/home/yhyr</p><p><code>local_root=xxx</code>：代表用户登录时指定文件目录为local_root所设置的目录；且local_root配置项的优先级高于chroot_local_user，即就是当同时设置了这两个参数，以local_root配置项为准</p></blockquote><p><em><font color="red">local_root所指定的路径和所登录的用户的权限信息必须匹配，否则会出现由于文件权限问题不对而导致登录失败的问题</font></em></p><p>　　Eg：设置local_root=/home/test，并用yhyr用户做测试，因为/home/test目录所属的用户组信息不是yhyr，所以yhyr用户并没有访问该目录的权限，因此在此场景下登录会报错：</p><p><img src="/images/placeholder.png" alt="用户文件路径权限不对导致登录失败" data-src="./18.png" class="lazyload"></p><h2 id="多用户数据目录隔离"><a href="#多用户数据目录隔离" class="headerlink" title="多用户数据目录隔离"></a><font color="red">多用户数据目录隔离</font></h2><p>　　在实际工作中，往往有这样的需求：一个FTP服务器会设置多个用户，且希望每个用户的文件存放路径都不一样。接下来分析一下这种业务场景如何实现。</p><p>　　首先在系统中新建一个test用户：<code>useradd -d /home/test test</code></p><p>　　然后设置test用户的密码：<code>passwd test</code></p><p>　　在这里首先注释掉上文中所配置的有关登录权限控制的两行配置，并重启服务后，用test用户测试，可以看到登录没有问题：</p><p><img src="/images/placeholder.png" alt="test用户登录" data-src="./19.png" class="lazyload"></p><p>　　因为我们配置了<code>chroot_local_user=YES</code>，所以test用户登陆进来后肯定是在/home/test目录下，这样yhyr用户和test用户的目录这不正好不冲突吗？这个问题不就不是个问题吗？</p><p>　　实则不然，结合实际的应用场景，我们不难发现，在实际的生产服务器里，往往都是会外挂一个容量很大的磁盘用来存放数据，而操作系统中诸如/home这类目录的大小通常都很小，而Linux的用户根目录也都是在/home下的，如果一次来作为存放数据的目录，想必要不了多久就会磁盘爆掉；因此我这里说指的目录隔离是基于此业务背景下，结合上文提到的<code>local_root</code>参数来实现多用户的目录隔离功能。</p><p>　　通过上文我们可以知道，通过local_root可以指定我们的数据目录，但是从上述的分析中可以看出是只能指定一个路径，对于多个用户登录，就必然会有一个或多个用户登录时出现“500  cannot change directory”的错误。如何实现目录隔离呢？在这里需要介绍一个新的配置项：</p><blockquote><p>user_config_dir：该配置项指定一个文件夹路径，该文件夹下存放各本地用户的配置文件信息；用户配置文件的名字与用户名保持一致</p></blockquote><h3 id="Step-1：指定用户数据存储目录"><a href="#Step-1：指定用户数据存储目录" class="headerlink" title="Step 1：指定用户数据存储目录"></a>Step 1：指定用户数据存储目录</h3><p>　　在根目录下新建一个data文件夹，模拟我们实际应用中的外挂数据盘：<code>mkdir /data</code></p><p>　　在该目录下创建ftp_data/test和ftp_data/yhyr文件夹，分别代表test用户和yhyr用户所对应的数据存放目录，然后分别在test和yhyr目录下各创建一个文件，便于后面的演示，如下图所示：</p><p><img src="/images/placeholder.png" alt="多用户隔离数据目录" data-src="./20.png" class="lazyload"></p><h3 id="Step-2：指定用户的数据存储路径"><a href="#Step-2：指定用户的数据存储路径" class="headerlink" title="Step 2：指定用户的数据存储路径"></a>Step 2：指定用户的数据存储路径</h3><p>　　然后在/etc/vsftpd路径下新建一个userconf目录，并在该目录下分别新建一个test文件和yhyr文件(文件名和用户名保持一致)，分别在不同的文件中配置local_root参数，用来指定该用户的数据存储路径：</p><p><img src="/images/placeholder.png" alt="user_config_dir文件配置" data-src="./21.png" class="lazyload"></p><p>　　并在配置文件中添加user_config_dir配置项，指向我们设定的userconf目录：</p><p><img src="/images/placeholder.png" alt="添加user_config_dir配置项" data-src="./22.png" class="lazyload"></p><p>　　重启服务后，分别用test用户和yhyr用户进行验证：</p><p><img src="/images/placeholder.png" alt="验证多用户目录隔离" data-src="./23.png" class="lazyload"></p><p>到此本地用户登录的配置以分享完成</p><h1 id="虚拟用户登录"><a href="#虚拟用户登录" class="headerlink" title="虚拟用户登录"></a>虚拟用户登录</h1><p>　　顾名思义，该用户不是真实存在于系统中的用户；但是<font color="red">与匿名用户登录不同，虚拟用户登录时需要密码认证</font>。当你了解了上文提到过的本地用户登录的配置原理后，就会很容易理解虚拟用户的配置。</p><p>具体配置如下所示(需要关闭匿名登录模式，即设置anonymous_enable=NO)：</p><p><img src="/images/placeholder.png" alt="虚拟用户配置文件" data-src="./24.png" class="lazyload"></p><p><strong>配置项解析</strong></p><blockquote><p><code>guest_enable=YES</code>：表示开始虚拟用户登录模式</p><p><code>guest_username=yhyr</code>：指定虚拟用户所依赖的本地用户的用户名</p><p><code>user_config_dir=xxx</code>：指定不同虚拟用户的详细配置信息</p></blockquote><p>具体操作与本地用户中的多用户数据目录隔离的配置方式类似</p><h3 id="Step-1-创建虚拟用户的数据存储目录"><a href="#Step-1-创建虚拟用户的数据存储目录" class="headerlink" title="Step 1: 创建虚拟用户的数据存储目录"></a>Step 1: 创建虚拟用户的数据存储目录</h3><p>假设有两个虚拟用户：</p><p>guest_user_1对应的数据存储目录为/data/ftp_data/guest_user_1</p><p>guest_user_1对应的数据存储目录为/data/ftp_data/guest_user_2</p><font color="red">注：需要把数据目录的用户组信息设置为guest_username所指定的用户</font><p>Eg：<code>chown yhyr:yhyr guest_user_1</code>、<code>chown yhyr:yhyr guest_user_2</code></p><h3 id="Step-2：指定虚拟用户的数据存储路径"><a href="#Step-2：指定虚拟用户的数据存储路径" class="headerlink" title="Step 2：指定虚拟用户的数据存储路径"></a>Step 2：指定虚拟用户的数据存储路径</h3><p>　　在/etc/vsftpd目录下新建一个guestuserconf目录，并在该目录下分别新建两个文件：guest_user_1和guest_user_2(文件名和虚拟用户的用户名保持一致)。在文件中添加local_root配置项指定该虚拟用户的数据存储路径：</p><p><img src="/images/placeholder.png" alt="虚拟用户user_config_dir配置" data-src="./25.png" class="lazyload"></p><h3 id="Step-3：设置虚拟用户的用户名和密码"><a href="#Step-3：设置虚拟用户的用户名和密码" class="headerlink" title="Step 3：设置虚拟用户的用户名和密码"></a>Step 3：设置虚拟用户的用户名和密码</h3><p>　　<font color="red">首先在/etc/vsftpd下新建一个guest_user_passwd文件</font>，该文件记录虚拟用户的用户名和密码信息(<strong>注：奇数行为用户名，偶数行为密码</strong>)，如下图所示，设置guest_user_1的密码为123456，设置guest_user_2的密码为abcdef</p><p><img src="/images/placeholder.png" alt="虚拟登录用户名和密码配置" data-src="./26.png" class="lazyload"></p><p><font color="red">然后生成虚拟用户认证的db文件</font>，命令如下所示：</p><p><code>db_load -T -t hash -f /etc/vsftpd/guest_user_passwd /etc/vsftpd/guest_user_passwd.db</code></p><p>　　<font color="red">最后修改/etc/pam.d/vsftpd文件，对虚拟用户授权</font>。注释掉<code>auth required pam_shells.so</code>、<code>auth include password-auth</code>、<code>account include password-auth</code>；并追加如下两行：  <code>auth required pam_userdb.so db=/etc/vsftpd/guest_user_passwd</code>和<code>account required pam_userdb.so db=/etc/vsftpd/guest_user_passwd</code></p><p><img src="/images/placeholder.png" alt="虚拟模式修改vsftpd文件" data-src="./27.png" class="lazyload"></p><p>然后重启服务，分别用guest_user_1和guest_user_2用户验证：</p><p><img src="/images/placeholder.png" alt="虚拟登录验证" data-src="./28.png" class="lazyload"></p><p>　　登录验证已经ok了，接下来只需要对虚拟用户赋予文件的读写权限即可。分别修改guestuserconf中各虚拟用户的配置文件，追加如下配置项，就可以正常的上传、下载和新建目录/文件。</p><blockquote><p>write_enable=YES</p><p>anon_upload_enable=YES</p><p>anon_other_write_enable=YES</p><p>anon_mkdir_write_enable=YES</p></blockquote><p><img src="/images/placeholder.png" alt="虚拟用户文件权限配置" data-src="./29.png" class="lazyload"></p>]]></content>
      
      
        <tags>
            
            <tag> Linux </tag>
            
            <tag> FTP </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>基于Python初探Linux下的僵尸进程和孤儿进程(三)</title>
      <link href="/2018/06/07/%E5%9F%BA%E4%BA%8EPython%E5%88%9D%E6%8E%A2Linux%E4%B8%8B%E7%9A%84%E5%83%B5%E5%B0%B8%E8%BF%9B%E7%A8%8B%E5%92%8C%E5%AD%A4%E5%84%BF%E8%BF%9B%E7%A8%8B-%E4%B8%89/"/>
      <content type="html"><![CDATA[<p>基于kafka+python实现消息多进程消费的应用场景探究新建子进程时僵尸进程自动消除的原因，并由此初探multiprocessing.Process.start()源码。</p><a id="more"></a><h1 id="场景描述"><a href="#场景描述" class="headerlink" title="场景描述"></a>场景描述</h1><p>　　在实现kafka消息的多进程消费时(即主进程用来获取消息，每条消息都会新起一个子进程来执行具体的业务逻辑)，存在新起一个子进程时会消除系统中遗留僵尸进程的情况，具体问题如下所述：</p><ul><li>a) 当消费一条消息时会生成一个子进程，且子进程结束后会变成僵尸进程；当再消费一条消息时，之前遗留的僵尸进程会消除，同时重新生成一个子进程来执行业务逻辑，结束后又变成僵尸进程。(以此类推)</li><li>b) 如果一次性消费多条消息，则会一次性生成多个子进程，且运行结束后会多个子进程均会变成僵尸进程；当再消费一条(多条)消息时，之前系统中遗留的多个僵尸进程均会被清除，并重新产生一个(多个)子进程来执行业务逻辑，结束后又会变成一个(多个)僵尸进程</li></ul><p>　　此应用场景的代码是<strong>基于未主动剔除僵尸进程的前提下</strong>实现的，<font color="red">重点讨论僵尸进程的自动消除的原因</font>；样例代码如下所示(若要探究如何消除僵尸进程，可详见<a href="https://yhyr.github.io/2018/06/06/%E5%9F%BA%E4%BA%8EPython%E5%88%9D%E6%8E%A2Linux%E4%B8%8B%E7%9A%84%E5%83%B5%E5%B0%B8%E8%BF%9B%E7%A8%8B%E5%92%8C%E5%AD%A4%E5%84%BF%E8%BF%9B%E7%A8%8B-%E4%BA%8C/" target="_blank" rel="noopener">传送门</a>)：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> kafka <span class="keyword">import</span> KafkaConsumer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ConsumerUtil</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, broker_list, topic_name, group_name=<span class="string">'consumer_group_1'</span>, api_version=<span class="string">'0.10'</span>, auto_offset_reset=<span class="string">'latest'</span>)</span>:</span></span><br><span class="line">        self.broker_list = broker_list</span><br><span class="line">        self.topic_name = topic_name</span><br><span class="line">        self.group_name = group_name</span><br><span class="line">        self.api_version = api_version</span><br><span class="line">        self.auto_offset_reset = auto_offset_reset</span><br><span class="line"></span><br><span class="line">        self.consumer = KafkaConsumer(self.topic_name, group_id=self.group_name, bootstrap_servers=self.broker_list, enable_auto_commit=<span class="keyword">True</span>, api_version=self.api_version, auto_offset_reset=self.auto_offset_reset)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">consumer_fun</span><span class="params">(self, message_consumer)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> msg <span class="keyword">in</span> self.consumer:</span><br><span class="line">            message_consumer(msg.value)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MainProcess</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, broker_list, topic_name)</span>:</span></span><br><span class="line">        self.broker_list = broker_list</span><br><span class="line">        self.topic_name = topic_name</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">consume_task</span><span class="params">(self, msg)</span>:</span></span><br><span class="line">        p = ChildProcess(msg)</span><br><span class="line">        print(<span class="string">'main process fork a new child process'</span>)</span><br><span class="line">        p.start()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">excutor</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'main process pid=&#123;0&#125;, ppid=&#123;1&#125; Begin'</span>.format(os.getpid(), os.getppid()))</span><br><span class="line">        ConsumerUtil(self.broker_list, self.topic_name).consumer_fun(self.consume_task)</span><br><span class="line">        print(<span class="string">'main process pid=&#123;0&#125;, ppid=&#123;1&#125; End'</span>.format(os.getpid(), os.getppid()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChildProcess</span><span class="params">(multiprocessing.Process)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, msg)</span>:</span></span><br><span class="line">        multiprocessing.Process.__init__(self)</span><br><span class="line">        self.msg = msg</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'child process pid=&#123;0&#125;, ppid=&#123;1&#125; Begin consumer msg, '</span>.format(os.getpid(), os.getppid()))</span><br><span class="line">        print(<span class="string">'child process pid=&#123;0&#125;, ppid=&#123;1&#125; Is being consumer msg=&#123;2&#125;'</span>.format(os.getpid(), os.getppid(), self.msg))</span><br><span class="line">        time.sleep(<span class="number">5</span>)</span><br><span class="line">        print(<span class="string">'child process pid=&#123;0&#125;, ppid=&#123;1&#125; End consumer msg'</span>.format(os.getpid(), os.getppid()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    action = MainProcess(<span class="string">'127.0.0.1:9092'</span>, <span class="string">'demo_topic'</span>)</span><br><span class="line">    action.excutor()</span><br></pre></td></tr></table></figure><h1 id="场景抽象"><a href="#场景抽象" class="headerlink" title="场景抽象"></a>场景抽象</h1><p>　　为了方便举例，将上述实际业务场景抽象成一个简单的demo：当主进程开始启动时，会新建一个子进程，此时子父进程是并行执行；当主进程执行了10秒后会再启动一个子进程(此处模拟再次消费kafka消息)，在新建子进程的时候上一个子进程已经结束；样例代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> signal</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MainProcess</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, main_process_time, child_process_time)</span>:</span></span><br><span class="line">        self.main_process_time = main_process_time</span><br><span class="line">        self.child_process_time = child_process_time</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">excutor</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'main process begin, pid=&#123;0&#125;, ppid=&#123;1&#125;'</span>.format(os.getpid(), os.getppid()))</span><br><span class="line">        p = ChildProcess(self.child_process_time)</span><br><span class="line">        p.start()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.main_process_time):</span><br><span class="line">            print(<span class="string">'main process, pid=&#123;0&#125;, ppid=&#123;1&#125;, times=&#123;2&#125;'</span>.format(os.getpid(), os.getppid(), i))</span><br><span class="line">            time.sleep(<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">10</span>:</span><br><span class="line">                p = ChildProcess(self.child_process_time)</span><br><span class="line">                p.start()</span><br><span class="line"></span><br><span class="line">        print(<span class="string">'main process end, pid=&#123;0&#125;, ppid=&#123;1&#125;'</span>.format(os.getpid(), os.getppid()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChildProcess</span><span class="params">(multiprocessing.Process)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, process_time)</span>:</span></span><br><span class="line">        multiprocessing.Process.__init__(self)</span><br><span class="line">        self.process_time = process_time</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'child process begin, pid=&#123;0&#125;, ppid=&#123;1&#125;'</span>.format(os.getpid(), os.getppid()))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.process_time):</span><br><span class="line">            print(<span class="string">'child process pid=&#123;0&#125;, ppid=&#123;1&#125;, times=&#123;2&#125;'</span>.format(os.getpid(), os.getppid(), i))</span><br><span class="line">            time.sleep(<span class="number">1</span>)</span><br><span class="line">        print(<span class="string">'child process end, pid=&#123;0&#125;, ppid=&#123;1&#125;'</span>.format(os.getpid(), os.getppid()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main_process_time = <span class="number">30</span></span><br><span class="line">    child_process_time = <span class="number">5</span></span><br><span class="line">    action = MainProcess(main_process_time, child_process_time)</span><br><span class="line">    action.excutor()</span><br></pre></td></tr></table></figure><p>代码的执行逻辑如下所述：</p><p>主进程启动后初始化一个子进程(主进程的执行周期远大于子进程)，当子进程尚未结束时，控制台的输出结果和进程状态如下图所示(<font color="red">注意子进程的进程状态</font>)：</p><p><img src="./第一次新建子进程且未执行完成.png" alt="第一次新建子进程且未执行完成"></p><p>当子进程结束后，父进程继续执行且尚未第二次新建子进程的时候，控制台的输出结果和进程状态如下图所示(<font color="red">注意子进程的进程状态</font>)：</p><p><img src="./第一次新建子进程且子进程执行结束.png" alt="第一次新建子进程且子进程执行结束"></p><p>当父进程第二次新建一个子进程，且子父进程并行执行时，控制台的输出结果和进程状态如下图所示(<font color="red">注意子进程的进程号</font>)：</p><p><img src="./第二次新建子进程且未执行完成.png" alt="第二次新建子进程且未执行完成"></p><p>当第二次新建的子进程结束且父进程尚未结束时，控制台的数据结果和进程状态如下图所示(<font color="red">注意子进程的进程状态</font>)：</p><p><img src="./第二次新建子进程且子进程执行完成.png" alt="第二次新建子进程且子进程执行完成"></p><h1 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h1><p>　　根据实验现象不难看出，每当新建一个子进程的时候，就会清楚掉之前所有的僵尸进程(特指该父进程下的所有僵尸进程)。而造成此现象的真正原因就在于新起子进程的这个动作。基于multiprocessing新建子进程的方式是p.start()，该方法的源码如下所示：</p><p><img src="./start源码.png" alt="start源码"></p><p>　看源码可知start主要干了三件事：</p><ul><li>1) 调用_cleanup()方法</li><li>2) 基于Popen初始化一个进程</li><li>3) 将初始化的进程加到当前进程额子进程列表里。</li></ul><p>而上述实验现象产生的根本原因就在与这个_cleanup()方法。首先看一下该方法的源码：</p><p><img src="/images/placeholder.png" alt="cleanup源码" data-src="./cleanup源码.png" class="lazyload"></p><p>　　代码逻辑也很简单，遍历当前进程的子进程列表，调用子进程的poll()方法(该方法其实就是调用os.waitpid)，如果有返回状态信息，则代表该子进程是僵尸进程且已被系统清除掉，则会将该子进程从子进程列表中移除。这就是为什么每当新建一个子进程的时候，系统中遗留的僵尸进程会被清除而非一直存在。</p><p>　　接下来一起了解一下Python的Popen类；这个类在start、join和_cleanup方法中都有出现，Popen的作用其实就是新建一个进程。因为multiprocessing是跨平台的，所以在forking模块下分别基于Windows平台和非Windows平台各实现了一个Popen类，用来新建子进程，不同平台的Popen实现大致对比图如下所示：</p><p><img src="/images/placeholder.png" alt="Popen不同系统对比" data-src="./Popen不同系统对比.png" class="lazyload"></p><p>由于实际的工作都是在Linux上运行的，所以在这里着重看一下Linux上的具体实现。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Unix</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> sys.platform != <span class="string">'win32'</span>:</span><br><span class="line">    <span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line">    exit = os._exit</span><br><span class="line">    duplicate = os.dup</span><br><span class="line">    close = os.close</span><br><span class="line"></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># We define a Popen class similar to the one from subprocess, but</span></span><br><span class="line">    <span class="comment"># whose constructor takes a process object as its argument.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">Popen</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, process_obj)</span>:</span></span><br><span class="line">            sys.stdout.flush()</span><br><span class="line">            sys.stderr.flush()</span><br><span class="line">            self.returncode = <span class="keyword">None</span></span><br><span class="line"></span><br><span class="line">            self.pid = os.fork()</span><br><span class="line">            <span class="keyword">if</span> self.pid == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">if</span> <span class="string">'random'</span> <span class="keyword">in</span> sys.modules:</span><br><span class="line">                    <span class="keyword">import</span> random</span><br><span class="line">                    random.seed()</span><br><span class="line">                code = process_obj._bootstrap()</span><br><span class="line">                sys.stdout.flush()</span><br><span class="line">                sys.stderr.flush()</span><br><span class="line">                os._exit(code)</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">poll</span><span class="params">(self, flag=os.WNOHANG)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> self.returncode <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">                    <span class="keyword">try</span>:</span><br><span class="line">                        pid, sts = os.waitpid(self.pid, flag)</span><br><span class="line">                    <span class="keyword">except</span> os.error <span class="keyword">as</span> e:</span><br><span class="line">                        <span class="keyword">if</span> e.errno == errno.EINTR:</span><br><span class="line">                            <span class="keyword">continue</span></span><br><span class="line">                        <span class="comment"># Child process not yet created. See #1731717</span></span><br><span class="line">                        <span class="comment"># e.errno == errno.ECHILD == 10</span></span><br><span class="line">                        <span class="keyword">return</span> <span class="keyword">None</span></span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="keyword">break</span></span><br><span class="line">                <span class="keyword">if</span> pid == self.pid:</span><br><span class="line">                    <span class="keyword">if</span> os.WIFSIGNALED(sts):</span><br><span class="line">                        self.returncode = -os.WTERMSIG(sts)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="keyword">assert</span> os.WIFEXITED(sts)</span><br><span class="line">                        self.returncode = os.WEXITSTATUS(sts)</span><br><span class="line">            <span class="keyword">return</span> self.returncode</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">wait</span><span class="params">(self, timeout=None)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> timeout <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                <span class="keyword">return</span> self.poll(<span class="number">0</span>)</span><br><span class="line">            deadline = time.time() + timeout</span><br><span class="line">            delay = <span class="number">0.0005</span></span><br><span class="line">            <span class="keyword">while</span> <span class="number">1</span>:</span><br><span class="line">                res = self.poll()</span><br><span class="line">                <span class="keyword">if</span> res <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                remaining = deadline - time.time()</span><br><span class="line">                <span class="keyword">if</span> remaining &lt;= <span class="number">0</span>:</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">                delay = min(delay * <span class="number">2</span>, remaining, <span class="number">0.05</span>)</span><br><span class="line">                time.sleep(delay)</span><br><span class="line">            <span class="keyword">return</span> res</span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">terminate</span><span class="params">(self)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> self.returncode <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    os.kill(self.pid, signal.SIGTERM)</span><br><span class="line">                <span class="keyword">except</span> OSError, e:</span><br><span class="line">                    <span class="keyword">if</span> self.wait(timeout=<span class="number">0.1</span>) <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">                        <span class="keyword">raise</span></span><br><span class="line"></span><br><span class="line"><span class="meta">        @staticmethod</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">thread_is_spawning</span><span class="params">()</span>:</span></span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">False</span></span><br></pre></td></tr></table></figure><p>　　看源码可知，multiprocessing适配类Unix平台的本质是采用fork(对Linux进程了解的小伙伴应该都很熟悉fork)，类Unix平台的Popen类中最主要的方法就是poll()；因为在外部直接调用p.join()其实就是调用Popen.wait()方法，而Popen.wait()底层调用的就是Popen.poll()；外部调用p.start()其实就是首先调用Popen.poll()实现僵尸进程的消除，然后初始化一个Popen从而实现新建一个子进程。</p><p>　　接下来分析一下Popen类：初始化Popen时，首先会调用Unix/Linux的fork来新建一个子进程；了解过Linux进程的人都知道，调用fork后会产生一个子进程且有两个返回值，父进程的返回值为子进程的进程id，子进程返回0；可以将fork的返回值理解为所产生的子进程的pid，因为经过fork后，原本的父进程就拥有了子进程，所以父进程的返回值为子进程的pid；因为所产生的子进程没有自己的子进程，所以它的返回值为0；因此在初始化Popen时，如果fork后当前进程为父进程则直接忽略不做处理，如果是子进程，则会首先通过<code>code = process_obj._bootstrap()</code>获取到父进程的pid，然后调用<code>os._exit(code)</code>强制退出主进程，从而保证经过初始化后只有一个新建的子进程。</p><p>　　因为wait方法的本质还是调用poll方法，所以在这里重点看一点poll方法：调用系统的os.waitpid()方法来释放子进程的退出状态信息，如果成功释放则函数返回退出状态码returncode；反之如果子进程并未结束，则直接退出并返回None。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>　　multiprocessing基于Linux平台实现多进程实则是基于fork来新建子进程的；调用p.start()的实质就是new一个Popen类(该类返回新建的子进程，并关闭对应的父进程)；与此同时p.start()会主动对子进程列表做一次清理操作(该操作是非阻塞的)；调用p.join()的本质是调用forking模块的Popen.wait()方法(该操作是阻塞的)。所以理解Popen是了解Python-multiprocessing的一个比较重要的前提。附上proces模块和forking模块之间的对象模型图。</p><p><img src="/images/placeholder.png" alt="process对象模型图" data-src="./process对象模型图.png" class="lazyload"></p><h2 id="未完待续"><a href="#未完待续" class="headerlink" title="未完待续"></a>未完待续</h2><p>常见的几种进程退出的方式对比分析(sys.exit(), os._exit())</p>]]></content>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Linux </tag>
            
            <tag> 多进程 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>基于Python初探Linux下的僵尸进程和孤儿进程(一)</title>
      <link href="/2018/06/07/%E5%9F%BA%E4%BA%8EPython%E5%88%9D%E6%8E%A2Linux%E4%B8%8B%E7%9A%84%E5%83%B5%E5%B0%B8%E8%BF%9B%E7%A8%8B%E5%92%8C%E5%AD%A4%E5%84%BF%E8%BF%9B%E7%A8%8B-%E4%B8%80/"/>
      <content type="html"><![CDATA[<p><em>通过对比子父进程的执行周期来详细讨论僵尸进程产生的原因和规避方法</em></p><a id="more"></a><p>样例代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MainProcess</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, main_process_time, child_process_time)</span>:</span></span><br><span class="line">        self.main_process_time = main_process_time</span><br><span class="line">        self.child_process_time = child_process_time</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">excutor</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'main process begin, pid=&#123;0&#125;, ppid=&#123;1&#125;'</span>.format(os.getpid(), os.getppid()))</span><br><span class="line">        p = ChildProcess(self.child_process_time)</span><br><span class="line">        p.start()</span><br><span class="line">        p.join()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.main_process_time):</span><br><span class="line">            print(<span class="string">'main process, pid=&#123;0&#125;, ppid=&#123;1&#125;, times=&#123;2&#125;'</span>.format(os.getpid(), os.getppid(), i))</span><br><span class="line">            time.sleep(<span class="number">1</span>)</span><br><span class="line">        print(<span class="string">'main process end, pid=&#123;0&#125;, ppid=&#123;1&#125;'</span>.format(os.getpid(), os.getppid()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChildProcess</span><span class="params">(multiprocessing.Process)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, process_time)</span>:</span></span><br><span class="line">        multiprocessing.Process.__init__(self)</span><br><span class="line">        self.process_time = process_time</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'child process begin, pid=&#123;0&#125;, ppid=&#123;1&#125;'</span>.format(os.getpid(), os.getppid()))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.process_time):</span><br><span class="line">            print(<span class="string">'child process pid=&#123;0&#125;, ppid=&#123;1&#125;, times=&#123;2&#125;'</span>.format(os.getpid(), os.getppid(), i))</span><br><span class="line">            time.sleep(<span class="number">1</span>)</span><br><span class="line">        print(<span class="string">'child process end, pid=&#123;0&#125;, ppid=&#123;1&#125;'</span>.format(os.getpid(), os.getppid()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main_process_time = <span class="number">5</span></span><br><span class="line">    child_process_time = <span class="number">10</span></span><br><span class="line">    action = MainProcess(main_process_time, child_process_time)</span><br><span class="line">    action.excutor()</span><br></pre></td></tr></table></figure><h1 id="业务场景及现象描述"><a href="#业务场景及现象描述" class="headerlink" title="业务场景及现象描述"></a>业务场景及现象描述</h1><h2 id="场景一：子进程的运行周期大于父进程"><a href="#场景一：子进程的运行周期大于父进程" class="headerlink" title="场景一：子进程的运行周期大于父进程"></a>场景一：子进程的运行周期大于父进程</h2><h3 id="子进程不调用join-方法：无僵尸进程存在"><a href="#子进程不调用join-方法：无僵尸进程存在" class="headerlink" title="子进程不调用join()方法：无僵尸进程存在"></a>子进程不调用join()方法：无僵尸进程存在</h3><p>样例代码中main_process_time代表主进程运行时长，child_process_time代表子进程运行时长；并注释掉p.join()，代码执行逻辑如下所述：</p><p>　　父进程执行到p.start()后，子父进程开始同时执行；当父进程结束后，子进程继续执行；此时父进程并不退出依然存在，且进程状态依然为休眠状态(S+)；当子进程结束后，子父进程同时销毁。打印结果如下图所示：</p><p><img src="./子进程运行周期长，且不调用join.png" alt="子进程运行周期长，且不调用join"></p><h3 id="子进程调用join-方法：无僵尸进程存在"><a href="#子进程调用join-方法：无僵尸进程存在" class="headerlink" title="子进程调用join()方法：无僵尸进程存在"></a>子进程调用join()方法：无僵尸进程存在</h3><p>取消p.join()的注释，代码执行逻辑如下所述：</p><p>　　首先启动父进程，当执行到p.start()后，子进程开始执行，此时父进程处于挂起状态；当子进程结束后，父进程开始继续执行后续代码。打印结果如下图所示：</p><p><img src="./子进程运行周期长，调用无参join.png" alt="子进程运行周期长，调用无参join"></p><h2 id="场景二：子进程运行周期小与父进程"><a href="#场景二：子进程运行周期小与父进程" class="headerlink" title="场景二：子进程运行周期小与父进程"></a>场景二：子进程运行周期小与父进程</h2><h3 id="子进程不调用join-方法：有僵尸进程存在"><a href="#子进程不调用join-方法：有僵尸进程存在" class="headerlink" title="子进程不调用join()方法：有僵尸进程存在"></a>子进程不调用join()方法：<font color="red">有僵尸进程存在</font></h3><p>修改main_process_time为30，child_process_time为10；并注释掉p.join()，代码执行逻辑如下所述：</p><p>　　首先启动父进程，当执行到p.start()后，子父进程开始同时执行；<font color="red">当子进程尚未结束时</font>，子父进程的打印结果及其进程状态如下图所示：</p><p><img src="./父进程运行周期长，不调用join，且子进程尚未结束.png" alt="父进程运行周期长，不调用join，且子进程尚未结束"></p><p><font color="red">当子进程结束，但父进程尚未结束时，子进程变为僵尸进程</font>，进程的打印结果和进程状态如下图所示：</p><p><img src="./父进程运行周期长，不调用join，且子进程已经结束.png" alt="父进程运行周期长，不调用join，且子进程已经结束"></p><h3 id="子进程调用join-方法：无僵尸进程存在-1"><a href="#子进程调用join-方法：无僵尸进程存在-1" class="headerlink" title="子进程调用join()方法：无僵尸进程存在"></a>子进程调用join()方法：无僵尸进程存在</h3><p>修改main_process_time为30，child_process_time为10；并取消p.join()的注释，代码执行逻辑如下所述：</p><p>　　当父进程执行到p.start()后，子进程开始执行，且父进程挂起；当子进程尚未结束时，程序打印结果以及系统中进程状态如下图所示：</p><p><img src="./父进程运行周期长，调用无参join，且子进程尚未结束.png" alt="父进程运行周期长，调用无参join，且子进程尚未结束"></p><p><font color="red">当子进程结束而父进程尚未结束时，子进程正常销毁</font>，此时只有父进程在继续运行;程序打印结果以及系统中进程状态如下图所示：</p><p><img src="/images/placeholder.png" alt="父进程运行周期长，调用无参join，且子进程已经结束" data-src="./父进程运行周期长，调用无参join，且子进程已经结束.png" class="lazyload"></p><h2 id="子父进程伪并发"><a href="#子父进程伪并发" class="headerlink" title="子父进程伪并发"></a>子父进程伪并发</h2><p>　　在写代码的时候，需要注意join()方法位置；否则有可能会导致看似的多进程并发代码，实则的多进程的串行执行。Eg：将样例代码中的MainProcess类的excutor方法改写成如下形式：</p><p><img src="/images/placeholder.png" alt="join串行代码写法" data-src="./join串行代码写法.png" class="lazyload"></p><p>　　当基于for循环创建子进程时，若将p.join()卸载循环体内，则实际的执行逻辑为：主线程 =&gt; 子线程1 =&gt; 子线程2 =&gt; 子线程3 =&gt; 主线程；代码打印结果如下图所示：</p><p><img src="/images/placeholder.png" alt="join串行代码输出结果" data-src="./join串行代码输出结果.png" class="lazyload"></p><p>　　若想基于该写法实现真并发，可将p.join()改写为p.join(0.001)即可；代表着新建子进程后父进程的挂起时间仅为0.001秒，因此可以近似等价于同时执行；执行效果如下图所示：</p><p><img src="/images/placeholder.png" alt="join并行输出结果" data-src="./join并行输出结果.png" class="lazyload"></p><p>不过不建议采用这样的写法，因为这样会产生僵尸进程(详见<a href="https://yhyr.github.io/2018/06/06/%E5%9F%BA%E4%BA%8EPython%E5%88%9D%E6%8E%A2Linux%E4%B8%8B%E7%9A%84%E5%83%B5%E5%B0%B8%E8%BF%9B%E7%A8%8B%E5%92%8C%E5%AD%A4%E5%84%BF%E8%BF%9B%E7%A8%8B-%E4%BA%8C/" target="_blank" rel="noopener">join详解</a>)。</p><h1 id="Linux进程基本概念"><a href="#Linux进程基本概念" class="headerlink" title="Linux进程基本概念"></a>Linux进程基本概念</h1><p>　　在Linux中，默认情况下当父进程创建完子进程后，子父进程的运行是相互独立的、异步的；即父进程无法感知到子进程何时结束。为了让父进程可以在任意时刻都能获取到子进程结束时的状态信息，提供了如下机制：</p><ul><li>1) 当子进程结束后，系统在释放该子进程的所有资源的同时(eg：占用的内存、打开的文件等)，仍会保留一定的信息，包括进程号(process id)，进程的退出状态(the termination status of the process)，运行时间(the amount of CPU time taken by the process)等。</li><li>2) 当父进程调用wait/waitpid方法获取子进程的退出状态信息后，系统会彻底释放掉对应子进程的所有信息。如果父进程没有调用wait/waitpid方法，且父进程一直存活，则该子进程所有用的端口号信息一直保存，从而该子进程变为僵尸进程(对系统有害)；若父进程没有调用wait/waitpid方法，且父进程已经结束，则子进程会从僵尸进程转变为孤儿进程(对系统无害)。</li></ul><h3 id="僵尸进程"><a href="#僵尸进程" class="headerlink" title="僵尸进程"></a>僵尸进程</h3><p>　　一个进程创建了一个子进程，且当该子进程结束后，父进程没有调用wait/waitpid方法来获取子进程的退出状态信息，那么该子进程将会一直保留在系统中，并持续占有该进程的端口号等信息；进程标识符为<code>&lt;defunct&gt;</code>，进程状态位为Z，这种进程称之为僵尸进程。如下图所示：</p><p><img src="/images/placeholder.png" alt="僵尸进程" data-src="./僵尸进程.png" class="lazyload"></p><h3 id="孤儿进程"><a href="#孤儿进程" class="headerlink" title="孤儿进程"></a>孤儿进程</h3><p>　　当父进程退出而子进程还在运行时，这些子进程将会变成孤儿进程。孤儿进程将会init进程统一管理。因为init进程的进程号为1，所以所有的孤儿进程的父进程号均为1；此外，因为init进程会主动收集所有子进程的退出状态信息，所有由init进程管理的子进程是不会变成僵尸进程。因此，孤儿进程是对系统无害的。</p><p>　　例如：在上述样例代码的基础上，将子父进程的运行周期均扩大为60(保证有足够的时间去手动kill掉父进程，方便举例验证)，当子父进程运行的同时，手动kill掉父进程，子进程的进程号变化如下图所示</p><p>kill前：</p><p><img src="/images/placeholder.png" alt="孤儿进程手动kill前" data-src="./孤儿进程手动kill前.png" class="lazyload"></p><p>kill后：</p><p><img src="/images/placeholder.png" alt="孤儿进程手动kill后" data-src="./孤儿进程手动kill后.png" class="lazyload"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>　　Linux中，如果父进程正常结束的同时，子进程还未结束，此时父进程并不会退出让子进程变成孤儿进程，而是会有一个等待的操作；以阻塞或者轮询的方式等待所有子进程的结束而结束，毕竟“爸爸管儿子是天经地义的事”；正因为如此，在该应用场景一下(子进程的运行周期大于父进程)，即使不调用join()方法，也不会存在僵尸进程。如果在父进程在执行过程中因为调用os.exit()或者外部直接kill掉，此处父进程就不会在管理自己所产生的子进程，从而会导致子进程变成孤儿进程。相反如果子进程结束时父进程还未结束，此时如果未调用join()方法，则会因为父进程没有获取并处理子进程的退出信息而导致子进程变成僵尸进程；如果父进程一直存在，则该僵尸进程也会一直存在，相反如果父进程结束，则父进程在结束的前会等待并清除自己所产生的所有子进程的退出信息，从而消除僵尸进程。</p><p>　　通过上述demo，可以看出在不加join的时候，子父进程的运行方式是一种真正意义上的并行，但是由于特定的场景会导致出现僵尸进程；而加了join后，可以有效的消除僵尸进程，但是所写的多进程代码实则是一种多进程的串行执行模式(即：父进程会等待子进程结束后在执行)，其实是因为join()方法本身就是一种以阻塞主进程来等待子进程的方法。关于join()的解释和切实有效的消除僵尸进程可详见<a href="https://yhyr.github.io/2018/06/06/%E5%9F%BA%E4%BA%8EPython%E5%88%9D%E6%8E%A2Linux%E4%B8%8B%E7%9A%84%E5%83%B5%E5%B0%B8%E8%BF%9B%E7%A8%8B%E5%92%8C%E5%AD%A4%E5%84%BF%E8%BF%9B%E7%A8%8B-%E4%BA%8C/" target="_blank" rel="noopener">传送门</a></p>]]></content>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Linux </tag>
            
            <tag> 多进程 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>基于Python初探Linux下的僵尸进程和孤儿进程(二)</title>
      <link href="/2018/06/06/%E5%9F%BA%E4%BA%8EPython%E5%88%9D%E6%8E%A2Linux%E4%B8%8B%E7%9A%84%E5%83%B5%E5%B0%B8%E8%BF%9B%E7%A8%8B%E5%92%8C%E5%AD%A4%E5%84%BF%E8%BF%9B%E7%A8%8B-%E4%BA%8C/"/>
      <content type="html"><![CDATA[<p>了解Python-Process的join()方法的含义、以及在解决僵尸进程的原理和不足；同时结合实际应用场景提出有效可行的消除僵尸进程的方案。</p><a id="more"></a><h1 id="multiprocessing-Process的join-方法"><a href="#multiprocessing-Process的join-方法" class="headerlink" title="multiprocessing.Process的join()方法"></a>multiprocessing.Process的join()方法</h1><p>　　通过<a href="https://yhyr.github.io/2018/06/07/%E5%9F%BA%E4%BA%8EPython%E5%88%9D%E6%8E%A2Linux%E4%B8%8B%E7%9A%84%E5%83%B5%E5%B0%B8%E8%BF%9B%E7%A8%8B%E5%92%8C%E5%AD%A4%E5%84%BF%E8%BF%9B%E7%A8%8B-%E4%B8%80/" target="_blank" rel="noopener">上篇博文</a>可以看出join()方法具有清除僵尸进程的作用，与此同时带来的负面作用就是子父进程的串行执行(此处假设我们的目标是保证子父进程的执行方式是非阻塞的；对于实际需求是需要父进程阻塞等待子进程结束后在执行的应用场景，可以忽略本篇博文)。接下来将从join的底层实现出发探究其能够清楚僵尸进程的原因和阻塞执行的方式；同时基于一个demo来给出实际工作中如何准确有效的避免和消除僵尸进程。</p><h2 id="join初探"><a href="#join初探" class="headerlink" title="join初探"></a>join初探</h2><p><img src="./join源码描述.png" alt="join源码描述"></p><p>　　基于PyCharm查看join的源码，如上图所示；官方描述该方法的功能是“等待，直到子进程结束”；从字面意思也不难看出，该方法是一个阻塞方法；需要注意的是这里<strong>等待的主语是主进程而非子进程</strong>。该方法主要做了两件事：</p><ol><li>(1) 通知父进程调用wait方法</li><li>(2) 将该子进程从父进程的子进程列表中移除</li></ol><p>　　第一件事调用wait方法背后的实际调用链是：process模块的Process.join()  =&gt; forking模块的Popen.wait()，实则是调用了os.waitpd方法【注意这里的Popen根据操作系统的不同而不同，分为Unix/Linux和Windows两种】；至于为什么要调用该方法可以看我<a href="https://yhyr.github.io/2018/06/07/%E5%9F%BA%E4%BA%8EPython%E5%88%9D%E6%8E%A2Linux%E4%B8%8B%E7%9A%84%E5%83%B5%E5%B0%B8%E8%BF%9B%E7%A8%8B%E5%92%8C%E5%AD%A4%E5%84%BF%E8%BF%9B%E7%A8%8B-%E4%B8%80/" target="_blank" rel="noopener">上篇博文</a>中有关Linux进程基本概念模块的描述。</p><p><img src="./join底层调用.png" alt="join底层调用"></p><p>　　看到这，对于join()能消除僵尸进程的原因应该有了较为深刻的认识了；但是还存在一个问题：进程的串行执行问题还未解决。源码中join有一个timeout的参数，该参数的作用是设置一个该方法调用的等待时间，如果不设置，则等待子进程结束后在执行父进程；如果设置了，当子进程的运行周期大于你所设置的timeout时长时，表示过了timeout时长后(单位是秒)，开始唤醒父进程，此时子父进程开始同时执行；如果子进程的运行周期小与你所设置的timeout时长时，当你的子进程结束后会立即执行父进程，而不用等待你所设置的时长结束后才开始唤醒父进程。光说这些理论可能印象不会太深刻，接下来用几组例子来抛砖引玉，在加深对join理解的同时，介绍两种僵尸进程的有效清除办法。</p><p>样例代码如下所示</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MainProcess</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, main_process_time, child_process_time)</span>:</span></span><br><span class="line">        self.main_process_time = main_process_time</span><br><span class="line">        self.child_process_time = child_process_time</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">excutor</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'main process begin, pid=&#123;0&#125;, ppid=&#123;1&#125;'</span>.format(os.getpid(), os.getppid()))</span><br><span class="line">        p = ChildProcess(self.child_process_time)</span><br><span class="line">        p.start()</span><br><span class="line">        p.join(<span class="number">3</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.main_process_time):</span><br><span class="line">            print(<span class="string">'main process, pid=&#123;0&#125;, ppid=&#123;1&#125;, times=&#123;2&#125;'</span>.format(os.getpid(), os.getppid(), i))</span><br><span class="line">            time.sleep(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChildProcess</span><span class="params">(multiprocessing.Process)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, process_time)</span>:</span></span><br><span class="line">        multiprocessing.Process.__init__(self)</span><br><span class="line">        self.process_time = process_time</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'child process begin, pid=&#123;0&#125;, ppid=&#123;1&#125;'</span>.format(os.getpid(), os.getppid()))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.process_time):</span><br><span class="line">            print(<span class="string">'child process pid=&#123;0&#125;, ppid=&#123;1&#125;, times=&#123;2&#125;'</span>.format(os.getpid(), os.getppid(), i))</span><br><span class="line">            time.sleep(<span class="number">1</span>)</span><br><span class="line">        print(<span class="string">'child process end, pid=&#123;0&#125;, ppid=&#123;1&#125;'</span>.format(os.getpid(), os.getppid()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main_process_time = <span class="number">15</span></span><br><span class="line">    child_process_time = <span class="number">10</span></span><br><span class="line">    action = MainProcess(main_process_time, child_process_time)</span><br><span class="line">    action.excutor()</span><br></pre></td></tr></table></figure><h2 id="场景一：子进程的运行周期大于父进程"><a href="#场景一：子进程的运行周期大于父进程" class="headerlink" title="场景一：子进程的运行周期大于父进程"></a>场景一：子进程的运行周期大于父进程</h2><p>　　在该应用场景下，无论是否调用join方法都不会有僵尸进程存在；如果调用join，则父进程会被挂起，子父进程串行执行；如果不调用join，子父进程并行执行；现在分析一下调用带参数的join方法(eg：p.join(3))，当父进程启动，子进程执行时间小于三秒时，执行效果如下图所示：</p><p><img src="./子进程周期大于父进程,join带参数且子进程运行时间小于三秒.png" alt="子进程周期大于父进程,join带参数且子进程运行时间小于三秒"></p><p>当子进程执行时间大于三秒且小于父进程的执行周期时，执行效果如下图所示：</p><p><img src="./子进程周期大于父进程,join带参数且子进程运行时间大于三秒小于父进程周期.png" alt="子进程周期大于父进程,join带参数且子进程运行时间大于三秒小于父进程周期"></p><p>当父进程结束，而子进程继续执行，程序输出结果如下图所示：</p><p><img src="./子进程周期大于父进程,join带参数且父进程结束.png" alt="子进程周期大于父进程,join带参数且父进程结束"></p><h2 id="场景二：子进程的运行周期小与父进程"><a href="#场景二：子进程的运行周期小与父进程" class="headerlink" title="场景二：子进程的运行周期小与父进程"></a>场景二：子进程的运行周期小与父进程</h2><p>　　在该应用场景下，如果不调用join，则会有僵尸进程产生；如果调用join，则可以消除僵尸进程，但是子父进程串行执行；这种结果也并非我们所需要的。接下来尝试一下调用带参数的join方法(eg：p.join(3))，修改上述样例代码将main_process_time设置为15，child_process_time设置为10：</p><p>当父进程启动，子进程执行时间小于三秒时，执行效果如下图所示：</p><p><img src="/images/placeholder.png" alt="子进程周期小与父进程，join带参数且子进程执行时间小于三秒" data-src="./子进程周期小与父进程，join带参数且子进程执行时间小于三秒.png" class="lazyload"></p><p>当子进程执行时间大于三秒且小于子进程的执行周期时，执行效果如下图所示：</p><p><img src="/images/placeholder.png" alt="子进程周期小与父进程，join带参数且子进程执行时间大于三秒且小于子进程周期" data-src="./子进程周期小与父进程，join带参数且子进程执行时间大于三秒且小于子进程周期.png" class="lazyload"></p><p>当子进程结束，父进程继续执行时，程序输出结果如下图所示：</p><p><img src="/images/placeholder.png" alt="子进程周期小与父进程，join带参数且子进程结束" data-src="./子进程周期小与父进程，join带参数且子进程结束.png" class="lazyload"></p><p>　　通过这个例子可以看出，在该应用场景下，<font color="red">不论是加了带参数的join还是不加join，都会有僵尸进程产生</font>；相反加了不带参数的join虽可以避免僵尸进程，但是由于子父进程的串行执行导致仍无法满足我们的需求；为什么带参数和不带参数的join执行效果会如此大相径庭呢？其实通过上述源码是可以看出，<strong>join方法确实是会调用系统的os.waitpid()方法来获取子进程的退出信息，从而达到消除子进程的目的；但是这个过程是一次性的</strong>。什么意思呢？就是如果不带参数，则会一直挂起父进程，直到子进程结束后再执行p.join()方法，从而清除子进程；相反如果带参数，则会挂起父进程timeout时长后，唤醒父进程，此时父进程首先会执行p.join(3)这行代码，如果当前时刻子进程还未结束，则p.join(3)获取子进程的退出状态信息为空，则不会清除子进程，然后会紧接着执行父进程的后续逻辑；这时子父进程开始并行执行。如果子进程在次之后结束的同时父进程还未结束，则父进程会因为无法获取到子进程的退出信息而导致子进程沦为僵尸进程。(开始自己以为join(3)意味着父进程会在三秒后唤醒的同时，父进程会轮询监控子进程的退出信息，从而达到消除僵尸进程的作用，^v^ 还是太年轻~想当然了~哈哈！！！)。</p><h1 id="消除僵尸进程"><a href="#消除僵尸进程" class="headerlink" title="消除僵尸进程"></a>消除僵尸进程</h1><p>　　随着对Process的join()方法的深入理解，越发觉得离我们的目标渐行渐远。要不就会产生僵尸进程，要不就会挂起父进程，从而无法实现并行效果。那么问题来了，到底该如何有效的消除僵尸进程呢？</p><p>　　网上有些帖子和博客说可以通过os._exti(0)或者sys.exit(0)可以有效的退出子进程，这一点毋庸置疑；但是需要注意的是这种退出并没有什么太大的作用，因为主动退出子进程并不会通知父进程去获取子进程得退出状态信息，从而导致子进程还是会变成僵尸进程。在这里我将介绍两种行之有效的方法来实现彻底消除僵尸进程的同时，实现子父进程的并发。</p><h2 id="方法一：创建两次子进程-fork两次"><a href="#方法一：创建两次子进程-fork两次" class="headerlink" title="方法一：创建两次子进程(fork两次)"></a>方法一：创建两次子进程(fork两次)</h2><p>　　如果百度过此类问题的不难发现，网上有很多说可以通过fork两次来避免僵尸进程。其实这是一个很不错的方法，也是一个比较容易理解的。只是关于该方法的解释不是很多(可能因为笔者太low，对于很多人来说都是一看就懂的^v^)，在这里我将就该方法做以详细的解释和说明，希望对刚接触此类问题的小伙伴们有所帮助。</p><p>　　首先需要注意的是fork函数是unix/linux系统上特有的，在Windows上运行该函数会直接报错，而通过都是用Windows机器做开发，在Linux上跑代码的这种，直接在Linux上写代码又是比较麻烦的(如果愿意可以基于VM搭建一个桌面版的CentOS，然后装个编译器来开发)，所以这里笔者从一开始就选择Python提供的一种跨平台的多进程模块 – multiprocessing来实现多进程(其实multiprocessing中基于Linux的代码实现逻辑就是fork，对于该模块源码初探可详见<a href="https://yhyr.github.io/2018/06/07/%E5%9F%BA%E4%BA%8EPython%E5%88%9D%E6%8E%A2Linux%E4%B8%8B%E7%9A%84%E5%83%B5%E5%B0%B8%E8%BF%9B%E7%A8%8B%E5%92%8C%E5%AD%A4%E5%84%BF%E8%BF%9B%E7%A8%8B-%E4%B8%89/" target="_blank" rel="noopener">传送门</a>)。</p><p>　　如何理解fork两次即可达到我们想要的想过呢？此处假设我们的业务场景是父进程一直存在，而子进程的执行周期短，且执行完后就退出。我们知道，当主进程创建一个子进程时，此时子进程的ppid即就是父进程的pid；而子进程结束后如果父进程没有获取子进程的退出状态信息，则子进程会变成僵尸进程；我们又知道，如果一个子进程是孤儿进程的话，那么它就是安全可靠的(不会产生僵尸进程)；所以基于以上原因，可以进行如下设计：主进程的业务逻辑保持不变，只是在主进程创建子进程的时候，不直接创建子进程去执行相应的业务逻辑；而是创建一个单独进程(此处理解为爸爸进程)，该进程只干一件事，就是创建原本应该有父进程创建的子进程。即就是<font color="red">将原本的“主进程 =&gt; 儿子进程”修改为“主进程 =&gt; 爸爸进程 =&gt; 儿子进程”</font>，这种设计里只有主进程和儿子进程是需要关注的，而爸爸进程逻辑很简单，就是初始化儿子进程；所以当爸爸进程结束后儿子进程就沦为孤儿进程了，这样无论儿子进程执行多久，都不会产生僵尸进程。</p><p>　　有人就会想，爸爸进程退出不也会产生僵尸进程吗？其实这个问题很好解决，利用上述中的不带参数的join()方法即可解决。可以在主进程中创建父进程的同时，添加p.join()方法，因为爸爸进程创建儿子进程的耗时很短，所以可以在主进程创建爸爸进程的时候使用p.join()挂起，这个时间差是可以忽略和接受的，这样当父进程创建完儿子进程后父进程就会立马结束，此时主进程就会执行p.join()方法获取到爸爸进程的退出信息，从而彻底消除爸爸进程；这样进程列表里就只剩下一个主进程和一个而孤儿进程(原本的儿子进程转化而来)；这样就实现了真正意义上的并发。为了测试时效果看的明显，在源码中添加了sleep()，如果在实际的业务开发中，可以注掉源码中的相关sleep()代码，具体源码如下所示(该写法可兼容Windows和Linux)：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MainProcess</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, main_process_time, child_process_time)</span>:</span></span><br><span class="line">        self.main_process_time = main_process_time</span><br><span class="line">        self.child_process_time = child_process_time</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">excutor</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'main process begin, pid=&#123;0&#125;'</span>.format(os.getpid()))</span><br><span class="line">        time.sleep(<span class="number">5</span>)</span><br><span class="line">        p = FatherProcess(self.child_process_time)</span><br><span class="line">        p.start()</span><br><span class="line">        p.join()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.main_process_time):</span><br><span class="line">            print(<span class="string">'main process, pid=&#123;0&#125;, times=&#123;1&#125;'</span>.format(os.getpid(), i))</span><br><span class="line">            time.sleep(<span class="number">1</span>)</span><br><span class="line">        print(<span class="string">'main process end, pid=&#123;0&#125;'</span>.format(os.getpid()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FatherProcess</span><span class="params">(multiprocessing.Process)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, process_time)</span>:</span></span><br><span class="line">        multiprocessing.Process.__init__(self)</span><br><span class="line">        self.process_time = process_time</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'father process begin, pid=&#123;&#125; =&gt; create childPorcess'</span>.format(os.getpid()))</span><br><span class="line">        p = ChildProcess(self.process_time)</span><br><span class="line">        time.sleep(<span class="number">5</span>)</span><br><span class="line">        p.start()</span><br><span class="line">        print(<span class="string">'father process end, pid=&#123;&#125;'</span>.format(os.getpid()))</span><br><span class="line">        time.sleep(<span class="number">5</span>)</span><br><span class="line">        os._exit(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChildProcess</span><span class="params">(multiprocessing.Process)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, process_time)</span>:</span></span><br><span class="line">        multiprocessing.Process.__init__(self)</span><br><span class="line">        self.process_time = process_time</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'child process begin, pid=&#123;0&#125;'</span>.format(os.getpid()))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.process_time):</span><br><span class="line">            print(<span class="string">'child process pid=&#123;0&#125;, times=&#123;1&#125;'</span>.format(os.getpid(), i))</span><br><span class="line">            time.sleep(<span class="number">1</span>)</span><br><span class="line">        print(<span class="string">'child process end, pid=&#123;0&#125;'</span>.format(os.getpid()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main_process_time = <span class="number">10</span></span><br><span class="line">    child_process_time = <span class="number">5</span></span><br><span class="line">    action = MainProcess(main_process_time, child_process_time)</span><br><span class="line">    action.excutor()</span><br></pre></td></tr></table></figure><h2 id="方法二：基于Linux信号清除僵尸进程"><a href="#方法二：基于Linux信号清除僵尸进程" class="headerlink" title="方法二：基于Linux信号清除僵尸进程"></a>方法二：基于Linux信号清除僵尸进程</h2><p>　　创建两次子进程的方法是比较好理解的，但是代码的入侵还是比较大的，基于Linux信号的方式可以只需要添加一行代码<code>signal.signal(signal.SIGCHLD, signal.SIG_IGN)</code>就可实现所需要的逻辑；不过该解决方案只适用于Linux/Unix系统；在Windows下执行是会报错。代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> multiprocessing</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> signal</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MainProcess</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, main_process_time, child_process_time)</span>:</span></span><br><span class="line">        self.main_process_time = main_process_time</span><br><span class="line">        self.child_process_time = child_process_time</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">excutor</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'main process begin, pid=&#123;0&#125;, ppid=&#123;1&#125;'</span>.format(os.getpid(), os.getppid()))</span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        添加信号</span></span><br><span class="line"><span class="string">            signal.SIGCHLD的语义为：子进程状态改变后产生此信号</span></span><br><span class="line"><span class="string">            signal.SIG_IGN的语义为：信号的处理方式为忽略模式</span></span><br><span class="line"><span class="string">            默认采用SIG_DFL, 代表默认的处理方式为不会理会这个信号，但是也不会丢弃该信号量，</span></span><br><span class="line"><span class="string">            如果系统不调用wait/waitpid，则会变成僵尸进程</span></span><br><span class="line"><span class="string">            </span></span><br><span class="line"><span class="string">            第二个参数也可以自定义处理逻辑，eg：将signal.SIG_IGN修改为自定义sigchld_handler方法，</span></span><br><span class="line"><span class="string">            专门用来处理对应的信号</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        signal.signal(signal.SIGCHLD, signal.SIG_IGN)</span><br><span class="line"></span><br><span class="line">        p = ChildProcess(self.child_process_time)</span><br><span class="line">        p.start()</span><br><span class="line">        p.join(<span class="number">5</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.main_process_time):</span><br><span class="line">            print(<span class="string">'main process, pid=&#123;0&#125;, ppid=&#123;1&#125;, times=&#123;2&#125;'</span>.format(os.getpid(), os.getppid(), i))</span><br><span class="line">            time.sleep(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ChildProcess</span><span class="params">(multiprocessing.Process)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, process_time)</span>:</span></span><br><span class="line">        multiprocessing.Process.__init__(self)</span><br><span class="line">        self.process_time = process_time</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span><span class="params">(self)</span>:</span></span><br><span class="line">        print(<span class="string">'child process begin, pid=&#123;0&#125;, ppid=&#123;1&#125;'</span>.format(os.getpid(), os.getppid()))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(self.process_time):</span><br><span class="line">            print(<span class="string">'child process pid=&#123;0&#125;, ppid=&#123;1&#125;, times=&#123;2&#125;'</span>.format(os.getpid(), os.getppid(), i))</span><br><span class="line">            time.sleep(<span class="number">1</span>)</span><br><span class="line">        print(<span class="string">'child process end, pid=&#123;0&#125;, ppid=&#123;1&#125;'</span>.format(os.getpid(), os.getppid()))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main_process_time = <span class="number">30</span></span><br><span class="line">    child_process_time = <span class="number">15</span></span><br><span class="line">    action = MainProcess(main_process_time, child_process_time)</span><br><span class="line">    action.excutor()</span><br></pre></td></tr></table></figure>]]></content>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> Linux </tag>
            
            <tag> 多进程 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Python-Mysql依赖初探</title>
      <link href="/2018/06/01/Python-Mysql%E4%BE%9D%E8%B5%96%E5%88%9D%E6%8E%A2/"/>
      <content type="html"><![CDATA[<h3 id="环境依赖"><a href="#环境依赖" class="headerlink" title="环境依赖"></a>环境依赖</h3><blockquote><p>MySQL 5.7.17</p><p>Python 2.7</p><p>Mysql-python 1.2.5</p></blockquote><a id="more"></a><p>　　MySQLdb是基于MySQL C API(原生MySQL API)为核心的面向Python的接口，封装了许多MySQL C API的方法，简化Python操作MySQL的难度。在原生的MySQL API中，万物皆String。(当然，可以通过自定义conv来实现数据类型的转化)。官方解释如下图所示：</p><p><img src="./mysqldb_官方解释.png" alt="mysqldb_官方解释"></p><p>　　Eg：原始的数据源中，age列是int类型，基于原生API查询后，所有的结果均为String类型；执行效果如下图所示：</p><p><img src="./mysql原生写法.png" alt="mysql原生写法"></p><p>　　可以通过自定义转换dict来实现查询结果的类型转换；具体的实现也很简单，只需要在mysql初始化的时候，自定义conv参数即可(eg：将SQL中int转化为Long，将SQL中的float转化为Double)，样例代码如下图所示：</p><p><img src="./mysql原生写法_自定义conv参数.png" alt="mysql原生写法_自定义conv参数"></p><p>　　由于原始API并不是那么的拿来主义，直接基于此操作需要care的东西太多，所以才有了MySQLdb这样简单易上手的第三方依赖包。MySQLdb带来的便捷主要体现在一下两点：</p><ol><li>封装并提供很多API接口，降低使用门槛</li><li>自动适配原始SQL表的字段类型</li></ol><p>　　由于高级的API方法在实际工作中以用到很多，并且和其他语言的mysql驱动没太大差别，在这里不做讨论。重点分享一下第二点，在数据类型自定识别中，存在如下一种情况：<strong>基于MySQLdb查询int字段时，实际默认的返回类型是Long型，而非int</strong>。 Eg：</p><p><img src="./mysqldb写法.png" alt="mysqldb写法"></p><p>　　官方给出的解释是：<em>while MySQL’s INTEGER column translates perfectly into a Python integer, UNSIGNED INTEGER could overflow, so these values are converted to Python long integers instead</em>. 简单的说，就是<strong>MySQL的int类型转化为Python的int类型时，可能存在无符号整形精度丢失的情况</strong>。如何理解这句话：通过在程序中(Python)，不会刻意的定义一个无符号的整形变量，一般默认的int为有符号整形，eg：假如为int32，则对应的有符号整形的范围为：-2^(32-1) ~ 2^(32-1) - 1；相反无符号整数的范围为：0 ~ 2^32 - 1；所以当数据可中指定int为无符号整型时，程序中用有符号整形接受，就会存在因为数据范围不一致而导致的数据精度丢失情况。基于此原因，MySQLdb在设计时，为了规避该潜在问题，默认将int类型转化为long型。这就是为什么整形查询结果后面会带有L后缀的原因。</p><p>　　当然MySQLdb也可以支持类似原生API那种通过自定义conv参数的方式来实现数据类型的自定义(不过不建议用户这样操作)。Eg：将SQL中Long转化为int，将SQL中的double转化为flout，代码样例如下图所示：</p><p><img src="./mysqldb自定conv参数.png" alt="mysqldb自定conv参数"></p><p>　　因为MySQLdb对应所有的mysql数据类型都有一套自己默认的转化映射关系(详见源码：MySQLdb.converters)，对于原生API，可以指定自己想转化的特定数据列；但是该方法在MySQLdb中并不那么的适用；如果要自定义数据类型，就必须指定所涵盖的所有数据列，这样很有可能因为枚举不够全面而导致程序报错的情况；所以在实际工作时，不建议在这一步转化数据类型，如有需要，在将数据加载到内存后，再转化成所需要的类型。</p><p>参考链接：<a href="http://mysql-python.sourceforge.net/MySQLdb.html" target="_blank" rel="noopener">官方Doc</a> </p>]]></content>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> MySQL </tag>
            
            <tag> MySQLdb </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>基于pandas的数据分析之数据类型转化踩坑总结</title>
      <link href="/2018/06/01/%E5%9F%BA%E4%BA%8Epandas%E7%9A%84%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E4%B9%8B%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E8%BD%AC%E5%8C%96%E8%B8%A9%E5%9D%91%E6%80%BB%E7%BB%93/"/>
      <content type="html"><![CDATA[<h3 id="从以下两个方面来讨论在实际工作中所遇到的数据类型转化问题"><a href="#从以下两个方面来讨论在实际工作中所遇到的数据类型转化问题" class="headerlink" title="从以下两个方面来讨论在实际工作中所遇到的数据类型转化问题"></a>从以下两个方面来讨论在实际工作中所遇到的数据类型转化问题</h3><ol><li>由于数据缺失导致DataFrame中int转float</li><li>由于数据类型字符串导致csv加载到DataFrame时String转numeric</li></ol><a id="more"></a><h3 id="环境依赖"><a href="#环境依赖" class="headerlink" title="环境依赖"></a>环境依赖</h3><blockquote><p>MySQL 5.7.17</p><p>Python 2.7</p><p>MySQL-python 1.2.5</p><p>Pandas 0.18.1</p></blockquote><h1 id="一、由于数据缺失导致DataFrame中int转float"><a href="#一、由于数据缺失导致DataFrame中int转float" class="headerlink" title="一、由于数据缺失导致DataFrame中int转float"></a>一、由于数据缺失导致DataFrame中int转float</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>将MySQL的表结构转为Pandas的DataFrame时，会出现如下问题：</p><ul><li>age列原本为int类型，但是在DataFrame中转化为float类型</li><li>对于SQL中的None值，在DataFrame里有多种表示方式</li></ul><p>数据源信息如下所示：</p><p><img src="./表数据信息.png" alt="表数据信息"></p><p><img src="./表字段信息.png" alt="表字段信息"></p><p><img src="./表信息在DataFrame中的展示效果.png" alt="表信息在DataFrame中的展示效果"></p><h2 id="原因分析"><a href="#原因分析" class="headerlink" title="原因分析"></a>原因分析</h2><p>　　通过查阅官方文档得知，pandas在处理缺失值上，拥有一套自己的转化逻辑；具体的转化规则如下图所示：</p><p><img src="./pandas缺失值处理规则.png" alt="pandas缺失值处理规则"></p><p>　　不难看出，当int型数据列包含空值时，会将该列转化为float类型；个人理解：在pandas中，对于数据类型的空值，是统一用Nan来表示的；Nan在pandas中是一float类型，代表一种特殊的值，而非其他语言中所定义的空对象；因此对于int类型的空值，因无法表示相应的空值，所以需要先做数据类型的转化，然后用Nan来表示其含义。</p><p><img src="./Python_nan值类型.png" alt="Python_nan值类型"></p><p>　　通过这个例子，希望在理解numeric类型转化原因的同时，能够提高对异常数据的警惕性和敏感度；理解业务，实现需求的同时，能有准确、有效的进行数据降噪，从而提升数据的真实性和可信度。</p><h1 id="由于数值类型字符串导致从csv加载到DataFrame时String转numeric"><a href="#由于数值类型字符串导致从csv加载到DataFrame时String转numeric" class="headerlink" title="由于数值类型字符串导致从csv加载到DataFrame时String转numeric"></a>由于数值类型字符串导致从csv加载到DataFrame时String转numeric</h1><h2 id="缺失值"><a href="#缺失值" class="headerlink" title="缺失值"></a>缺失值</h2><p>缺失值分别在MySQL、Python和Pandas上的表现形式如下所示：</p><table><thead><tr><th>/</th><th>字符串空值</th><th>空字符串</th><th>数值类型空值</th></tr></thead><tbody><tr><td>MySQL</td><td>Null</td><td>‘’</td><td>Null</td></tr><tr><td>Python</td><td>None</td><td>‘’</td><td>None</td></tr><tr><td>Pandas</td><td>None</td><td>‘’</td><td>Nan</td></tr></tbody></table><p>　　<strong>由于字符串空值和空字符串这两种情况在写到csv的效果完全一致，从而导致在读取数据时无法做区分</strong>。如果后续业务明确要求区分处理这两种情况，则会因为一次文件的读写操作导致数据失真。基于此原因，建议在实际开发的过程中，在同一个team下规定一个唯一的字符串标识符来代表None值(参考数仓建设)，从而有效区分字符串空值和空字符串的区别。</p><h2 id="数据类型转化"><a href="#数据类型转化" class="headerlink" title="数据类型转化"></a>数据类型转化</h2><p>　　通过pd.read_csv方法加载csv文件时，如果某一列为数值字符串，则该列会别识别为numeric类型(eg：int, float)，而非原始的String类型。Eg：将前文中MySQL数据表中的sex列和passWord列的内容修改为纯数值，数据集及各列对应的数据类型分别如下所示：</p><p><img src="/images/placeholder.png" alt="数值型字符串数据集" data-src="./数值型字符串数据集.png" class="lazyload"></p><p><img src="/images/placeholder.png" alt="原始数据集数据类型" data-src="./原始数据集数据类型.png" class="lazyload"></p><p>　　并将该表中的数据先写到本地csv文件；然后通过pd.read_csv读取文件时发现sex列被识别为int类型，passWord列被识别为float类型。数据展示效果及各列对应的数据类型分别如下图所示：</p><p><img src="/images/placeholder.png" alt="经过一次读写操作后的数据集" data-src="./经过一次读写操作后的数据集.png" class="lazyload"></p><p><img src="/images/placeholder.png" alt="经过一次读写操作后的数据集数据类型" data-src="./经过一次读写操作后的数据集数据类型.png" class="lazyload"></p><p>　　所以在基于pandas操作csv文件时，需要特别注意这种情况。如果在后续的分析中，需要保留原始数据格式，则在读取csv文件时，需要通过dtype参数来显示的指定目标字段的数据类型，从而保证数据类型的前后统一。</p><p><img src="/images/placeholder.png" alt="读csv指定数据类型" data-src="./读csv指定数据类型.png" class="lazyload"></p><p><img src="/images/placeholder.png" alt="指定读取类型后的字段数据类型" data-src="./指定读取类型后的字段数据类型.png" class="lazyload"></p><p>　　通过上述案例分析，希望今后在操作csv文件时注意区分空字符串和字符串空值在不同应用场景下的实际意义，以免造成潜在的数据隐患。</p>]]></content>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 数据分析 </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>基于GitHub Pages + Hexo 多端搭建本地博客环境</title>
      <link href="/2018/05/28/%E5%9F%BA%E4%BA%8EGitHub-Pages-Hexo-%E5%A4%9A%E7%AB%AF%E6%90%AD%E5%BB%BA%E6%9C%AC%E5%9C%B0%E5%8D%9A%E5%AE%A2%E7%8E%AF%E5%A2%83/"/>
      <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>阅读此文前，假定你已经了解如何基于GitHub Pages + Hexo来搭建个人博客。如果不了解的，可参考如下<a href="https://www.cnblogs.com/fengxiongZz/p/7707219.html" target="_blank" rel="noopener">教程</a>或<a href="http://crazymilk.github.io/2015/12/28/GitHub-Pages-Hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/" target="_blank" rel="noopener">教程</a>(就不重复造轮子了)；为自己的博客添加主题，其实也很简单：只需要在<a href="https://hexo.io/themes/" target="_blank" rel="noopener">Hexo</a>中挑选出自己心仪的风格，并将对应的源码从GitHub上clone到本地，并存放在你本地博客目录的themes路径下，并修改博客根目录的_config.yml文件，将theme: landscape中的landscape替换为你clone下来的文件夹名即可。具体操作参考<a href="http://www.cnblogs.com/fengxiongZz/p/7707568.html" target="_blank" rel="noopener">教程</a>。</p><a id="more"></a><p>接下来进入正题：搭建过个人博客的都清楚，Hexo是通过Node.js将本地的md文件(即就是你的博客源文件)基于指定的主题渲染成静态页面，在本地生成public文件夹，然后通过部署将public文件夹同步到GitHub上，这样你就可以通过username.github.io来访问。那么问题来了，当你如果换了台电脑或者想在其他机器上修改你的博客或者写一篇新博客，这就没招了。因为通过Hexo发布到GitHub上的是经过Node.js渲染过后的HTML文件，而非原始的md文件；所有你clone下GitHub上的源码也没什么用，除非你可以接受直接基于HTML写文档，那在这里我给你一个大写的服字。</p><h2 id="Hexo的安装过程"><a href="#Hexo的安装过程" class="headerlink" title="Hexo的安装过程"></a>Hexo的安装过程</h2><p>首先，通过<code>npm install hexo -g</code>在本地安装hexo(保证电脑上已经安装了git和node.js)</p><p>然后，在指定路径下执行<code>hexo init</code>命令来初始化hexo环境的相关文件，结果如下图所示：</p><p><img src="./hexo_init.png" alt="hexo初始化目录列表"></p><p>初始化出来的文件均为hexo环境配置</p><p>其次，通过<code>npm install</code>命令安装相关依赖，再通过<code>hexo g</code>来实现文档渲染，最后通过<code>hexo s</code>开始本地服务，如果报端口占用，可通过<code>hexo s -p 5000</code>重定向端口号，就可以通过localhost:5000来实现本地模式查看。当通过<code>hexo g</code>渲染文档后，会发现在博客根目录下新建了一个public目录，如下图所示：</p><p><img src="./hexo_g.png" alt="hexo渲染目录结构"></p><p>public目录就是最终发布到GitHub上的目录，node_module目录是执行完<code>npm install</code>后安装在本地的相关依赖，不用care它。</p><p>在理解了hexo的目录结构以后不难看出，其实博客网站只关注public目录；而hexo的环境信息(所使用的主题、所有博客的原md格式的文档、博客的配置信息)都是在scaffolds，source，themes和_config.yml中，所以只需要将这些文件维护到GitHub上，就可以随时随地在任何地方down下自己的环境文件，从而就可以开始你的创作了。</p><h2 id="实现方案"><a href="#实现方案" class="headerlink" title="实现方案"></a>实现方案</h2><p>在username.github.io仓库中新建一个hexo分支，用master分支来存放hexo发布的静态网页信息(即就是public目录下的内容)，用hexo分支来存放你的博客环境信息。其实当你执行完<code>hexo init</code>初始化命令后，你会发现hexo会默认帮你生成一个.gitignore文件，内容如下所示：</p><p><img src="./hexo_init_gitignore.png" alt="gitignore内容"></p><p>这里面已经自动帮你剔除了所有和hexo环境信息无关的目录了，所有你不做主动做任何过滤，直接在本地新建一个hexo分支，并将环境信息提交到GitHub对应的hexo分支即可。在博客根目录的_config.yml中指定deploy的branch为master分支即可。</p><h2 id="本地自测"><a href="#本地自测" class="headerlink" title="本地自测"></a>本地自测</h2><p>在本机新建一个文件夹，将username.github.io仓库的hexo分支clone到本地，然后执行<code>npm install</code>命令来安装所需要的相关相关依赖。<strong>切结不要执行<code>hexo init</code>命令</strong> ，因为该命令会初始化本地hexo环境，会用默认的hexo配置和主题来覆盖你自己的设置；因为我们hexo分支上的源码已经是自己当初所配置的环境，所以没必要初始化，只需要安装相关依赖，然后渲染自己的博客源文件<code>hexo g</code>,并在本地新开一个端口做测试<code>hexo s -p 5000</code>, 你会发现本地展示效果和通过username.github.io一致。至此已搞定全部问题。</p><p>需要注意的是，我这里是在本地做的测试。如果换台机器，保证已安装有git和node.js的前提下，按照上述再操作一遍即可。</p>]]></content>
      
      
        <tags>
            
            <tag> GitHub Pages </tag>
            
            <tag> Hexo </tag>
            
            <tag> 博客搭建 </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
